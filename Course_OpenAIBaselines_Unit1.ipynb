{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI with ROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotignite_logo_text.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 1: Exploring the ROS + OpenAI structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time to completion: <b>4 hours</b><br><br>\n",
    "In this unit, you are going to be following, step by step, the full workflow of a CartPole simulated environment, including all the environments and scripts involved in its training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">END OF SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this chapter will be to try to explain, in the simplest way possible, how a training session using OpenAI, ROS, and Gazebo will be structured and what is the full workflow. Each one of these fields is quite complex already on its own, let alone when they are combined. But don't panic just yet! The Robot Ignite Academy is coming to your rescue! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a common structure in order to organize and split up everything you need to create your training environments from the beginning! And the main aim of this structure is to have everything organized with a simple and easy to understand method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, this structure can be divided into two big parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training Script**: The training script will define and set up the learning algorithm that you are going to use to train your agent.\n",
    "\n",
    "\n",
    "* **Training Environments**: The training environments will be the ones in charge of providing all the needed data to your learning algorithm in order to make the agent learn. There are different types of Training Environments: **Gazebo Environment**, **Robot Environment**, and **Task Environment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll talk more about the Training Environments later in the chapter, but for now, let's focus on the Training Script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the whole explanation, we will be using the CartPole problem. The CartPole problem consists of a cart that moves along a horizontal axis and a pole that is connected with one joint on top of the moving cart. So, in any state, the cart only has two possible actions: move to the left or move to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cartpole.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that although we will be using the CartPole simulation for this explanation, the structure that you are going to learn is applicable to any other case. You will just need to adapt some parts of the code, depending on the problem you want to solve. So, with the proper introductions made, let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Demo 1.1**</p>\n",
    "<br>\n",
    "\n",
    "a) Launch the training script and see how the CartPole starts training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roslaunch cartpole_v0_training start_training.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see how the CartPole starts moving and learning, as in the video below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cartpole_gif.gif\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Demo 1.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, in the previous demo, you launched a Python script called **start_training.py**, which activated the whole learning process. But... do you really want to know what this script does? Do you really want to know what's going on behind the scenes? What's the whole structure that holds this training? Then... let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's talk about this **start_training.py** script. This script is the one that sets up the algorithm that you are going to use in order to make your robot learn. In this case, we are going to use Qlearn. Q-learning is a reinforcement learning technique used in machine learning. The goal of Q-Learning is to learn a policy, which tells an agent which action to take under which circumstances. But we're not going to enter into details about how Qlearn works now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can see an example of a Training Script that uses Qlearn. Then, we'll have a look at the most important parts of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**start_training.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import gym\n",
    "import time\n",
    "import numpy\n",
    "import random\n",
    "import time\n",
    "import qlearn\n",
    "from gym import wrappers\n",
    "\n",
    "# ROS packages required\n",
    "import rospy\n",
    "import rospkg\n",
    "\n",
    "# import our training environment\n",
    "from openai_ros.task_envs.cartpole_stay_up import stay_up\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    rospy.init_node('cartpole_gym', anonymous=True, log_level=rospy.WARN)\n",
    "\n",
    "    # Create the Gym environment\n",
    "    env = gym.make('CartPoleStayUp-v0')\n",
    "    rospy.loginfo ( \"Gym environment done\")\n",
    "        \n",
    "    # Set the logging system\n",
    "    rospack = rospkg.RosPack()\n",
    "    pkg_path = rospack.get_path('cartpole_v0_training')\n",
    "    outdir = pkg_path + '/training_results'\n",
    "    env = wrappers.Monitor(env, outdir, force=True) \n",
    "    rospy.loginfo ( \"Monitor Wrapper started\")\n",
    "\n",
    "    last_time_steps = numpy.ndarray(0)\n",
    "\n",
    "    # Loads parameters from the ROS param server\n",
    "    # Parameters are stored in a yaml file inside the config directory\n",
    "    # They are loaded at runtime by the launch file\n",
    "    Alpha = rospy.get_param(\"/cartpole_v0/alpha\")\n",
    "    Epsilon = rospy.get_param(\"/cartpole_v0/epsilon\")\n",
    "    Gamma = rospy.get_param(\"/cartpole_v0/gamma\")\n",
    "    epsilon_discount = rospy.get_param(\"/cartpole_v0/epsilon_discount\")\n",
    "    nepisodes = rospy.get_param(\"/cartpole_v0/nepisodes\")\n",
    "    nsteps = rospy.get_param(\"/cartpole_v0/nsteps\")\n",
    "    running_step = rospy.get_param(\"/cartpole_v0/running_step\")\n",
    "\n",
    "    # Initialises the algorithm that we are going to use for learning\n",
    "    qlearn = qlearn.QLearn(actions=range(env.action_space.n),\n",
    "                    alpha=Alpha, gamma=Gamma, epsilon=Epsilon)\n",
    "    initial_epsilon = qlearn.epsilon\n",
    "\n",
    "    start_time = time.time()\n",
    "    highest_reward = 0\n",
    "\n",
    "    # Starts the main training loop: the one about the episodes to do\n",
    "    for x in range(nepisodes):\n",
    "        rospy.logdebug(\"############### START EPISODE => \" + str(x))\n",
    "        \n",
    "        cumulated_reward = 0  \n",
    "        done = False\n",
    "        if qlearn.epsilon > 0.05:\n",
    "            qlearn.epsilon *= epsilon_discount\n",
    "        \n",
    "        # Initialize the environment and get first state of the robot\n",
    "        \n",
    "        observation = env.reset()\n",
    "        state = ''.join(map(str, observation))\n",
    "        \n",
    "        # Show on screen the actual situation of the robot\n",
    "        # for each episode, we test the robot for nsteps\n",
    "        for i in range(nsteps):\n",
    "            \n",
    "            rospy.loginfo(\"############### Start Step => \"+str(i))\n",
    "            # Pick an action based on the current state\n",
    "            action = qlearn.chooseAction(state)\n",
    "            rospy.loginfo (\"Next action is: %d\", action)\n",
    "            # Execute the action in the environment and get feedback\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            rospy.loginfo(str(observation) + \" \" + str(reward))\n",
    "            cumulated_reward += reward\n",
    "            if highest_reward < cumulated_reward:\n",
    "                highest_reward = cumulated_reward\n",
    "\n",
    "            nextState = ''.join(map(str, observation))\n",
    "\n",
    "            # Make the algorithm learn based on the results\n",
    "            #rospy.logwarn(\"############### State we were => \" + str(state))\n",
    "            #rospy.logwarn(\"############### Action that we took => \" + str(action))\n",
    "            #rospy.logwarn(\"############### Reward that action gave => \" + str(reward))\n",
    "            #rospy.logwarn(\"############### State in which we will start next step => \" + str(nextState))\n",
    "            qlearn.learn(state, action, reward, nextState)\n",
    "\n",
    "            if not(done):\n",
    "                state = nextState\n",
    "            else:\n",
    "                rospy.loginfo (\"DONE\")\n",
    "                last_time_steps = numpy.append(last_time_steps, [int(i + 1)])\n",
    "                break\n",
    "            rospy.loginfo(\"############### END Step =>\" + str(i))\n",
    "            #raw_input(\"Next Step...PRESS KEY\")\n",
    "            #rospy.sleep(2.0)\n",
    "        m, s = divmod(int(time.time() - start_time), 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        rospy.logwarn ( (\"EP: \"+str(x+1)+\" - [alpha: \"+str(round(qlearn.alpha,2))+\" - gamma: \"+str(round(qlearn.gamma,2))+\" - epsilon: \"+str(round(qlearn.epsilon,2))+\"] - Reward: \"+str(cumulated_reward)+\"     Time: %d:%02d:%02d\" % (h, m, s)))\n",
    "\n",
    "    \n",
    "    rospy.loginfo ( (\"\\n|\"+str(nepisodes)+\"|\"+str(qlearn.alpha)+\"|\"+str(qlearn.gamma)+\"|\"+str(initial_epsilon)+\"*\"+str(epsilon_discount)+\"|\"+str(highest_reward)+\"| PICTURE |\"))\n",
    "\n",
    "    l = last_time_steps.tolist()\n",
    "    l.sort()\n",
    "\n",
    "    #print(\"Parameters: a=\"+str)\n",
    "    rospy.loginfo(\"Overall score: {:0.2f}\".format(last_time_steps.mean()))\n",
    "    rospy.loginfo(\"Best 100 score: {:0.2f}\".format(reduce(lambda x, y: x + y, l[-100:]) / len(l[-100:])))\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**start_training.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the most important parts of this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openai_ros.task_envs.cartpole_stay_up import stay_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the line above, we are importing the Python file that contains the **Task Environment** that we want to use to learn. We'll have a look later at this Task Environment that we are importing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rospy.init_node('cartpole_gym', anonymous=True, log_level=rospy.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the line above, we are initializing our ROS node. Since we'll be working with Gazebo + ROS, we need to initialize a ROS node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPoleStayUp-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above line, we are creating the Gym environment that we imported before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rospack = rospkg.RosPack()\n",
    "pkg_path = rospack.get_path('cartpole_v0_training')\n",
    "outdir = pkg_path + '/training_results'\n",
    "env = wrappers.Monitor(env, outdir, force=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are basically indicating where to store the training results we'll get when the training finishes. In this case, it will be in the training_results folder, inside the cartpole_v0_training package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qlearn = qlearn.QLearn(actions=range(env.action_space.n),\n",
    "                       alpha=Alpha, gamma=Gamma, epsilon=Epsilon)\n",
    "initial_epsilon = qlearn.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are initializing the Qlearn algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in range(nepisodes):\n",
    "        \n",
    "        cumulated_reward = 0  \n",
    "        done = False\n",
    "        if qlearn.epsilon > 0.05:\n",
    "            qlearn.epsilon *= epsilon_discount\n",
    "        \n",
    "        observation = env.reset()\n",
    "        state = ''.join(map(str, observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are starting the main training loop. It will be executed once for each step we do during the training. It basically resets the environment, and gets the first state of the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(nsteps):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next loop is the steps loop. For each episode, x number of steps will be executed. There are several interesting things here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action = qlearn.chooseAction(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above line, the Qlearn algorithm chooses the next action to take, given the current state of the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we execute the step, providing the action that will take place. After the execution of the step, the environment will return four variables: **observation, reward, done, and info**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **observation**: It gives information about the status of the robot, using its sensors. What it will exactly return will be defined in the Task Environment.\n",
    "\n",
    "* **reward**: It gives a reward for the action taken. How this reward will be calculated will also be defined in the Task Environment.\n",
    "\n",
    "* **done**: This variable tells if the step has finished or not. You can consider that a step has finished, for instance, when a certain condition is met. You will also define this variable in the Task Environment.\n",
    "\n",
    "* **info**: It provides some extra information regarding the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qlearn.learn(state, action, reward, nextState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above line, we are actually making the Qlearn algorithm learn. We provide the initial state, the action taken in the step and its reward, and the state of the robot after the step to Qlearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MOST IMPORTANT THING** you need to understand from the training script is that it is totally independent from the environments. The purpose of this training script is to set up the learning algorithm that you want to use in order to make your agent learn, regardless of what is being done in the environments. This means that **you can change the algorithm you use to learn in the training script without having to worry about modifying your environment's struture**. And this is very powerful! Let's test it with the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 1.1**</p>\n",
    "<br>\n",
    "\n",
    "a) In your workspace, create a new package called **my_cartpole_training**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catkin_create_pkg my_cartpole_training rospy openai_ros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Inside the package, create two new folders called **launch** and **config**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Inside the launch folder, create a new file named **start_training.launch**. You can copy the following contents into it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**start_training.launch**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<launch>\n",
    "    <rosparam command=\"load\" file=\"$(find my_cartpole_training)/config/cartpole_qlearn_params.yaml\" />\n",
    "    <!-- Launch the training system -->\n",
    "    <node pkg=\"my_cartpole_training\" name=\"cartpole_gym\" type=\"start_training.py\" output=\"screen\"/> -->\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**start_training.launch**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the launch file is quite self-explanatory. What you are doing is, first, loading some parameters that Qlearn needs in order to train, and second, launching the training script that you've reviewed in the above section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) The next step will be to create this parameters file for Qlearn. Inside the **config** folder, create a new file named **cartpole_qlearn_params.yaml**. You can copy the following contents into it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**cartpole_qlearn_params.yaml**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartpole_v0: #namespace\n",
    "\n",
    "    n_actions: 4\n",
    "    control_type: \"velocity\"\n",
    "    \n",
    "    #environment variables\n",
    "    min_pole_angle: -0.7 #-23°\n",
    "    max_pole_angle: 0.7 #23°\n",
    "    \n",
    "    max_base_velocity: 50\n",
    "    max_base_pose_x: 2.5\n",
    "    min_base_pose_x: -2.5\n",
    "    \n",
    "    # those parameters are very important. They are affecting the learning experience\n",
    "    # They indicate how fast the control can be\n",
    "    # If the running step is too large, then there will be a long time between 2 ctrl commans\n",
    "    # If the pos_step is too large, then the changes in position will be very abrupt\n",
    "    running_step: 0.04 # amount of time the control will be executed\n",
    "    pos_step: 0.016     # increment in position for each command\n",
    "    \n",
    "    #qlearn parameters\n",
    "    alpha: 0.5\n",
    "    gamma: 0.9\n",
    "    epsilon: 0.1\n",
    "    epsilon_discount: 0.999\n",
    "    nepisodes: 1000\n",
    "    nsteps: 1000\n",
    "    number_splits: 10 #set to change the number of state splits for the continuous problem and also the number of env_variable splits\n",
    "\n",
    "    init_pos: 0.0 # Position in which the base will start\n",
    "    wait_time: 0.1 # Time to wait in the reset phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**cartpole_qlearn_params.yaml**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we are just setting some parameters that we'll need for our training. These parameters will first be loaded into the ROS parameter server, and later retrieved by our Training Environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e) Now, you will need to place a file that defines the Qlearn algorithm inside the **src** folder of your package. Create a file named **qlearn.py** here, and place the following contents into it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**qlearn.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Q-learning approach for different RL problems\n",
    "as part of the basic series on reinforcement learning @\n",
    "https://github.com/vmayoral/basic_reinforcement_learning\n",
    " \n",
    "Inspired by https://gym.openai.com/evaluations/eval_kWknKOkPQ7izrixdhriurA\n",
    " \n",
    "        @author: Victor Mayoral Vilches <victor@erlerobotics.com>\n",
    "'''\n",
    "import random\n",
    "\n",
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon, alpha, gamma):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha      # discount constant\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        '''\n",
    "        Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))            \n",
    "        '''\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state, return_q=False):\n",
    "        q = [self.getQ(state, a) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))] \n",
    "            maxQ = max(q)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        # In case there're several state-action max values \n",
    "        # we select a random one among them\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "\n",
    "        action = self.actions[i]        \n",
    "        if return_q: # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**qlearn.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file basically defines the Qlearn algorithm. It is imported into our training script in the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import qlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "f) Finally, also inside the **src** folder of your package, you'll have to place the **start_training.py** script that we've been reviewing. You will need to modify the name of the package where to store the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkg_path = rospack.get_path('my_cartpole_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to also create the **training_results** folder inside your package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd my_cartpole_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) And that's it! Now you have created a package in order to reproduce the demo you executed in the first step. You can run the launch file and see how the CartPole starts to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**End of Exercise 1.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 1.2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Now, let's change the learning algorithm that we use for learning. In this case, instead of the Qlearn algorithm, we'll use the sarsa algorithm. Make the necessary changes to your package in order to use the sarsa algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you have the file that defines the sarsa learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**sarsa.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Sarsa:\n",
    "    def __init__(self, actions, epsilon=0.1, alpha=0.2, gamma=0.9):\n",
    "        self.q = {}\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward \n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.getQ(state, a) for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2, action2):\n",
    "        qnext = self.getQ(state2, action2)\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma * qnext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**sarsa.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b) Launch your Training script again and check if you see any differences between the performances of Qlearn and sarsa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**End of Exercise 1.2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code, you saw that we were starting an environment called **CartPoleStayUp-v0**. But what is this environment for? What does it do?\n",
    "\n",
    "An environment is, basically, a problem (like the CartPole) with a minimal interface that an agent (a robot) can interact with. The environments in the OpenAI Gym are designed to allow objective testing and bench-marking of an agent's abilities.\n",
    "\n",
    "In basic OpenAI systems, you can usually define everything with just one environment. But this is a more complex situation, since we want to integrate OpenAI with ROS and Gazebo. We could probably define everything in just one environment, but it would be a complete mess, and the code would be incomprehensible.\n",
    "\n",
    "That's why we have created a structure to organize and split these environments in the same way every time, so that it is easier to understand and to work with them. This structure divides the OpenAI environment into three different types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Task Environment**: This environment is dedicated to implementing all the functions that a specific training session will use. For instance, using the same Robot Environment, you could create different Task Environments in order to train your robot in different tasks, or using different methods (actions).\n",
    "\n",
    "\n",
    "* **Robot Environment**: This environment is dedicated to implementing all the functions that a specific robot will use during its training. It will not include, though, the specific tasks that depend on the training that you are going to implement.\n",
    "\n",
    "\n",
    "\n",
    "* **Gazebo Environment**: This environment is common for any training or robot that you want to implement. This environment will generate all the connections between your robot and Gazebo, so that you don't have to worry about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important that you understand that this environment structure will always be the same, regardless of the problem you want to solve or the learning algorithm that you want to use. Also, these three environments will be connected to each other in the sense that each environment will inherit from the previous one. It goes like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Task Environment** inherits from the **Robot Environment**.\n",
    "\n",
    "\n",
    "* **Robot Environment** inherits from the **Gazebo Environment**.\n",
    "\n",
    "\n",
    "* **Gazebo Environment** inherits from the **Gym Environment**. The Gym Environment (gym.Env) is the most basic Environment structure provided by OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tons of information, right? But don't worry too much, since we'll have an extensive look at all of these environments. For now, let's focus on the Task Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Task Environment is the highest one in the inheritance structure, which also means that it is the most specific one. This environent will fully depend on the kind of training we want to make.\n",
    "\n",
    "Let's use the CartPole example again. Let's imagine that we have a common Robot Environment for the CartPole, but now we want to train it in two different ways. The first one will try to make the Pole stay up on the Cart, by only using two actions: move the cart to the right, and move the cart to the left. The second one will also try to make the Pole stay up on the Cart, but this time using four different actions: move the cart to the right, move the cart to the left, and again the same two previous actions, but applying a higher velocity to the cart so that it moves faster. Also, you can change the rewards given to each action, which will impact the way the agent learns.\n",
    "\n",
    "This differences in the training method will be specified in this Task Environment. Next, you can see an example of a Training Environment that tries to make the Pole stay up on the Cart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\" id=\"stay-up\">**stay_up.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym import utils\n",
    "from openai_ros.robot_envs import cartpole_env\n",
    "from gym.envs.registration import register\n",
    "from gym import error, spaces\n",
    "import rospy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# The path is __init__.py of openai_ros, where we import the CartPoleStayUpEnv directly\n",
    "register(\n",
    "        id='CartPoleStayUp-v0',\n",
    "        entry_point='openai_ros:CartPoleStayUpEnv',\n",
    "        timestep_limit=1000,\n",
    "    )\n",
    "\n",
    "class CartPoleStayUpEnv(cartpole_env.CartPoleEnv):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.get_params()\n",
    "        \n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        \n",
    "        cartpole_env.CartPoleEnv.__init__(\n",
    "            self, control_type=self.control_type\n",
    "            )\n",
    "            \n",
    "    def get_params(self):\n",
    "        #get configuration parameters\n",
    "        self.n_actions = rospy.get_param('/cartpole_v0/n_actions')\n",
    "        self.min_pole_angle = rospy.get_param('/cartpole_v0/min_pole_angle')\n",
    "        self.max_pole_angle = rospy.get_param('/cartpole_v0/max_pole_angle')\n",
    "        self.max_base_velocity = rospy.get_param('/cartpole_v0/max_base_velocity')\n",
    "        self.min_base_pose_x = rospy.get_param('/cartpole_v0/min_base_pose_x')\n",
    "        self.max_base_pose_x = rospy.get_param('/cartpole_v0/max_base_pose_x')\n",
    "        self.pos_step = rospy.get_param('/cartpole_v0/pos_step')\n",
    "        self.running_step = rospy.get_param('/cartpole_v0/running_step')\n",
    "        self.init_pos = rospy.get_param('/cartpole_v0/init_pos')\n",
    "        self.wait_time = rospy.get_param('/cartpole_v0/wait_time')\n",
    "        self.control_type = rospy.get_param('/cartpole_v0/control_type')\n",
    "        \n",
    "    def _set_action(self, action):\n",
    "        \n",
    "        # Take action\n",
    "        if action == 0: #LEFT\n",
    "            rospy.loginfo(\"GO LEFT...\")\n",
    "            self.pos[0] -= self.pos_step\n",
    "        elif action == 1: #RIGHT\n",
    "            rospy.loginfo(\"GO RIGHT...\")\n",
    "            self.pos[0] += self.pos_step\n",
    "        elif action == 2: #LEFT BIG\n",
    "            rospy.loginfo(\"GO LEFT BIG...\")\n",
    "            self.pos[0] -= self.pos_step * 10\n",
    "        elif action == 3: #RIGHT BIG\n",
    "            rospy.loginfo(\"GO RIGHT BIG...\")\n",
    "            self.pos[0] += self.pos_step * 10\n",
    "            \n",
    "            \n",
    "        # Apply action to simulation.\n",
    "        rospy.loginfo(\"MOVING TO POS==\"+str(self.pos))\n",
    "\n",
    "        # 1st: unpause simulation\n",
    "        rospy.logdebug(\"Unpause SIM...\")\n",
    "        self.gazebo.unpauseSim()\n",
    "\n",
    "        self.move_joints(self.pos)\n",
    "        rospy.logdebug(\"Wait for some time to execute movement, time=\"+str(self.running_step))\n",
    "        rospy.sleep(self.running_step) #wait for some time\n",
    "        rospy.logdebug(\"DONE Wait for some time to execute movement, time=\" + str(self.running_step))\n",
    "\n",
    "        # 3rd: pause simulation\n",
    "        rospy.logdebug(\"Pause SIM...\")\n",
    "        self.gazebo.pauseSim()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \n",
    "        data = self.joints\n",
    "        #       base_postion                base_velocity              pole angle                 pole velocity\n",
    "        obs = [round(data.position[1],1), round(data.velocity[1],1), round(data.position[0],1), round(data.velocity[0],1)]\n",
    "\n",
    "        return obs\n",
    "        \n",
    "    def _is_done(self, observations):\n",
    "        done = False\n",
    "        data = self.joints\n",
    "        \n",
    "        rospy.loginfo(\"BASEPOSITION==\"+str(observations[0]))\n",
    "        rospy.loginfo(\"POLE ANGLE==\" + str(observations[2]))\n",
    "        if (self.min_base_pose_x >= observations[0] or observations[0] >= self.max_base_pose_x): #check if the base is still within the ranges of (-2, 2)\n",
    "            rospy.logerr(\"Base Outside Limits==>min=\"+str(self.min_base_pose_x)+\",pos=\"+str(observations[0])+\",max=\"+str(self.max_base_pose_x))\n",
    "            done = True\n",
    "        if (self.min_pole_angle >= observations[2] or observations[2] >= self.max_pole_angle): #check if pole has toppled over\n",
    "            rospy.logerr(\n",
    "                \"Pole Angle Outside Limits==>min=\" + str(self.min_pole_angle) + \",pos=\" + str(observations[2]) + \",max=\" + str(\n",
    "                    self.max_pole_angle))\n",
    "            done = True\n",
    "            \n",
    "        return done\n",
    "        \n",
    "    def _compute_reward(self, observations, done):\n",
    "\n",
    "        \"\"\"\n",
    "        Gives more points for staying upright, gets data from given observations to avoid\n",
    "        having different data than other previous functions\n",
    "        :return:reward\n",
    "        \"\"\"\n",
    "        \n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warning(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    def _init_env_variables(self):\n",
    "        \"\"\"\n",
    "        Inits variables needed to be initialised each time we reset at the start\n",
    "        of an episode.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.steps_beyond_done = None\n",
    "        \n",
    "    def _set_init_pose(self):\n",
    "        \"\"\"\n",
    "        Sets joints to initial position [0,0,0]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        self.check_publishers_connection()\n",
    "        \n",
    "        # Reset Internal pos variable\n",
    "        self.init_internal_vars(self.init_pos)\n",
    "        self.move_joints(self.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**stay_up.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, as you may have already guessed, this environment will inherit from the previous one, the Robot Environment (in this case, it is called CartPoleEnv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CartPoleStayUpEnv(cartpole_env.CartPoleEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initialization of this Robot Environment, we will also need to pass its required variables to the Robot Environment: in this case, it's the **control_type**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartpole_env.CartPoleEnv.__init__(self, control_type=self.control_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, in the initialization of the Task Environment class, we are defining the action space of our environment. This is, basically, the number of actions (**self.n_actions**) that the robot will be able to take. This number will be taken from the ROS Parameter Server, and it is loaded from a YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.action_space = spaces.Discrete(self.n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at the different functions that we have in this Task Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_params(self):\n",
    "        \n",
    "        self.n_actions = rospy.get_param('/cartpole_v0/n_actions')\n",
    "        self.min_pole_angle = rospy.get_param('/cartpole_v0/min_pole_angle')\n",
    "        self.max_pole_angle = rospy.get_param('/cartpole_v0/max_pole_angle')\n",
    "        self.max_base_velocity = rospy.get_param('/cartpole_v0/max_base_velocity')\n",
    "        self.min_base_pose_x = rospy.get_param('/cartpole_v0/min_base_pose_x')\n",
    "        self.max_base_pose_x = rospy.get_param('/cartpole_v0/max_base_pose_x')\n",
    "        self.pos_step = rospy.get_param('/cartpole_v0/pos_step')\n",
    "        self.running_step = rospy.get_param('/cartpole_v0/running_step')\n",
    "        self.init_pos = rospy.get_param('/cartpole_v0/init_pos')\n",
    "        self.wait_time = rospy.get_param('/cartpole_v0/wait_time')\n",
    "        self.control_type = rospy.get_param('/cartpole_v0/control_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **get_params(self)** function, gets all the parameters needed by Qlearn, which are loaded from a YAML file, and stores them into variables of the environment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _set_action(self, action):\n",
    "        \n",
    "        # Take action\n",
    "        if action == 0: #LEFT\n",
    "            rospy.loginfo(\"GO LEFT...\")\n",
    "            self.pos[0] -= self.pos_step\n",
    "        elif action == 1: #RIGHT\n",
    "            rospy.loginfo(\"GO RIGHT...\")\n",
    "            self.pos[0] += self.pos_step\n",
    "        elif action == 2: #LEFT BIG\n",
    "            rospy.loginfo(\"GO LEFT BIG...\")\n",
    "            self.pos[0] -= self.pos_step * 10\n",
    "        elif action == 3: #RIGHT BIG\n",
    "            rospy.loginfo(\"GO RIGHT BIG...\")\n",
    "            self.pos[0] += self.pos_step * 10\n",
    "            \n",
    "            \n",
    "        # Apply action to simulation.\n",
    "        rospy.loginfo(\"MOVING TO POS==\"+str(self.pos))\n",
    "\n",
    "        # 1st: unpause simulation\n",
    "        rospy.logdebug(\"Unpause SIM...\")\n",
    "        self.gazebo.unpauseSim()\n",
    "\n",
    "        self.move_joints(self.pos)\n",
    "        rospy.logdebug(\"Wait for some time to execute movement, time=\"+str(self.running_step))\n",
    "        rospy.sleep(self.running_step) #wait for some time\n",
    "        rospy.logdebug(\"DONE Wait for some time to execute movement, time=\" + str(self.running_step))\n",
    "\n",
    "        # 3rd: pause simulation\n",
    "        rospy.logdebug(\"Pause SIM...\")\n",
    "        self.gazebo.pauseSim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **_set_action(self, action)** function defines what each action will do. Each action will be represented by an integer between 0 and 3. And depending on the action number that this function receives, the CartPole will do one thing or another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_obs(self):\n",
    "        \n",
    "        data = self.joints\n",
    "        #       base_postion                base_velocity              pole angle                 pole velocity\n",
    "        obs = [round(data.position[1],1), round(data.velocity[1],1), round(data.position[0],1), round(data.velocity[0],1)]\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **_get_obs(self)** function returns an array with all the data we need from the robot sensors in order to learn. In this case, we are getting the position and velocity of the Cart, and the angle and velocity of the Pole, which is the most relevant data to have on whether the agent is doing well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _is_done(self, observations):\n",
    "        done = False\n",
    "        data = self.joints\n",
    "        \n",
    "        rospy.loginfo(\"BASEPOSITION==\"+str(observations[0]))\n",
    "        rospy.loginfo(\"POLE ANGLE==\" + str(observations[2]))\n",
    "        if (self.min_base_pose_x >= observations[0] or observations[0] >= self.max_base_pose_x): #check if the base is still within the ranges of (-2, 2)\n",
    "            rospy.logerr(\"Base Outside Limits==>min=\"+str(self.min_base_pose_x)+\",pos=\"+str(observations[0])+\",max=\"+str(self.max_base_pose_x))\n",
    "            done = True\n",
    "        if (self.min_pole_angle >= observations[2] or observations[2] >= self.max_pole_angle): #check if pole has toppled over\n",
    "            rospy.logerr(\n",
    "                \"Pole Angle Outside Limits==>min=\" + str(self.min_pole_angle) + \",pos=\" + str(observations[2]) + \",max=\" + str(\n",
    "                    self.max_pole_angle))\n",
    "            done = True\n",
    "            \n",
    "        return done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **_is_done(self, observations)** function decides whether the current step has finished or not, and returns a variable (boolean) that says so. For the CartPole case, we say the step has finished either when the Cart is close to the bar limits or when the angle of the Pole is too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _compute_reward(self, observations, done):\n",
    "\n",
    "        \"\"\"\n",
    "        Gives more points for staying upright, gets data from given observations to avoid\n",
    "        having different data than other previous functions\n",
    "        :return:reward\n",
    "        \"\"\"\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warning(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **_compute_reward(self, observations, done)** function decides the reward to give for each step. In this specific case of the CartPole, it will give one reward point for each step (**steps_beyond_done**) that the Pole is upright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _init_env_variables(self):\n",
    "        \"\"\"\n",
    "        Inits variables needed to be initialised each time we reset at the start\n",
    "        of an episode.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.steps_beyond_done = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **_init_env_variables(self)** function initializes all the variables that your particular system is using after each step. In the CartPole case, as you can see, we are initializing the **steps_beyond_done** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _set_init_pose(self):\n",
    "        \"\"\"\n",
    "        Sets joints to initial position [0,0,0]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        self.check_publishers_connection()\n",
    "        \n",
    "        # Reset Internal pos variable\n",
    "        self.init_internal_vars(self.init_pos)\n",
    "        self.move_joints(self.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **_set_init_pose(self)** function sets the initial position of the joints of your robot. In this case, the initial position will be with the Cart parallel to the rail, and the Pole perpendicular to them at an angle of 90º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important that you understand that you can add all the functions that you want into this Task Environment. However, there are some functions that will **ALWAYS** need to be defined here because they are required for the rest of the Environments to work properly. These functions are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **_set_action()** function\n",
    "* The **_get_obs()** function\n",
    "* The **_is_done()** function\n",
    "* The **_compute_reward()** function\n",
    "* The **_set_init_pose()** function\n",
    "* The **_init_env_variables()** function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for your specific task, you don't need to implement some of these functions, you can just make them **pass** (like we do in this case with the _set_init_pose() and _init_env_variables() functions). But you will need to add them to your code anyway, or you will get a **NotImplementedError()**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 1.3**</p>\n",
    "<br>\n",
    "\n",
    "a) Inside **src** folder of the **my_cartpole_training** package, add a new script called **my_cartpole_task_env.py**. Into this new file, copy the contents of the <a href=\"#stay-up\">**stay_up.py**</a> script that you have just reviewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now, modify the Task Environment code so that it takes only two actions: move right and move left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Also, you will need to modify the registration id of the environment, in order to avoid conflicts. You just need to change the **v0** for a **v1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "register(\n",
    "        id='CartPoleStayUp-v1',\n",
    "        entry_point='my_cartpole_task_env:CartPoleStayUpEnv',\n",
    "        timestep_limit=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Next, you'll need to also modify the file **start_training.py**, so that it takes your Task Environment file insted of the default one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import my_cartpole_task_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, remember to modify the name of the Environment, which you have updated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPoleStayUp-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Finally, launch the training scrcipt and check out how the modifications affect the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**End of Exercise 1.3**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 1.4**</p>\n",
    "<br>\n",
    "\n",
    "a) Modify the Task Environment so that it now gives more reward for going to the left than for going to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**End of Exercise 1.4**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot environment will then contain all the functions associated with the specific \"robot\" that you want to train. This means that it will contain all the functionalities that your robot will need in order to be controlled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let's focus on the CartPole example. Let's say that in order to be able to control a CartPole environment, we will basically need three things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A way to check that all the systems (ROS topics, Publishers, etc.) are ready and working okay\n",
    "\n",
    "* A way to move the Cart alongside the rails\n",
    "\n",
    "* A way to get all the data about all the sensors that we want to evaluate (including the position of the Pole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this case, then, we would need to define all these functions into the Robot environment. Let's see an example of a Robot Environment script that takes into account the three points introduced above. You can check out the code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\" id=\"cartpole-env\">**cartpole_env.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import gym\n",
    "import rospy\n",
    "import roslaunch\n",
    "import time\n",
    "import numpy as np\n",
    "from gym import utils, spaces\n",
    "from geometry_msgs.msg import Twist\n",
    "from std_srvs.srv import Empty\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import register\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "\n",
    "from sensor_msgs.msg import JointState\n",
    "from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\n",
    "from std_msgs.msg import Float64\n",
    "from gazebo_msgs.srv import SetLinkState\n",
    "from gazebo_msgs.msg import LinkState\n",
    "from rosgraph_msgs.msg import Clock\n",
    "from openai_ros import robot_gazebo_env\n",
    "\n",
    "class CartPoleEnv(robot_gazebo_env.RobotGazeboEnv):\n",
    "    def __init__(\n",
    "        self, control_type\n",
    "    ):\n",
    "        \n",
    "        self.publishers_array = []\n",
    "        self._base_pub = rospy.Publisher('/cartpole_v0/foot_joint_velocity_controller/command', Float64, queue_size=1)\n",
    "        self._pole_pub = rospy.Publisher('/cartpole_v0/pole_joint_velocity_controller/command', Float64, queue_size=1)\n",
    "        self.publishers_array.append(self._base_pub)\n",
    "        self.publishers_array.append(self._pole_pub)\n",
    "        \n",
    "        rospy.Subscriber(\"/cartpole_v0/joint_states\", JointState, self.joints_callback)\n",
    "        \n",
    "        self.control_type = control_type\n",
    "        if self.control_type == \"velocity\":\n",
    "            self.controllers_list = ['joint_state_controller',\n",
    "                                    'pole_joint_velocity_controller',\n",
    "                                    'foot_joint_velocity_controller',\n",
    "                                    ]\n",
    "                                    \n",
    "        elif self.control_type == \"position\":\n",
    "            self.controllers_list = ['joint_state_controller',\n",
    "                                    'pole_joint_position_controller',\n",
    "                                    'foot_joint_position_controller',\n",
    "                                    ]\n",
    "                                    \n",
    "        elif self.control_type == \"effort\":\n",
    "            self.controllers_list = ['joint_state_controller',\n",
    "                                    'pole_joint_effort_controller',\n",
    "                                    'foot_joint_effort_controller',\n",
    "                                    ]\n",
    "\n",
    "        self.robot_name_space = \"cartpole_v0\"\n",
    "        self.reset_controls = True\n",
    "\n",
    "        # Seed the environment\n",
    "        self._seed()\n",
    "        self.steps_beyond_done = None\n",
    "        \n",
    "        super(CartPoleEnv, self).__init__(\n",
    "            controllers_list=self.controllers_list,\n",
    "            robot_name_space=self.robot_name_space,\n",
    "            reset_controls=self.reset_controls\n",
    "            )\n",
    "\n",
    "    def joints_callback(self, data):\n",
    "        self.joints = data\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "        \n",
    "    # RobotEnv methods\n",
    "    # ----------------------------\n",
    "\n",
    "    def _env_setup(self, initial_qpos):\n",
    "        self.init_internal_vars(self.init_pos)\n",
    "        self.set_init_pose()\n",
    "        self.check_all_systems_ready()\n",
    "        \n",
    "    def init_internal_vars(self, init_pos_value):\n",
    "        self.pos = [init_pos_value]\n",
    "        self.joints = None\n",
    "        \n",
    "    def check_publishers_connection(self):\n",
    "        \"\"\"\n",
    "        Checks that all the publishers are working\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        rate = rospy.Rate(10)  # 10hz\n",
    "        while (self._base_pub.get_num_connections() == 0 and not rospy.is_shutdown()):\n",
    "            rospy.logdebug(\"No susbribers to _base_pub yet so we wait and try again\")\n",
    "            try:\n",
    "                rate.sleep()\n",
    "            except rospy.ROSInterruptException:\n",
    "                # This is to avoid error when world is rested, time when backwards.\n",
    "                pass\n",
    "        rospy.logdebug(\"_base_pub Publisher Connected\")\n",
    "\n",
    "        while (self._pole_pub.get_num_connections() == 0 and not rospy.is_shutdown()):\n",
    "            rospy.logdebug(\"No susbribers to _pole_pub yet so we wait and try again\")\n",
    "            try:\n",
    "                rate.sleep()\n",
    "            except rospy.ROSInterruptException:\n",
    "                # This is to avoid error when world is rested, time when backwards.\n",
    "                pass\n",
    "        rospy.logdebug(\"_pole_pub Publisher Connected\")\n",
    "\n",
    "        rospy.logdebug(\"All Publishers READY\")\n",
    "\n",
    "    def _check_all_systems_ready(self, init=True):\n",
    "        self.base_position = None\n",
    "        while self.base_position is None and not rospy.is_shutdown():\n",
    "            try:\n",
    "                self.base_position = rospy.wait_for_message(\"/cartpole_v0/joint_states\", JointState, timeout=1.0)\n",
    "                rospy.logdebug(\"Current cartpole_v0/joint_states READY=>\"+str(self.base_position))\n",
    "                if init:\n",
    "                    # We Check all the sensors are in their initial values\n",
    "                    positions_ok = all(abs(i) <= 1.0e-02 for i in self.base_position.position)\n",
    "                    velocity_ok = all(abs(i) <= 1.0e-02 for i in self.base_position.velocity)\n",
    "                    efforts_ok = all(abs(i) <= 1.0e-01 for i in self.base_position.effort)\n",
    "                    base_data_ok = positions_ok and velocity_ok and efforts_ok\n",
    "                    rospy.logdebug(\"Checking Init Values Ok=>\" + str(base_data_ok))\n",
    "            except:\n",
    "                rospy.logerr(\"Current cartpole_v0/joint_states not ready yet, retrying for getting joint_states\")\n",
    "        rospy.logdebug(\"ALL SYSTEMS READY\")\n",
    "        \n",
    "            \n",
    "    def move_joints(self, joints_array):\n",
    "        joint_value = Float64()\n",
    "        joint_value.data = joints_array[0]\n",
    "        rospy.logdebug(\"Single Base JointsPos>>\"+str(joint_value))\n",
    "        self._base_pub.publish(joint_value)\n",
    "\n",
    "        \n",
    "    def get_clock_time(self):\n",
    "        self.clock_time = None\n",
    "        while self.clock_time is None and not rospy.is_shutdown():\n",
    "            try:\n",
    "                self.clock_time = rospy.wait_for_message(\"/clock\", Clock, timeout=1.0)\n",
    "                rospy.logdebug(\"Current clock_time READY=>\" + str(self.clock_time))\n",
    "            except:\n",
    "                rospy.logdebug(\"Current clock_time not ready yet, retrying for getting Current clock_time\")\n",
    "        return self.clock_time\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**cartpole_env.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can differentiate two main parts in the CartPoleEnv class: the initialization of the class and the specific functions defined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you can see that this class inherits from the RobotGazeboEnv class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CartPoleEnv(robot_gazebo_env.RobotGazeboEnv):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that all the functions that are defined in the Gazebo Environment will also be accessible from this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **\\__init\\__()** function of the class, we are doing several things. First, we are creating all the Publishers and Subscribers required for this specific Cartpole environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.publishers_array = []\n",
    "self._base_pub = rospy.Publisher('/cartpole_v0/foot_joint_velocity_controller/command', Float64, queue_size=1)\n",
    "self._pole_pub = rospy.Publisher('/cartpole_v0/pole_joint_velocity_controller/command', Float64, queue_size=1)\n",
    "self.publishers_array.append(self._base_pub)\n",
    "self.publishers_array.append(self._pole_pub)\n",
    "\n",
    "rospy.Subscriber(\"/cartpole_v0/joint_states\", JointState, self.joints_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the Publishers needed to move the Cart alongside the rails, and the Subscribers needed to know the positions of the joints at any given time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.control_type = control_type\n",
    "if self.control_type == \"velocity\":\n",
    "    self.controllers_list = ['joint_state_controller',\n",
    "                            'pole_joint_velocity_controller',\n",
    "                            'foot_joint_velocity_controller',\n",
    "                            ]\n",
    "\n",
    "elif self.control_type == \"position\":\n",
    "    self.controllers_list = ['joint_state_controller',\n",
    "                            'pole_joint_position_controller',\n",
    "                            'foot_joint_position_controller',\n",
    "                            ]\n",
    "\n",
    "elif self.control_type == \"effort\":\n",
    "    self.controllers_list = ['joint_state_controller',\n",
    "                            'pole_joint_effort_controller',\n",
    "                            'foot_joint_effort_controller',\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are defining which kind of controllers we will use for our joints: velocity, position, or effort controllers. We decide this depending on the contents of the control_type variable, which has been defined in the Task Environment. Depending on this variable, then, we will fill the value of the **controllers_list** variable, which needs to be passed to the Gazebo Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside the **controllers_list** variable, we will need to pass two other variables to the Gazebo Environment: **robot_name_space** and **reset_controls**. This is done here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "super(CartPoleEnv, self).__init__(\n",
    "            controllers_list=self.controllers_list,\n",
    "            robot_name_space=self.robot_name_space,\n",
    "            reset_controls=self.reset_controls\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the **robot_name_space** and **reset_controls** variables are assigned right above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.robot_name_space = \"cartpole_v0\"\n",
    "self.reset_controls = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is basically it for the initialization of the environment. Now, let's have a look at some of the most important functions of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joints_callback(self, data):\n",
    "        self.joints = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **joints_callback(self, data)** function gets the data from the Subscriber of the topic **/joint_states** and stores it in the **self.joints** class variable. If you remember from the previous section, this self.joints class variable will be used in order to generate the observation (inside the _get_obs() function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_publishers_connection(self):\n",
    "        \"\"\"\n",
    "        Checks that all the publishers are working\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        rate = rospy.Rate(10)  # 10hz\n",
    "        while (self._base_pub.get_num_connections() == 0 and not rospy.is_shutdown()):\n",
    "            rospy.logdebug(\"No susbribers to _base_pub yet so we wait and try again\")\n",
    "            try:\n",
    "                rate.sleep()\n",
    "            except rospy.ROSInterruptException:\n",
    "                # This is to avoid error when world is rested, time when backwards.\n",
    "                pass\n",
    "        rospy.logdebug(\"_base_pub Publisher Connected\")\n",
    "\n",
    "        while (self._pole_pub.get_num_connections() == 0 and not rospy.is_shutdown()):\n",
    "            rospy.logdebug(\"No susbribers to _pole_pub yet so we wait and try again\")\n",
    "            try:\n",
    "                rate.sleep()\n",
    "            except rospy.ROSInterruptException:\n",
    "                # This is to avoid error when world is rested, time when backwards.\n",
    "                pass\n",
    "        rospy.logdebug(\"_pole_pub Publisher Connected\")\n",
    "\n",
    "        rospy.logdebug(\"All Publishers READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **check_publishers_connection(self)** basically checks that the Publishers are connected and working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _check_all_systems_ready(self, init=True):\n",
    "        self.base_position = None\n",
    "        while self.base_position is None and not rospy.is_shutdown():\n",
    "            try:\n",
    "                self.base_position = rospy.wait_for_message(\"/cartpole_v0/joint_states\", JointState, timeout=1.0)\n",
    "                rospy.logdebug(\"Current cartpole_v0/joint_states READY=>\"+str(self.base_position))\n",
    "                if init:\n",
    "                    # We Check all the sensors are in their initial values\n",
    "                    positions_ok = all(abs(i) <= 1.0e-02 for i in self.base_position.position)\n",
    "                    velocity_ok = all(abs(i) <= 1.0e-02 for i in self.base_position.velocity)\n",
    "                    efforts_ok = all(abs(i) <= 1.0e-01 for i in self.base_position.effort)\n",
    "                    base_data_ok = positions_ok and velocity_ok and efforts_ok\n",
    "                    rospy.logdebug(\"Checking Init Values Ok=>\" + str(base_data_ok))\n",
    "            except:\n",
    "                rospy.logerr(\"Current cartpole_v0/joint_states not ready yet, retrying for getting joint_states\")\n",
    "        rospy.logdebug(\"ALL SYSTEMS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **_check_all_systems_ready(self, init=True)** function checks that all the joints are at their initial values before starting each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_joints(self, joints_array):\n",
    "        joint_value = Float64()\n",
    "        joint_value.data = joints_array[0]\n",
    "        rospy.logdebug(\"Single Base JointsPos>>\"+str(joint_value))\n",
    "        self._base_pub.publish(joint_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the **move_joints(self, joints_array)** functions are in charge of moving the joints (the Cart, in this case) of the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important that you understand that you can add all the functions that you want to this Robot Environment. However, there are some functions that will **ALWAYS** need to be defined here because they are required for the rest of the Environments to work properly. These functions include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **_check_all_systems_ready()** function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gazebo Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I've said before, the Gazebo Environment is mainly used to connect the simulated environment to the Gazebo simulator. For instance, it takes care of the resets of the simulator after each step, or the resets of the controllers (if needed).\n",
    "\n",
    "The most important thing you need to know about this environment is that it **will be transparent** to you. And what does this mean? Well, it basically means that you don't have to worry about it. This environment **will always be the same**, regardless of the robot or the kind of training you want to perform. So, you won't have to change it or work over it. Good news, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can have a look at how this environment looks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**robot_gazebo_env.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rospy\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from .gazebo_connection import GazeboConnection\n",
    "from .controllers_connection import ControllersConnection\n",
    "#https://bitbucket.org/theconstructcore/theconstruct_msgs/src/master/msg/RLExperimentInfo.msg\n",
    "from theconstruct_msgs.msg import RLExperimentInfo\n",
    "\n",
    "# https://github.com/openai/gym/blob/master/gym/core.py\n",
    "class RobotGazeboEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, robot_name_space, controllers_list, reset_controls):\n",
    "\n",
    "        # To reset Simulations\n",
    "        self.gazebo = GazeboConnection()\n",
    "        self.controllers_object = ControllersConnection(namespace=robot_name_space, controllers_list=controllers_list)\n",
    "        self.reset_controls = reset_controls\n",
    "        self.seed()\n",
    "\n",
    "        # Set up ROS related variables\n",
    "        self.episode_num = 0\n",
    "        self.reward_pub = rospy.Publisher('/openai/reward', RLExperimentInfo, queue_size=1)\n",
    "\n",
    "    # Env methods\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Function executed each time step.\n",
    "        Here we get the action execute it in a time step and retrieve the\n",
    "        observations generated by that action.\n",
    "        :param action:\n",
    "        :return: obs, reward, done, info\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Here we should convert the action num to movement action, execute the action in the\n",
    "        simulation and get the observations result of performing that action.\n",
    "        \"\"\"\n",
    "        self.gazebo.unpauseSim()\n",
    "        self._set_action(action)\n",
    "        self.gazebo.pauseSim()\n",
    "        obs = self._get_obs()\n",
    "        done = self._is_done(obs)\n",
    "        info = {}\n",
    "        reward = self._compute_reward(obs, done)\n",
    "        self._publish_reward_topic(reward, self.episode_num)\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        rospy.logdebug(\"Reseting RobotGazeboEnvironment\")\n",
    "        self._reset_sim()\n",
    "        self._init_env_variables()\n",
    "        self._update_episode()\n",
    "        obs = self._get_obs()\n",
    "        return obs\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Function executed when closing the environment.\n",
    "        Use it for closing GUIS and other systems that need closing.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        rospy.logdebug(\"Closing RobotGazeboEnvironment\")\n",
    "        rospy.signal_shutdown(\"Closing RobotGazeboEnvironment\")\n",
    "\n",
    "    def _update_episode(self):\n",
    "        \"\"\"\n",
    "        Increases the episode number by one\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.episode_num += 1\n",
    "\n",
    "    def _publish_reward_topic(self, reward, episode_number=1):\n",
    "        \"\"\"\n",
    "        This function publishes the given reward in the reward topic for\n",
    "        easy access from ROS infrastructure.\n",
    "        :param reward:\n",
    "        :param episode_number:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        reward_msg = RLExperimentInfo()\n",
    "        reward_msg.episode_number = episode_number\n",
    "        reward_msg.episode_reward = reward\n",
    "        self.reward_pub.publish(reward_msg)\n",
    "\n",
    "    # Extension methods\n",
    "    # ----------------------------\n",
    "\n",
    "    def _reset_sim(self):\n",
    "        \"\"\"Resets a simulation\n",
    "        \"\"\"\n",
    "        if self.reset_controls :\n",
    "            self.gazebo.unpauseSim()\n",
    "            self.controllers_object.reset_controllers()\n",
    "            self._check_all_systems_ready()\n",
    "            self._set_init_pose()\n",
    "            self.gazebo.pauseSim()\n",
    "            self.gazebo.resetSim()\n",
    "            self.gazebo.unpauseSim()\n",
    "            self.controllers_object.reset_controllers()\n",
    "            self._check_all_systems_ready()\n",
    "            self.gazebo.pauseSim()\n",
    "            \n",
    "        else:\n",
    "            self.gazebo.unpauseSim()\n",
    "            \n",
    "            self._check_all_systems_ready()\n",
    "            self._set_init_pose()\n",
    "            self.gazebo.pauseSim()\n",
    "            self.gazebo.resetSim()\n",
    "            self.gazebo.unpauseSim()\n",
    "            \n",
    "            self._check_all_systems_ready()\n",
    "            self.gazebo.pauseSim()\n",
    "        \n",
    "\n",
    "        return True\n",
    "\n",
    "    def _set_init_pose(self):\n",
    "        \"\"\"Sets the Robot in its init pose\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _check_all_systems_ready(self):\n",
    "        \"\"\"\n",
    "        Checks that all the sensors, publishers and other simulation systems are\n",
    "        operational.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Returns the observation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _init_env_variables(self):\n",
    "        \"\"\"Inits variables needed to be initialised each time we reset at the start\n",
    "        of an episode.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _set_action(self, action):\n",
    "        \"\"\"Applies the given action to the simulation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _is_done(self, observations):\n",
    "        \"\"\"Indicates whether or not the episode is done ( the robot has fallen for example).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _compute_reward(self, observations, done):\n",
    "        \"\"\"Calculates the reward to give based on the observations given.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _env_setup(self, initial_qpos):\n",
    "        \"\"\"Initial configuration of the environment. Can be used to configure initial state\n",
    "        and extract information from the simulation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**robot_gazebo_env.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the **RobotGazeboEnv** class, some of the most important functions are, for instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **step() function**: It defines what to do at each step of the training.\n",
    "\n",
    "* **reset() function**: It defines how to reset the simulation after each step of the training.\n",
    "\n",
    "* **close() function**: It closes the whole ROS system.\n",
    "\n",
    "* **publish_reward_topic() function**: It publishes the reward for each step in the topic **/openai/reward**, for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing you need to know about this environment is that it requires three variables, as you can see in its definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __init__(self, robot_name_space, controllers_list, reset_controls):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **robot_name_space**: A string with the namespace of our robot.\n",
    "\n",
    "* **controllers_list**: An array containing the names of the controllers of the robot.\n",
    "\n",
    "* **reset_controls**: A boolean that tells whether or not to reset the controllers of the robot each time we reset the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when creating the Robot Environment, bear in mind that you'll need to fill these three variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But also, within this environment, we have some functions that are defined, but not implemented (they raise a NotImplementedError()). These functions need to be implemented either in the Robot Environment or in the Task Environment. **This is MANDATORY**. These functions are:\n",
    "\n",
    "* **_set_init_pose()**\n",
    "\n",
    "* **_check_all_systems_ready()**\n",
    "\n",
    "* **_get_obs()**\n",
    "\n",
    "* **_init_env_variables()**\n",
    "\n",
    "* **_set_action()**\n",
    "\n",
    "* **_is_done()**\n",
    "\n",
    "* **_compute_reward()**\n",
    "\n",
    "* **_env_setup()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of them have already been explained in the previous Robot and Task Environments. But you can review the function of each one in the **robot_gazebo_env.py** file. \n",
    "\n",
    "Besides the functions introduced above, we also have the **_reset_sim()** function. Although this function is actually implemented, it can also be re-implemented in any of the other Environments. Let's say that within the Gazebo Environment we have a default implementation, which should work for most of the cases. But if you need to change the behavior of how your simulation resets, you can re-implement this function either in the Robot or the Task Environments, and they will override the one defined in the Gazebo Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 1.5**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Add a new script to the **src** folder of your **my_cartpole_training** package. You can name it, for instance, **my_cartpole_robot_env.py**. Into this new file, copy the contents of the <a href=\"#cartpole-env\">**cartpole_env.py**</a> script that you have just reviewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Next, you will need to make sure that the Task Environment is correctly importing and using your new Robot Environment file, **my_cartpole_robot_env.py**. Also, you will need to modify some references in the Task Environment to the old Robot Environment file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Add a function in the Robot Environment file that changes the way in which the CartPole simulation is reset. For instance, make it reset to a position where the Pole is at an angle of 30º in respect to the Cart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**End of Exercise 1.5**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in the previous section, where we discussed about the Gazebo Environment, this environment publishes the reward into a topic named **/openai/reward**. This is very important, because it will allow us to visualize our reward using regular ROS tools. For instance, **rqt_multiplot**. \n",
    "\n",
    "**rqt_multiplot** is similar to rqt_plot, since it also provides a GUI for visualizing 2D plots, but it's a little bit more complex and provides more features. For instance, you can visualize multiple plots at the same time, or you can also customize the axis for your plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next exercise, we are going to see how to visualize the reward of our training using the rqt_multiplot tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 1.6**</p>\n",
    "<br>\n",
    "\n",
    "a) First of all, let's start the rqt_multiplot tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun rqt_multiplot rqt_multiplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Hit the icon with a screen in the top-right corner of the IDE window \n",
    "<img src=\"img/font-awesome_desktop.png\"/> \n",
    "in order to open the Graphic Interface window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see something like this in this new window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rqt_multiplot1.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Next, let's launch our training script, so that we start getting the rewards published into the **/openai/reward** topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roslaunch cartpole_v0_training start_training.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Next, let's configure our plot. For that, first, you'll need to click on the **Configure plot** button, at the top-right side of the window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rqt_multiplot2.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Give a name to your new plot, and click on the **Add curve** icon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rqt_multiplot3.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Next, set the topic to **/openai/reward**. Automatically, the rest of the field will be autocompleted. For the X-Axis, select the **episode_number** field. For the Y-Axis, select the **episode_reward** field. Like in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rqt_multiplot4.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, just click on the **Enter** key on your keyboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Finally, in the plot visualization window, you will need to click on the **Run plot** button in order to start visualizing your new plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rqt_multiplot5.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went OK, you should visualize something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rqt_multiplot7.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save, if you want, your plot configuration. This way, you will be able to load it at any time later. Remember to save it into your **catkin_ws/src** workspace, so that it will also be saved when you close the Course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rqt_multiplot6.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**End of Exercise 1.6**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Congratulations!! You are now capable of creating your own Environments structure and make it learn using simple learning algorithms, like Qlearn."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
