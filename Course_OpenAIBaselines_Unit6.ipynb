{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI with ROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotignite_logo_text.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 6: How to change the learning algorithm: Cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time to completion: **2 h**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this unit, we are going to see how to set up the environment in order to be able to use the OpenAI Baselines deepq algorithm. For that, we are going to use a virtual environment. We are going to use the RoboCube environment during the Unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">END OF SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to only change the learning script from using **Qlearn** to use **DeepQ**. We won't change anything else. Here you will also see the power of environments. This allows you to change the learning algorith easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you already have a version of the CubeEnvironment to use with deepq:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**my_start_deepq.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import gym\n",
    "from baselines import deepq\n",
    "import rospy\n",
    "import rospkg\n",
    "#from openai_gazebo.cartpole_stay_up import stay_up\n",
    "import my_one_disk_walk\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    rospy.init_node('movingcube_onedisk_walk_gym', anonymous=True, log_level=rospy.WARN)\n",
    "    \n",
    "    # Set the path where learned model will be saved\n",
    "    rospack = rospkg.RosPack()\n",
    "    pkg_path = rospack.get_path('my_moving_cube_pkg')\n",
    "    models_dir_path = os.path.join(pkg_path, \"models_saved\")\n",
    "    if not os.path.exists(models_dir_path):\n",
    "        os.makedirs(models_dir_path)\n",
    "    \n",
    "    out_model_file_path = os.path.join(models_dir_path, \"movingcube_model.pkl\") \n",
    "    \n",
    "    \n",
    "    max_timesteps = rospy.get_param(\"/moving_cube/max_timesteps\")\n",
    "    buffer_size = rospy.get_param(\"/moving_cube/buffer_size\")\n",
    "    # We convert to float becase if we are using Ye-X notation, it sometimes treats it like a string.\n",
    "    lr = float(rospy.get_param(\"/moving_cube/lr\"))\n",
    "    \n",
    "    exploration_fraction = rospy.get_param(\"/moving_cube/exploration_fraction\")\n",
    "    exploration_final_eps = rospy.get_param(\"/moving_cube/exploration_final_eps\")\n",
    "    print_freq = rospy.get_param(\"/moving_cube/print_freq\")\n",
    "    \n",
    "    reward_task_learned = rospy.get_param(\"/moving_cube/reward_task_learned\")\n",
    "    \n",
    "    \n",
    "    def callback(lcl, _glb):\n",
    "        # stop training if reward exceeds 199\n",
    "        aux1 = lcl['t'] > 100\n",
    "        aux2 = sum(lcl['episode_rewards'][-101:-1]) / 100\n",
    "        is_solved = aux1 and aux2 >= reward_task_learned\n",
    "        \n",
    "        rospy.logdebug(\"aux1=\"+str(aux1))\n",
    "        rospy.logdebug(\"aux2=\"+str(aux2))\n",
    "        rospy.logdebug(\"reward_task_learned=\"+str(reward_task_learned))\n",
    "        rospy.logdebug(\"IS SOLVED?=\"+str(is_solved))\n",
    "        \n",
    "        return is_solved\n",
    "    \n",
    "    env = gym.make(\"MyMovingCubeOneDiskWalkEnv-v0\")\n",
    "    \n",
    "    \n",
    "    model = deepq.models.mlp([64])\n",
    "    act = deepq.learn(\n",
    "        env,\n",
    "        q_func=model,\n",
    "        lr=lr, \n",
    "        max_timesteps=max_timesteps, \n",
    "        buffer_size=buffer_size,\n",
    "        exploration_fraction=exploration_fraction,\n",
    "        exploration_final_eps=exploration_final_eps,\n",
    "        print_freq=print_freq, # how many apisodes until you print total rewards and info\n",
    "        param_noise=False,\n",
    "        callback=callback\n",
    "    )\n",
    "    \n",
    "    env.close()\n",
    "    rospy.logwarn(\"Saving model to movingcube_model.pkl\")\n",
    "    act.save(out_model_file_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**END my_start_deepq.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are loading all the parameters from the param server, therefore, we will have to create a new config file to be loaded when launching this deepq script. Here you have it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**my_one_disk_walk_openai_params_deepQ.yaml**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moving_cube: #namespace\n",
    "\n",
    "    \n",
    "    # DeepQ Parameters\n",
    "    max_timesteps: 1000 # Maximum time steps of all the steps done throughout all the episodes\n",
    "    buffer_size: 500 # size of the replay buffer\n",
    "    lr: 1e-3 # learning rate for adam optimizer\n",
    "    exploration_fraction: 0.1 # fraction of entire training period over which the exploration rate is annealed\n",
    "    exploration_final_eps: 0.02 # final value of random action probability\n",
    "    print_freq: 1 # how often (Ex: 1 means every episode, 2 every two episode we print ) to print out training progress set to None to disable printing\n",
    "    \n",
    "    reward_task_learned: 10000\n",
    "    \n",
    "    # Learning General Parameters\n",
    "    n_actions: 5 # We have 3 actions\n",
    "    n_observations: 6 # We have 6 different observations\n",
    "\n",
    "    speed_step: 1.0 # Time to wait in the reset phases\n",
    "\n",
    "    init_roll_vel: 0.0 # Initial speed of the Roll Disk\n",
    "\n",
    "    roll_speed_fixed_value: 100.0 # Speed at which it will move forwards or backwards\n",
    "    roll_speed_increment_value: 10.0 # Increment that could be done in each step\n",
    "\n",
    "    max_distance: 2.0 # Maximum distance allowed for the RobotCube\n",
    "    max_pitch_angle: 0.2 # Maximum Angle radians in Pitch that we allow before terminating episode\n",
    "    max_yaw_angle: 0.1 # Maximum yaw angle deviation, after that it starts getting negative rewards\n",
    "\n",
    "    max_y_linear_speed: 100 # Free fall get to around 30, so we triple it \n",
    "\n",
    "    init_cube_pose:\n",
    "      x: 0.0\n",
    "      y: 0.0\n",
    "      z: 0.0\n",
    "\n",
    "    end_episode_points: 1000 # Points given when ending an episode\n",
    "\n",
    "    move_distance_reward_weight: 1000.0 # Multiplier for the moved distance reward, Ex: inc_d = 0.1 --> 100points\n",
    "    y_linear_speed_reward_weight: 1000.0 # Multiplier for moving fast in the y Axis\n",
    "    y_axis_angle_reward_weight: 1000.0 # Multiplier of angle of yaw, to keep it straight\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**END my_one_disk_walk_openai_params_deepQ.yaml**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just comment on the **deepq** related parameters; the other ones are exactly the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DeepQ Parameters\n",
    "max_timesteps: 1000 # Maximum time steps of all the steps done throughout all the episodes\n",
    "buffer_size: 500 # size of the replay buffer\n",
    "lr: 1e-3 # learning rate for adam optimizer\n",
    "exploration_fraction: 0.1 # fraction of entire training period over which the exploration rate is annealed\n",
    "exploration_final_eps: 0.02 # final value of random action probability\n",
    "print_freq: 1 # how often (Ex: 1 means every episode, 2 every two episode we print ) to print out training progress set to None to disable printing\n",
    "\n",
    "reward_task_learned: 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_timesteps**: This parameter allows you to set the maximum **TOTAL** steps throughout the entire learning process. It doesn't matter if you do 1000 steps in 1000 episodes, or in one episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**print_freq**: We define how many episodes until we print the average reward and other learning info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reward_task_learned**: When the sum of all the rewards in that episode reach this value, it will consider the robot to have learned the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this model has many more options, so please feel free to experiment. Here you have all the parameter options from the deepQ class:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Parameters\n",
    "    -------\n",
    "    env: gym.Env\n",
    "        environment to train on\n",
    "    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n",
    "        the model that takes the following inputs:\n",
    "            observation_in: object\n",
    "                the output of observation placeholder\n",
    "            num_actions: int\n",
    "                number of actions\n",
    "            scope: str\n",
    "            reuse: bool\n",
    "                should be passed to outer variable scope\n",
    "        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n",
    "    lr: float\n",
    "        learning rate for adam optimizer\n",
    "    max_timesteps: int\n",
    "        number of env steps to optimizer for\n",
    "    buffer_size: int\n",
    "        size of the replay buffer\n",
    "    exploration_fraction: float\n",
    "        fraction of entire training period over which the exploration rate is annealed\n",
    "    exploration_final_eps: float\n",
    "        final value of random action probability\n",
    "    train_freq: int\n",
    "        update the model every `train_freq` steps.\n",
    "        set to None to disable printing\n",
    "    batch_size: int\n",
    "        size of a batched sampled from replay buffer for training\n",
    "    print_freq: int\n",
    "        how often to print out training progress\n",
    "        set to None to disable printing\n",
    "    checkpoint_freq: int\n",
    "        how often to save the model. This is so that the best version is restored\n",
    "        at the end of the training. If you do not wish to restore the best version at\n",
    "        the end of the training set this variable to None.\n",
    "    learning_starts: int\n",
    "        how many steps of the model to collect transitions for before learning starts\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    target_network_update_freq: int\n",
    "        update the target network every `target_network_update_freq` steps.\n",
    "    prioritized_replay: True\n",
    "        if True prioritized replay buffer will be used.\n",
    "    prioritized_replay_alpha: float\n",
    "        alpha parameter for prioritized replay buffer\n",
    "    prioritized_replay_beta0: float\n",
    "        initial value of beta for prioritized replay buffer\n",
    "    prioritized_replay_beta_iters: int\n",
    "        number of iterations over which beta will be annealed from initial value\n",
    "        to 1.0. If set to None equals to max_timesteps.\n",
    "    prioritized_replay_eps: float\n",
    "        epsilon to add to the TD errors when updating priorities.\n",
    "    callback: (locals, globals) -> None\n",
    "        function called at every steps with state of the algorithm.\n",
    "        If callback returns true training stops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we have to follow the same steps you took for the **CartPole** to setup the **Baselines,** if you haven't done it yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Up the Baselines for DeepQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Python3_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you have to do is create a **Python 3** workspace. This is due to the fact that ROS doesn't directly support **Python 3**. This means that if you launch programs for ROS with libraries for Python 3, it will most likely give an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd\n",
    "source /opt/ros/kinetic/setup.bash\n",
    "mkdir -p ~/python3_ws/src\n",
    "cd ~/python3_ws/\n",
    "catkin_make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Baselines Git Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will have to move your code to the **python3_ws**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have it, let's downlad the baselines git and setup the **Virtual Environment,** which uses, among other things, Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd ~/python3_ws/src\n",
    "git clone https://github.com/openai/baselines.git\n",
    "cd baselines\n",
    "virtualenv venv --python=python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We activate the Virtual Environment:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source venv/bin/activate\n",
    "# for deactivating : deactivate simply, nothing else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we install all the dependencies needed, within the safe confines of our Virtual Environment:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install -e .\n",
    "pip install pytest\n",
    "# If you want to test instalation, just type: pytest\n",
    "pip install pyyaml rospkg catkin_pkg exception defusedxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now everything is ready to start using deepq!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile ROS TF for Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using these ROS libraries in our code, we will need to compile them in our **Virtual Environment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ~/python3_ws/src;cd baselines\n",
    "source venv/bin/activate\n",
    "\n",
    "pip install catkin_pkg pyyaml empy rospkg numpy\n",
    "cd ~/python3_ws/src\n",
    "git clone https://github.com/ros/geometry\n",
    "git clone https://github.com/ros/geometry2\n",
    "cd ~/python3_ws\n",
    "rm -rf build devel\n",
    "catkin_make --force-cmake\n",
    "source devel/setup.bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that you are using **Python 3** and that everythng works, execute the following in an **ipython** interpreter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that the tf2 works with Python 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(venv) user:~/catkin_ws$ python\n",
    "Python 3.5.2 (default, Nov 23 2017, 16:37:01)\n",
    "[GCC 5.4.0 20160609] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import tf\n",
    ">>> tf.__file__\n",
    "'/home/user/catkin_ws/devel/lib/python3/dist-packages/tf/__init__.py'\n",
    ">>> import tf2_py\n",
    ">>> tf2_py.__file__\n",
    "'/home/user/catkin_ws/devel/lib/python3/dist-packages/tf2_py/__init__.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should give you an output like this one, using Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to get inside the VENV every time you open a new shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to do this very often, so here are the commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ~/python3_ws/src/baselines\n",
    "source venv/bin/activate\n",
    "cd ~/python3_ws\n",
    "source devel/setup.bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start the Learning script using DeepQ for the MyMovingCubeOneDiskWalkEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just have to create a new launch file that launches the deepq Python script, instead of the Qlearn learning script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**my_start_training_deepq_version.launch**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<launch>\n",
    "    <rosparam command=\"load\" file=\"$(find my_moving_cube_pkg)/config/my_one_disk_walk_openai_params_deepQ.yaml\" />\n",
    "    <!-- Launch the training system -->\n",
    "    <node pkg=\"my_moving_cube_pkg\" name=\"movingcube_gym\" type=\"my_start_deepq.py\" output=\"screen\"/>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we didn't change much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**END my_start_training_deepq_version.launch**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the learning, get inside the venv and launch the deepq launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ~/python3_ws/src/baselines\n",
    "source venv/bin/activate\n",
    "cd ~/python3_ws\n",
    "source devel/setup.bash\n",
    "roslaunch my_moving_cube_pkg my_start_training_deepq_version.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You should get something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cube_deepQ.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the total steps done reaches the maximum, you have established or reached the reward for the task to be considered learned. Now, the system will automatically create a **pickle** file for you, with the model of the learned task inside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#FF5E00;color:white;\">**EXTRA exercise U3.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the parameter so that:\n",
    "* It spends more time learning\n",
    "* It stops learning when it reaches a lower reward average\n",
    "* It prints the learning information less frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#FF5E00;color:white;\">**END EXTRA exercise U3.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And that's it! You now know how to reuse Task Environments for different learning algorithms."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
