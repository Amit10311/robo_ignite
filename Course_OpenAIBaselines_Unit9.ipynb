{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Baselines MicroCourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotignite_logo_text.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 4: Project: Training a Hopper robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopper Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the project, you will use the Hopper robot. The Hopper robot is a one-legged robot whose biggest challenge is being able to stand up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hopper.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks to accomplish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create all the environments needed in order to be able to train the Hopper robot. First, generate the Robot Environment. You can start with the below templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**hopper_env.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openai_ros import robot_gazebo_env\n",
    "\n",
    "\n",
    "class HopperEnv(robot_gazebo_env.RobotGazeboEnv):\n",
    "    \"\"\"Superclass for all Robot environments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a new Robot environment.\n",
    "        \"\"\"\n",
    "        # Variables that we give through the constructor.\n",
    "\n",
    "        # Internal Vars\n",
    "        self.controllers_list = ['my_robot_controller1','my_robot_controller2', ..., 'my_robot_controllerX']\n",
    "\n",
    "        self.robot_name_space = \"my_robot_namespace\"\n",
    "\n",
    "        reset_controls_bool = True or False\n",
    "        \n",
    "        # We launch the init function of the Parent Class robot_gazebo_env.RobotGazeboEnv\n",
    "        \n",
    "        super(MyRobotEnv, self).__init__(controllers_list=self.controllers_list,\n",
    "                                                robot_name_space=self.robot_name_space,\n",
    "                                                reset_controls=reset_controls_bool)\n",
    "\n",
    "    # Methods needed by the RobotGazeboEnv\n",
    "    # ----------------------------\n",
    "    \n",
    "    \n",
    "\n",
    "    def _check_all_systems_ready(self):\n",
    "        \"\"\"\n",
    "        Checks that all the sensors, publishers and other simulation systems are\n",
    "        operational.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return True\n",
    "    \n",
    "    # Methods that the TrainingEnvironment will need to define here as virtual\n",
    "    # because they will be used in RobotGazeboEnv GrandParentClass and defined in the\n",
    "    # TrainingEnvironment.\n",
    "    # ----------------------------\n",
    "    def _set_init_pose(self):\n",
    "        \"\"\"Sets the Robot in its init pose\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def _init_env_variables(self):\n",
    "        \"\"\"Inits variables needed to be initialized each time we reset at the start\n",
    "        of an episode.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _compute_reward(self, observations, done):\n",
    "        \"\"\"Calculates the reward to give based on the observations given.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _set_action(self, action):\n",
    "        \"\"\"Applies the given action to the simulation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _is_done(self, observations):\n",
    "        \"\"\"Checks if episode done based on observations given.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    # Methods that the TrainingEnvironment will need.\n",
    "    # ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate the Task Environment. You can start with the below template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**hopper_stay_up_env.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import hopper_env\n",
    "from gym.envs.registration import register\n",
    "import rospy\n",
    "\n",
    "# The path is __init__.py of openai_ros, where we import the MovingCubeOneDiskWalkEnv directly\n",
    "timestep_limit_per_episode = 1000 # Can be any Value\n",
    "\n",
    "register(\n",
    "        id='HopperStayUp-v0',\n",
    "        entry_point='hopper_stay_up_env:HopperStayUpEnv',\n",
    "        timestep_limit=timestep_limit_per_episode,\n",
    "    )\n",
    "\n",
    "class HopperStayUpEnv(hopper_env.HopperEnv):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Only variable needed to be set here\n",
    "        number_actions = rospy.get_param('/my_robot_namespace/n_actions')\n",
    "        self.action_space = spaces.Discrete(number_actions)\n",
    "        \n",
    "        # This is the most common case of Box observation type\n",
    "        high = numpy.array([\n",
    "            obs1_max_value,\n",
    "            obs12_max_value,\n",
    "            ...\n",
    "            obsN_max_value\n",
    "            ])\n",
    "            \n",
    "        self.observation_space = spaces.Box(-high, high)\n",
    "        \n",
    "        # Variables that we retrieve through the param server, loded when launch training launch.\n",
    "        \n",
    "\n",
    "\n",
    "        # Here we will add any init functions prior to starting the MyRobotEnv\n",
    "        super(MyTrainingEnv, self).__init__()\n",
    "\n",
    "\n",
    "    def _set_init_pose(self):\n",
    "        \"\"\"Sets the Robot in its init pose\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def _init_env_variables(self):\n",
    "        \"\"\"\n",
    "        Inits variables needed to be initialised each time we reset at the start\n",
    "        of an episode.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    def _set_action(self, action):\n",
    "        \"\"\"\n",
    "        Move the robot based on the action variable given\n",
    "        \"\"\"\n",
    "        # TODO: Move robot\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Here we define what sensor data of our robots observations\n",
    "        To know which Variables we have access to, we need to read the\n",
    "        MyRobotEnv API DOCS\n",
    "        :return: observations\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return observations\n",
    "\n",
    "    def _is_done(self, observations):\n",
    "        \"\"\"\n",
    "        Decide if episode is done based on the observations\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return done\n",
    "\n",
    "    def _compute_reward(self, observations, done):\n",
    "        \"\"\"\n",
    "        Return the reward based on the observations given\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return reward\n",
    "        \n",
    "    # Internal TaskEnv Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a training script that will use the Qlearn algorithm in order to train the Hopper robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create a training script that will use the deepq algorithm in order to train the Hopper robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compare the results between the two learning algorithms, and check which one perpforms better. Try to improve the larning script."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_metadata": {
   "chapter": "5 - Grasping",
   "chapter_title": "Unit 5. Grasping",
   "course_title": "ROS MANIPULATION IN 5 DAYS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
