{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5: People Perception Part 2. Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know how to detect different faces, but what if you want to recognise a familiar face? Well, this is what you will learn here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work with this scene: a Fetch robot and three different people that you will have to be able to recognise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "People Recognition World with Fetch Robot",
    "image": true,
    "name": "perception_unit5_threepersons",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/perception_unit5_threepersons.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start The Face Recognition Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, you will use the <a href=\"https://github.com/ageitgey/face_recognition\">face_recognition python module</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has many features apart from the FaceRecognition feature, like face finding or applying digital makeup. But it gives a very simple way of doing a rudimentary face recognition. Here you have an example of how to recognise **one face**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some elements to comment on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from sensor_msgs.msg import Image\n",
    "import rospkg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need these special packages, like **cv2** (OpenCV), **cv_bridge** (CV_Bridge), and **face_recognition** to be able to recognize the faces. You will also need **rospkg** in order to easily find files that are inside other ROS packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get an instance of RosPack with the default search paths\n",
    "rospack = rospkg.RosPack()\n",
    "# get the file path for face_recognition_pkg\n",
    "self.path_to_package = rospack.get_path('face_recognition_pkg')\n",
    "print self.path_to_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to **retrieve an image** of the person that you want to recognise. **It is important to note** that by giving only one image, the face recognition system won't make positive detections unless what it visualizes is very similar to the image provided. So, the more images you provide using different configurations, the better the detections will be. Anyways, one image is more than enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a sample picture and learn how to recognize it.\n",
    "image_path = os.path.join(self.path_to_package,\"scripts/standing_person.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Standing Person Face",
    "image": true,
    "name": "perception_unit5_standing_person2",
    "width": "7cm"
   },
   "source": [
    "<img src=\"img/perception_unit5_standing_person2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have that, it starts the real image processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standing_person_image = face_recognition.load_image_file(image_path)\n",
    "standing_person_face_encoding = face_recognition.face_encodings(standing_person_image)[0]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see that you load the image into a **face_recognition** variable, and then you extract the encodings. Note that you are getting the first encoding [0]. This is because you only have one image of that person. More images will generate more encodings.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise some variables and then we underscale the image retrieved from the cameras. In this case, half of the original size. That way, you work with less pixels, making the recognition process faster. But, when you show the images captured, you will show the original size.<br>\n",
    "But this has a side effect: if you reduce the size **too much,** the face encoder has less information to work with and, therefore, it makes it harder to extract human features and match them with someone.<br>\n",
    "It has to be a compromise between performance and functionality. If you are too far away from your subjects, then you will have to work with bigger images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Face Image Too Small or far away, loosing definition",
    "image": true,
    "name": "perception_unit5_smallbobeye",
    "width": "5cm"
   },
   "source": [
    "<img src=\"img/perception_unit5_smallbobeye.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an image this small, it will be difficult for the algorithm to work, so you will have to get closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "# Resize frame of video to 1/2 size for faster face recognition processing\n",
    "small_frame = cv2.resize(video_capture, (0, 0), fx=0.5, fy=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the image frame. Bear in mind that, here, you are processing one and not the next one. This, again, is to reduce the number of images to be processed, making the recognition faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only process every other frame of video to save time\n",
    "if process_this_frame:\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(small_frame)\n",
    "    face_encodings = face_recognition.face_encodings(small_frame, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        # See if the face is a match for the known face(s)\n",
    "        match = face_recognition.compare_faces([standing_person_face_encoding], face_encoding)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        if match[0]:\n",
    "            print \"MATCH\"\n",
    "            name = \"StandingPerson\"\n",
    "\n",
    "        face_names.append(name)\n",
    "\n",
    "process_this_frame = not process_this_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, extract the location of the faces. Then, from those locations, you extract the encodings. These encodings are like the \"fingerprint\" of each face, defining it. You do this for each image frame that you capture.<br>\n",
    "Then, for all the faces detected and, therefore, encodings, you **compare** those encodings with the standing_person_face_encoding. If there is a match, then you found the standing person, and you add that name to the face_names list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you display the results of the face recognition. This means showing the image captured and the locations where you detected a face, and draw a square with the name of the recognised face there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display the results\n",
    "for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "    # Scale back up face locations since the frame we detected in was scaled to 1/2 size\n",
    "    top *= 2\n",
    "    right *= 2\n",
    "    bottom *= 2\n",
    "    left *= 2\n",
    "\n",
    "    # Draw a box around the face\n",
    "    cv2.rectangle(video_capture, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "    # Draw a label with a name below the face\n",
    "    cv2.rectangle(video_capture, (left, bottom - 35), (right, bottom), (0, 0, 255))\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    cv2.putText(video_capture, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "# Display the resulting image\n",
    "cv2.imshow(\"Image window\", video_capture)\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this should be the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Face Recognition of Standing person and two unknown persons",
    "image": true,
    "name": "perception_unit5_threepeopleunknown",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/perception_unit5_threepeopleunknown.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the final code you would have, including all the parts explained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**recognise_face.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import rospy\n",
    "import cv2\n",
    "import face_recognition\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from sensor_msgs.msg import Image\n",
    "import rospkg\n",
    "\n",
    "\n",
    "\n",
    "class FaceRecogniser(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        rospy.loginfo(\"Start FaceRecogniser Init process...\")\n",
    "        # get an instance of RosPack with the default search paths\n",
    "        rospack = rospkg.RosPack()\n",
    "        # get the file path for face_recognition_pkg\n",
    "        self.path_to_package = rospack.get_path('face_recognition_pkg')\n",
    "\n",
    "        self.bridge_object = CvBridge()\n",
    "        rospy.loginfo(\"Start camera suscriber...\")\n",
    "        self.image_sub = rospy.Subscriber(\"/head_camera/rgb/image_raw\",Image,self.camera_callback)\n",
    "        rospy.loginfo(\"Finished FaceRecogniser Init process...Ready\")\n",
    "        \n",
    "    def camera_callback(self,data):\n",
    "        \n",
    "        self.recognise(data)\n",
    "\n",
    "    def recognise(self,data):\n",
    "        \n",
    "        # Get a reference to webcam #0 (the default one)\n",
    "        try:\n",
    "            # We select bgr8 because its the OpneCV encoding by default\n",
    "            video_capture = self.bridge_object.imgmsg_to_cv2(data, desired_encoding=\"bgr8\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "        # Load a sample picture and learn how to recognize it.\n",
    "        image_path = os.path.join(self.path_to_package,\"person_img/standing_person.png\")\n",
    "        \n",
    "        standing_person_image = face_recognition.load_image_file(image_path)\n",
    "        standing_person_face_encoding = face_recognition.face_encodings(standing_person_image)[0]\n",
    "        \n",
    "        \n",
    "        # Initialize some variables\n",
    "        face_locations = []\n",
    "        face_encodings = []\n",
    "        face_names = []\n",
    "        process_this_frame = True\n",
    "\n",
    "        # Resize frame of video to 1/2 size for faster face recognition processing\n",
    "        # If this is done be aware that you will have to make the recognition nearer.\n",
    "        # In this case it will work around maximum 1 meter, more it wont work properly\n",
    "        small_frame = cv2.resize(video_capture, (0, 0), fx=0.5, fy=0.5)\n",
    "        \n",
    "        #cv2.imshow(\"SMALL Image window\", small_frame)\n",
    "        \n",
    "        # Only process every other frame of video to save time\n",
    "        if process_this_frame:\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(small_frame, face_locations)\n",
    "    \n",
    "            if not face_encodings:\n",
    "                rospy.logwarn(\"No Faces found, please get closer...\")\n",
    "                \n",
    "            face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                # See if the face is a match for the known face(s)\n",
    "                match = face_recognition.compare_faces([standing_person_face_encoding], face_encoding)\n",
    "                name = \"Unknown\"\n",
    "    \n",
    "                if match[0]:\n",
    "                    rospy.loginfo(\"MATCH\")\n",
    "                    name = \"StandingPerson\"\n",
    "                else:\n",
    "                    rospy.logwarn(\"NO Match\")\n",
    "    \n",
    "                face_names.append(name)\n",
    "\n",
    "        process_this_frame = not process_this_frame\n",
    "    \n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 2\n",
    "            right *= 2\n",
    "            bottom *= 2\n",
    "            left *= 2\n",
    "    \n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(video_capture, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "    \n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(video_capture, (left, bottom - 35), (right, bottom), (0, 0, 255))\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(video_capture, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow(\"Image window\", video_capture)\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    rospy.init_node('face_recognising_python_node', anonymous=True)\n",
    "   \n",
    "    line_follower_object = FaceRecogniser()\n",
    "\n",
    "    rospy.spin()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**END recognise_face.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a package called **my_face_recogniser**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd;cd ../src\n",
    "catkin_create_pkg my_face_recogniser rospy sensor_msgs cv_bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to your package some example recognition images. You can get this images from the **person_img** folder of the <i>face_recognition_pkg</i> package. To copy this folder to your package, just execute the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd face_recognition_pkg/\n",
    "cp -r person_img /home/user/catkin_ws/src/my_face_recogniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need a script to move Fetch robot closer to the people. Here you can see an example script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**move_fetch_robot_client.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright (c) 2015, Fetch Robotics Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "#\n",
    "#     * Redistributions of source code must retain the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer.\n",
    "#     * Redistributions in binary form must reproduce the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer in the\n",
    "#       documentation and/or other materials provided with the distribution.\n",
    "#     * Neither the name of the Fetch Robotics Inc. nor the names of its\n",
    "#       contributors may be used to endorse or promote products derived from\n",
    "#       this software without specific prior written permission.\n",
    "# \n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "# ARE DISCLAIMED. IN NO EVENT SHALL FETCH ROBOTICS INC. BE LIABLE FOR ANY\n",
    "# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
    "# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
    "# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
    "# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n",
    "# THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "# Author: Michael Ferguson\n",
    "import numpy\n",
    "import copy\n",
    "import actionlib\n",
    "import rospy\n",
    "import time\n",
    "from math import sin, cos\n",
    "from moveit_python import (MoveGroupInterface,\n",
    "                           PlanningSceneInterface,\n",
    "                           PickPlaceInterface)\n",
    "from moveit_python.geometry import rotate_pose_msg_by_euler_angles\n",
    "\n",
    "from control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryGoal\n",
    "from control_msgs.msg import PointHeadAction, PointHeadGoal\n",
    "from geometry_msgs.msg import Twist, Pose\n",
    "from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\n",
    "from moveit_msgs.msg import PlaceLocation, MoveItErrorCodes\n",
    "from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\n",
    "from nav_msgs.msg import Odometry\n",
    "\n",
    "# Move base using navigation stack\n",
    "class MoveBasePublisher(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._move_base_publisher = rospy.Publisher('/base_controller/command', Twist, queue_size=1)\n",
    "        self.odom_subs = rospy.Subscriber(\"/odom\", Odometry, self.odom_callback)\n",
    "        self.actual_pose = Pose()\n",
    "    \n",
    "    def odom_callback(self,msg):\n",
    "        self.actual_pose = msg.pose.pose\n",
    "\n",
    "    def move_to(self, twist_object):\n",
    "        self._move_base_publisher.publish(twist_object)\n",
    "    \n",
    "    def stop(self):\n",
    "        twist_object = Twist()\n",
    "        self.move_to(twist_object)\n",
    "        \n",
    "    def move_forwards_backwards(self,speed_ms,position_x, allowed_error=0.1):\n",
    "        \"\"\"\n",
    "        distance_metres: positive is go forwards, negative go backwards\n",
    "        \"\"\"\n",
    "        rate = rospy.Rate(5)\n",
    "        move_twist = Twist()\n",
    "        \n",
    "        start_x = self.actual_pose.position.x\n",
    "        direction_speed = numpy.sign(position_x - start_x)\n",
    "        move_twist.linear.x = direction_speed*speed_ms\n",
    "        \n",
    "        \n",
    "        in_place = False\n",
    "        while not in_place:\n",
    "            self.move_to(move_twist)\n",
    "            x_actual = self.actual_pose.position.x\n",
    "            print x_actual\n",
    "\n",
    "            if  abs(position_x - x_actual) <= allowed_error:\n",
    "                print \"Reached position\"\n",
    "                break\n",
    "\n",
    "            rate.sleep()\n",
    "        self.stop()\n",
    "\n",
    "\n",
    "# Move base using navigation stack\n",
    "class MoveBaseClient(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = actionlib.SimpleActionClient(\"move_base\", MoveBaseAction)\n",
    "        rospy.loginfo(\"Waiting for move_base...\")\n",
    "        self.client.wait_for_server()\n",
    "\n",
    "    def goto(self, x, y, theta, frame=\"map\"):\n",
    "        move_goal = MoveBaseGoal()\n",
    "        move_goal.target_pose.pose.position.x = x\n",
    "        move_goal.target_pose.pose.position.y = y\n",
    "        move_goal.target_pose.pose.orientation.z = sin(theta/2.0)\n",
    "        move_goal.target_pose.pose.orientation.w = cos(theta/2.0)\n",
    "        move_goal.target_pose.header.frame_id = frame\n",
    "        move_goal.target_pose.header.stamp = rospy.Time.now()\n",
    "\n",
    "        # TODO wait for things to work\n",
    "        self.client.send_goal(move_goal)\n",
    "        self.client.wait_for_result()\n",
    "\n",
    "# Send a trajectory to controller\n",
    "class FollowTrajectoryClient(object):\n",
    "\n",
    "    def __init__(self, name, joint_names):\n",
    "        self.client = actionlib.SimpleActionClient(\"%s/follow_joint_trajectory\" % name,\n",
    "                                                   FollowJointTrajectoryAction)\n",
    "        rospy.loginfo(\"Waiting for %s...\" % name)\n",
    "        self.client.wait_for_server()\n",
    "        self.joint_names = joint_names\n",
    "\n",
    "    def move_to(self, positions, duration=5.0):\n",
    "        if len(self.joint_names) != len(positions):\n",
    "            print(\"Invalid trajectory position\")\n",
    "            return False\n",
    "        trajectory = JointTrajectory()\n",
    "        trajectory.joint_names = self.joint_names\n",
    "        trajectory.points.append(JointTrajectoryPoint())\n",
    "        trajectory.points[0].positions = positions\n",
    "        trajectory.points[0].velocities = [0.0 for _ in positions]\n",
    "        trajectory.points[0].accelerations = [0.0 for _ in positions]\n",
    "        trajectory.points[0].time_from_start = rospy.Duration(duration)\n",
    "        follow_goal = FollowJointTrajectoryGoal()\n",
    "        follow_goal.trajectory = trajectory\n",
    "\n",
    "        self.client.send_goal(follow_goal)\n",
    "        self.client.wait_for_result()\n",
    "\n",
    "# Point the head using controller\n",
    "class PointHeadClient(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = actionlib.SimpleActionClient(\"head_controller/point_head\", PointHeadAction)\n",
    "        rospy.loginfo(\"Waiting for head_controller...\")\n",
    "        self.client.wait_for_server()\n",
    "\n",
    "    def look_at(self, x, y, z, frame, duration=1.0):\n",
    "        goal = PointHeadGoal()\n",
    "        goal.target.header.stamp = rospy.Time.now()\n",
    "        goal.target.header.frame_id = frame\n",
    "        goal.target.point.x = x\n",
    "        goal.target.point.y = y\n",
    "        goal.target.point.z = z\n",
    "        goal.min_duration = rospy.Duration(duration)\n",
    "        self.client.send_goal(goal)\n",
    "        self.client.wait_for_result()\n",
    "\n",
    "# Tools for grasping\n",
    "class GraspingClient(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scene = PlanningSceneInterface(\"base_link\")\n",
    "        self.pickplace = PickPlaceInterface(\"arm\", \"gripper\", verbose=True)\n",
    "        self.move_group = MoveGroupInterface(\"arm\", \"base_link\")\n",
    "\n",
    "   \n",
    "    def getGraspableCube(self):\n",
    "        \n",
    "        # nothing detected\n",
    "        return None, None\n",
    "\n",
    "    def getSupportSurface(self, name):\n",
    "        for surface in self.support_surfaces:\n",
    "            if surface.name == name:\n",
    "                return surface\n",
    "        return None\n",
    "\n",
    "    def getPlaceLocation(self):\n",
    "        pass\n",
    "\n",
    "    def pick(self, block, grasps):\n",
    "        success, pick_result = self.pickplace.pick_with_retry(block.name,\n",
    "                                                              grasps,\n",
    "                                                              support_name=block.support_surface,\n",
    "                                                              scene=self.scene)\n",
    "        self.pick_result = pick_result\n",
    "        return success\n",
    "\n",
    "    def place(self, block, pose_stamped):\n",
    "        places = list()\n",
    "        l = PlaceLocation()\n",
    "        l.place_pose.pose = pose_stamped.pose\n",
    "        l.place_pose.header.frame_id = pose_stamped.header.frame_id\n",
    "\n",
    "        # copy the posture, approach and retreat from the grasp used\n",
    "        l.post_place_posture = self.pick_result.grasp.pre_grasp_posture\n",
    "        l.pre_place_approach = self.pick_result.grasp.pre_grasp_approach\n",
    "        l.post_place_retreat = self.pick_result.grasp.post_grasp_retreat\n",
    "        places.append(copy.deepcopy(l))\n",
    "        # create another several places, rotate each by 90 degrees in yaw direction\n",
    "        l.place_pose.pose = rotate_pose_msg_by_euler_angles(l.place_pose.pose, 0, 0, 1.57)\n",
    "        places.append(copy.deepcopy(l))\n",
    "        l.place_pose.pose = rotate_pose_msg_by_euler_angles(l.place_pose.pose, 0, 0, 1.57)\n",
    "        places.append(copy.deepcopy(l))\n",
    "        l.place_pose.pose = rotate_pose_msg_by_euler_angles(l.place_pose.pose, 0, 0, 1.57)\n",
    "        places.append(copy.deepcopy(l))\n",
    "\n",
    "        success, place_result = self.pickplace.place_with_retry(block.name,\n",
    "                                                                places,\n",
    "                                                                scene=self.scene)\n",
    "        return success\n",
    "\n",
    "    def tuck(self):\n",
    "        joints = [\"shoulder_pan_joint\", \"shoulder_lift_joint\", \"upperarm_roll_joint\",\n",
    "                  \"elbow_flex_joint\", \"forearm_roll_joint\", \"wrist_flex_joint\", \"wrist_roll_joint\"]\n",
    "        pose = [1.32, 1.40, -0.2, 1.72, 0.0, 1.66, 0.0]\n",
    "        while not rospy.is_shutdown():\n",
    "            result = self.move_group.moveToJointPosition(joints, pose, 0.02)\n",
    "            if result.error_code.val == MoveItErrorCodes.SUCCESS:\n",
    "                return\n",
    "\n",
    "\n",
    "\n",
    "def Move_to_Two_locations_demo():\n",
    "    # Create a node\n",
    "    rospy.init_node(\"move_to_two_locations_demo_node\")\n",
    "\n",
    "    # Make sure sim time is working\n",
    "    while not rospy.Time.now():\n",
    "        pass\n",
    "\n",
    "    # Setup clients\n",
    "    move_base = MoveBaseClient()\n",
    "    torso_action = FollowTrajectoryClient(\"torso_controller\", [\"torso_lift_joint\"])\n",
    "    head_action = PointHeadClient()\n",
    "    grasping_client = GraspingClient()\n",
    "    \n",
    "    rospy.loginfo(\"Start Sequence complete\")\n",
    "    \n",
    "    \n",
    "    # Move the base to be in front of the table\n",
    "    # Demonstrates the use of the navigation stack\n",
    "    rospy.loginfo(\"Moving to table...\")\n",
    "    move_base.goto(2.250, 3.118, 0.0)\n",
    "    move_base.goto(2.750, 3.118, 0.0)\n",
    "    \n",
    "    # Raise the torso using just a controller\n",
    "    rospy.loginfo(\"Raising torso...\")\n",
    "    torso_action.move_to([0.4, ])\n",
    "\n",
    "    # Point the head at the cube we want to pick\n",
    "    head_action.look_at(3.7, 3.18, 0.0, \"map\")\n",
    "\n",
    "    # Tuck the arm\n",
    "    grasping_client.tuck()\n",
    "\n",
    "    # Lower torso\n",
    "    rospy.loginfo(\"Lowering torso...\")\n",
    "    torso_action.move_to([0.0, ])\n",
    "\n",
    "    # Move to second table\n",
    "    rospy.loginfo(\"Moving to second table...\")\n",
    "    move_base.goto(-3.53, 3.75, 1.57)\n",
    "    move_base.goto(-3.53, 4.15, 1.57)\n",
    "\n",
    "    # Raise the torso using just a controller\n",
    "    rospy.loginfo(\"Raising torso...\")\n",
    "    torso_action.move_to([0.4, ])\n",
    "\n",
    "    # Tuck the arm, lower the torso\n",
    "    grasping_client.tuck()\n",
    "    torso_action.move_to([0.0, ])\n",
    "\n",
    "\n",
    "def Face_Recognition_demo():\n",
    "    # Create a node\n",
    "    rospy.init_node(\"face_recognition_demo_node\")\n",
    "\n",
    "    # Make sure sim time is working\n",
    "    while not rospy.Time.now():\n",
    "        pass\n",
    "\n",
    "    # Setup clients\n",
    "    # Navigation doesnt work very well \n",
    "    \n",
    "    \n",
    "    \n",
    "    move_base_pub = MoveBasePublisher()\n",
    "    torso_action = FollowTrajectoryClient(\"torso_controller\", [\"torso_lift_joint\"])\n",
    "    head_action = PointHeadClient()\n",
    "    grasping_client = GraspingClient()\n",
    "    \n",
    "    rospy.loginfo(\"Start Sequence complete\")\n",
    "    \n",
    "    rospy.loginfo(\"Move to position...\")\n",
    "    move_base_pub.move_forwards_backwards(speed_ms=0.3,position_x=1.0, allowed_error=0.1)\n",
    "    \n",
    "    # Raise the torso using just a controller\n",
    "    rospy.loginfo(\"Raising torso...\")\n",
    "    torso_action.move_to([0.4, ])\n",
    "    \n",
    "    # Point the head at the cube we want to pick\n",
    "    #rospy.loginfo(\"1,0,-1\")\n",
    "    #head_action.look_at(0.0, 0.0, 2.0, \"map\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Face_Recognition_demo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**END move_fetch_robot_client.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise U5-1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Move the Fetch robot close to the people and raise its torso, using the following command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun face_recognition_tc move_fetch_robot_client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Execute the **recognise_face.py** script and check how it works. Rotate the Fetch robot so that it can try to recognise all 3 people in the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once you have checked that it only works with one person (which is called **standing person**), try to also be able to recognise the other two people. This means, taking a picture, and also modifying what you think has to be changed so that it now recognises all 3 people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**Data for Exercise U5-1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remember to launch the Graphical Interface window in order to visualize the detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/font-awesome_desktop.png\" width=\"30\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When trying to recognise the faces, you will see that only one of them is recognised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "<img src=\"img/recognise1.png\" width=\"250\" style=\"float: left; margin: 0px 0px 15px 15px;\"/>\n",
    "<img src=\"img/recognise2.png\" width=\"250\" style=\"float: left; margin: 0px 0px 15px 60px;\"/>\n",
    "<img src=\"img/recognise3.png\" width=\"250\" style=\"float: left; margin: 0px 0px 15px 60px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a face is successfulyy recognised, you will get a message like this in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[INFO] [WallTime: 1517396015.764194] [432.655000] MATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When face is not correctly recognised, you will get this instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[WARN] [WallTime: 1517397392.869854] [606.444000] NO Match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**END Data for Exercise U5-1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise U5-1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Face Detection at the Same Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise U5-2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following:<br>\n",
    "\n",
    "* Create a new script that is able to recognise all three people at the same time.\n",
    "* Use the below images for doing the face recognition. You can store them in the **person_img** folder in your package **my_face_recogniser**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bob:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Bob",
    "image": true,
    "name": "perception_unit4_bobeye",
    "width": "8cm"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>\n",
    "    <figure>\n",
    "      <img id=\"fig-A.1\" src=\"img/perception_unit4_bobeye.png\" width=\"300\"/>\n",
    "       <center> <figcaption><h2>Bob</h2></figcaption></center>\n",
    "    </figure>\n",
    "\n",
    "    </th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naoko:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Naoko",
    "image": true,
    "name": "perception_unit4_naoko",
    "width": "8cm"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>\n",
    "        <figure>\n",
    "      <img id=\"fig-A.2\" src=\"img/perception_unit4_naoko.png\" width=\"300\"/>\n",
    "       <center> <figcaption><h2>Naoko</h2></figcaption></center>\n",
    "    </figure>\n",
    "    </th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standing person:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Standing Person",
    "image": true,
    "name": "perception_unit5_standing_person2",
    "width": "8cm"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "        \n",
    "    <th>\n",
    "        <figure>\n",
    "      <img id=\"fig-A.2\" src=\"img/perception_unit5_standing_person2.png\" width=\"300\"/>\n",
    "       <center> <figcaption><h2>Standing Person</h2></figcaption></center>\n",
    "    </figure>\n",
    "    </th> \n",
    "\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise U5-2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**Solution Exercise U5.2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please try to do it by yourself unless you get stuck or need some inspiration. You will learn much more if you fight for each exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "RobotIgnite",
    "image": true,
    "name": "robotignite_logo_text",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/robotignite_logo_text.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Face recognition of Bob, Naoko and StandingPerson",
    "image": true,
    "name": "perception_unit5_recogthreepoepple",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/perception_unit5_recogthreepoepple.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, an interesting application is to be able to recognise all of these people at the same time. Here you have an example of how it could be done:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**recognise_multiple_faces.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import rospy\n",
    "import cv2\n",
    "import face_recognition\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from sensor_msgs.msg import Image\n",
    "import rospkg\n",
    "\n",
    "\n",
    "\n",
    "class FaceRecogniser(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        rospy.loginfo(\"Start FaceRecogniser Init process...\")\n",
    "        # get an instance of RosPack with the default search paths\n",
    "        rospack = rospkg.RosPack()\n",
    "        # get the file path for face_recognition_pkg\n",
    "        self.path_to_package = rospack.get_path('face_recognition_pkg')\n",
    "    \n",
    "        self.bridge_object = CvBridge()\n",
    "        rospy.loginfo(\"Start camera suscriber...\")\n",
    "        self.image_sub = rospy.Subscriber(\"/head_camera/rgb/image_raw\",Image,self.camera_callback)\n",
    "        rospy.loginfo(\"Finished FaceRecogniser Init process...Ready\")\n",
    "    \n",
    "    def camera_callback(self,data):\n",
    "        \n",
    "        self.recognise(data)\n",
    "\n",
    "    def recognise(self,data):\n",
    "        \n",
    "        # Get a reference to webcam #0 (the default one)\n",
    "        try:\n",
    "            # We select bgr8 because its the OpneCV encoding by default\n",
    "            video_capture = self.bridge_object.imgmsg_to_cv2(data, desired_encoding=\"bgr8\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "        # Load a sample picture of each person you want to recognise.\n",
    "        bobeye_image_file = os.path.join(self.path_to_package,\"person_img/bobeye.png\")\n",
    "        naoko_image_file = os.path.join(self.path_to_package,\"person_img/naoko.png\")\n",
    "        standing_person_image_file = os.path.join(self.path_to_package,\"person_img/standing_person.png\")\n",
    "        \n",
    "        # Get the face encodings for each face in each image file\n",
    "        # Since there could be more than one face in each image, it returns a list of encordings.\n",
    "        # But since I know each image only has one face, I only care about the first encoding in each image, so I grab index 0.\n",
    "        bobeye_image = face_recognition.load_image_file(bobeye_image_file)\n",
    "        bobeye_face_encoding = face_recognition.face_encodings(bobeye_image)[0]\n",
    "        \n",
    "        naoko_image = face_recognition.load_image_file(naoko_image_file)\n",
    "        naoko_face_encoding = face_recognition.face_encodings(naoko_image)[0]\n",
    "        \n",
    "        standing_person_image = face_recognition.load_image_file(standing_person_image_file)\n",
    "        standing_person_face_encoding = face_recognition.face_encodings(standing_person_image)[0]\n",
    "        \n",
    "        \n",
    "        known_faces = [\n",
    "                        bobeye_face_encoding,\n",
    "                        naoko_face_encoding,\n",
    "                        standing_person_face_encoding\n",
    "                        ]\n",
    "        \n",
    "        # Initialize some variables\n",
    "        face_locations = []\n",
    "        face_encodings = []\n",
    "        face_names = []\n",
    "        process_this_frame = True\n",
    "\n",
    "\n",
    "        # Grab a single frame of video\n",
    "        #ret, frame = video_capture.read()\n",
    "        \n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(video_capture, (0, 0), fx=0.5, fy=0.5)\n",
    "        \n",
    "        # Only process every other frame of video to save time\n",
    "        if process_this_frame:\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(small_frame, face_locations)\n",
    "    \n",
    "            if not face_encodings:\n",
    "                rospy.logwarn(\"No Faces found, please get closer...\")\n",
    "    \n",
    "            face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                # See if the face is a match for the known face(s)\n",
    "                match = face_recognition.compare_faces(known_faces, face_encoding)\n",
    "                name = \"Unknown\"\n",
    "    \n",
    "                if match[0] and not match[1] and not match[2]:\n",
    "                    name = \"Bob\"\n",
    "                elif not match[0] and match[1] and not match[2]:\n",
    "                    name = \"Naoko\"\n",
    "                elif not match[0] and not match[1] and match[2]:\n",
    "                    name = \"Standing_person\"\n",
    "                elif not match[0] and not match[1] and not match[2]:\n",
    "                    name = \"\"\n",
    "                else:\n",
    "                    name = \"SOMETHING_IS_WRONG\"\n",
    "                rospy.loginfo(name)\n",
    "    \n",
    "                face_names.append(name)\n",
    "    \n",
    "        process_this_frame = not process_this_frame\n",
    "    \n",
    "        \n",
    "        # Display the results\n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 2\n",
    "            right *= 2\n",
    "            bottom *= 2\n",
    "            left *= 2\n",
    "    \n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(video_capture, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "    \n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(video_capture, (left, bottom - 35), (right, bottom), (0, 0, 255))\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(video_capture, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "            \n",
    "        \n",
    "        # Display the resulting image\n",
    "        cv2.imshow(\"Image window\", video_capture)\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    rospy.init_node('face_recognising_python_node', anonymous=True)\n",
    "   \n",
    "    line_follower_object = FaceRecogniser()\n",
    "\n",
    "    rospy.spin()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**END recognise_multiple_faces.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise EXTRA U5-3**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a program that does the following:<br>\n",
    "\n",
    "* Reads a topic where someone publishes the name of one of the three people.\n",
    "* Uses the Face Detection system created in the previous unit to look around for people.\n",
    "* Once the robot has detected a face, it moves towards it until it recognises which face it is. If it's the person that it's looking for, the program ends. If not, it memorises that person's location to know it's not the person it's looking for. Then, it starts the search again, filtering the people already discarded.\n",
    "* Publishes the face data so that you can use the <a href=\"http://jsk-visualization.readthedocs.io/en/latest/jsk_rviz_plugins/index.html\">FaceDetection RVIZ elements</a> to see the recognised faces. Use these markers to also be able to print the name of the person recognised. If that person has been discarded, then the marker has to be of a different color, or indicate it in some other way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise EXTRA U5-3**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#417FB1;color:white;\">**Project**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now do the fifth exercise of the Aibo Project. There, you will have to look for your owner, Olive. There will be two humans in the scene. You will have to find Olive and give her paw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#417FB1;color:white;\">**END Project**</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_metadata": {
   "chapter": "5 - People Perception Part 2, Face Recognition",
   "chapter_title": "Unit 5: People Perception Part 2, Face Recognition",
   "course_title": "ROS PERCEPTION IN 5 DAYS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
