{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mastering with ROS: Turlebot3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotis_logo.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/t3_line.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotignite_logo_text.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3: Follow a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time of completion: **2h**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this unit, you will learn how to start using the most basic and also the most powerful tool for perception in ROS: OpenCV.\n",
    "OpneCV is the most extensive and complete library for image recognition. With it, you will be able to work with images like never before: applying filters, postprocessing, and working with images in any way you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">END OF SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use OpenCV in ROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have guessed, OpenCV is not a ROS library, but it's been integrated nicely into it with <a href=\"http://wiki.ros.org/cv_bridge\">OpenCV_bridge</a>.\n",
    "This package allows the ROS imaging topics to use the OpenCV image variable format.<br>\n",
    "\n",
    "For example, OpenCV images come in BGR image format, while regular ROS images are in the more standard RGB encoding. OpenCV_bridge provides a nice feature to convert between them. Also, there are many other functions to transfer images to OpenCV variables transparently.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn how to use OpenCV, you will use the RGB camera of this turtlebot3 robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this turtlebot is in a very strange environment. On the floor, there is a yellow path painted and some stars of different colours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Map of lines to be followed",
    "image": true,
    "name": "perception_unit2_map",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/perception_unit2_map.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you will have to do is make the robot move in this environment, following the yellow line. For that, we will divide the work into the following phases:\n",
    "\n",
    "* Get images from the ROS topic and convert them into OpenCV format\n",
    "* Process the images using OpenCV libraries to obtain the data we want for the task\n",
    "* Move the robot along the yellow line, based on the data obtained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Images from a ROS topic and show them with OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing anything, create a new package called **my_following_line_package** with dependency in <i>rospy</i>, and also a launch and scripts directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step will be to get the images from a ROS topic, put them into the OpenCV format, and show them in the Graphical Tool.<br>\n",
    "\n",
    "Here you have an example of how to do it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**line_follower_basics.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import roslib\n",
    "import sys\n",
    "import rospy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from geometry_msgs.msg import Twist\n",
    "from sensor_msgs.msg import Image\n",
    "\n",
    "\n",
    "class LineFollower(object):\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        self.bridge_object = CvBridge()\n",
    "        self.image_sub = rospy.Subscriber(\"/camera/rgb/image_raw\",Image,self.camera_callback)\n",
    "\n",
    "    def camera_callback(self,data):\n",
    "        \n",
    "        try:\n",
    "            # We select bgr8 because its the OpneCV encoding by default\n",
    "            cv_image = self.bridge_object.imgmsg_to_cv2(data, desired_encoding=\"bgr8\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "\n",
    "        cv2.imshow(\"Image window\", cv_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    line_follower_object = LineFollower()\n",
    "    rospy.init_node('line_following_node', anonymous=True)\n",
    "    try:\n",
    "        rospy.spin()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Shutting down\")\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**END line_follower_basics.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are several elements to comment on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cv_bridge import CvBridge, CvBridgeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These imports are the basics necessary to work with images in ROS.<br>\n",
    "You have OpenCV2 library (cv2). Why the 2? Because there is already a version 3.<br>\n",
    "You have the numpy library, which makes the matrix and other operations easy to work with, and\n",
    "CV_Bridge, which allows ROS to work with OpenCV easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.image_sub = rospy.Subscriber(\"/camera/rgb/image_raw\",Image,self.camera_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subscriber to the image topic. This topic publishes information of the type **sensor_msgs/Image<b/>. Execute the following command to see what all the different variables are inside this message type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosmsg show  sensor_msgs/Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_msgs/Header header                                           \n",
    "  uint32 seq                                           \n",
    "  time stamp                                           \n",
    "  string frame_id                                          \n",
    "uint32 height                                          \n",
    "uint32 width                                           \n",
    "string encoding                                          \n",
    "uint8 is_bigendian                                           \n",
    "uint32 step                                          \n",
    "uint8[] data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract data from certain variables by doing as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rostopic echo -n1 /camera/rgb/image_raw/height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rostopic echo -n1 /camera/rgb/image_raw/width "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rostopic echo -n1 /camera/rgb/image_raw/encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rostopic echo -n1 /camera/rgb/image_raw/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should give you something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user ~ $ rostopic echo -n1 /camera/rgb/image_raw/height                                                                                                      \n",
    "1080                                   \n",
    "---                                                                                                                                                          \n",
    "user ~ $ rostopic echo -n1 /camera/rgb/image_raw/width                                                                                                       \n",
    "1920                                                                                                                                                          \n",
    "---                                                                                                                                                          \n",
    "user ~ $ rostopic echo -n1 /camera/rgb/image_raw/encoding                                                                                                    \n",
    "rgb8                                                                                                                                                         \n",
    "---  \n",
    "user ~ $ rostopic echo -n1 /camera/rgb/image_raw/data \n",
    "[129, 104, 104, 129, 104,\n",
    "...\n",
    "129, 104, 104, 129, 104]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important information here is:<br>\n",
    "\n",
    "* height and width: These are the dimensions in camera pixels. In this case, it's **1080 by 1960**.\n",
    "* encoding: How these pixels are encoded. This means what each value in the data array will mean. In this case, it's **rgb8**. This means that the values in data will be a color value represented as red/green/blue in 8-bit integers.\n",
    "* data: The Image data.\n",
    "\n",
    "If you want the full documentation of this class, please refer to: http://docs.ros.org/api/sensor_msgs/html/msg/Image.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the cv_bridge package, we can easily convert the image data contained in an ImageSensorData into a format that OpenCV understands. By converting it into OpenCV, we can use all the power of that library to process the images of the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # We select bgr8 because its the OpenCV encoding by default\n",
    "    cv_image = self.bridge_object.imgmsg_to_cv2(data, desired_encoding=\"bgr8\")\n",
    "except CvBridgeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the image from a ROS topic and store it in an OpenCV variable. The var <i>data</i> is the one that contains a ROS message with the image captured by the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.imshow(\"Image window\", cv_image)\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will open a gui, where you can see the contents of the variable \"cv_image.\" This is essential to see the effects of the different filters and cropping of the image afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will close all the image windows when the program is terminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing this program, you will have to see the image by clicking on the **Graphical Tools** icon:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Graphical Tool Icon",
    "image": true,
    "name": "font-awesome_desktop",
    "width": "1.3cm"
   },
   "source": [
    "<img src=\"img/font-awesome_desktop.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program will give you an image similar to this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Basic Image View",
    "image": true,
    "name": "perception_unit3_filter1",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/line_image.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 3.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you just have to create a python script with the commented code and see if it works.<br>\n",
    "Create a package for the occasion called \"my_following_line_package,\" and put that python file in a scripts folder.\n",
    "\n",
    "Then, launch the code using the command below, and test that it actually works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun my_following_line_package line_follower_basics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise 3.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply Filters To the Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw image is useless unless you filter it to only see the color you want to track, and you crop the parts of the image you are not interested in. This is to make your program faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to extract some data from the images in a way that we can move the robot to follow the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: Get Image Info and Crop the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start using the images for detecting things, you must take into account two things:\n",
    "\n",
    "* One of the most basic pieces of data that you need to work with images are the dimensions. Is it 800x600? 1200x1024? 60x60?<br>\n",
    "This is crucial to positioning the elements detected in the image.\n",
    "* And second is cropping the image. It's very important to work as soon as possible with the minimum size of the image required for the task. This makes the detecting system mush faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We get image dimensions and crop the parts of the image we dont need\n",
    "# Bear in mind that because its image matrix first value is start and second value is down limit.\n",
    "# Select the limits so that they get the line not too close, not too far, and the minimum portion possible\n",
    "# To make the process faster.\n",
    "height, width, channels = cv_image.shape\n",
    "descentre = 160\n",
    "rows_to_watch = 100\n",
    "crop_img = cv_image[(height)/2+descentre:(height)/2+(descentre+rows_to_watch)][1:width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why this values and not other values? Well, it depends on the task. In this case, you are interested in lines that aren't too far away from the robot, nor too near. If you concentrate on lines too far away, it won't follow the lines, but it will just go across the map. On the other hand, concentrating on lines that are too close won't give the robot time to adapt to changes in the line.<br>\n",
    "It's also vital to optimise the region of the image as a result of cropping. If it's too big, too much data will be processed, making your program too slow. On the other hand, it has to have enough image to work with.<br>\n",
    "At the end, you will have to adapt it to each situation.<br>\n",
    "Do **Exercise U2.3** to test the effects of different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: Convert from BGR to HSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that OpenCV works with BGR instead of RGB, for historical reasons (some cameras, in the days when OpenCV was created, worked with BGR).<br>\n",
    "Well, it seems that it is not very easy to work with either RGB or BGR, to differentiate colors. That's why HSV is used. The idea behind HSV is to remove the component of color saturation. This way, it's easier to recognise the same color in different light conditions, which is a serious issue in image recognition.\n",
    "More information: https://en.wikipedia.org/wiki/HSL_and_HSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "HSV cone",
    "image": true,
    "name": "HSV_color_solid_cone_chroma_gray",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/HSV_color_solid_cone_chroma_gray.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>By <a href=\"//commons.wikimedia.org/wiki/File:Hcl-hcv_models.svg\" title=\"File:Hcl-hcv models.svg\">Hcl-hcv_models.svg</a>: <a href=\"//commons.wikimedia.org/wiki/User:Jacobolus\" title=\"User:Jacobolus\">Jacob Rus</a>\n",
    "<a href=\"//commons.wikimedia.org/wiki/File:HSV_color_solid_cone.png\" title=\"File:HSV color solid cone.png\">HSV_color_solid_cone.png</a>: <a href=\"//commons.wikimedia.org/wiki/User:SharkD\" title=\"User:SharkD\">SharkD</a>\n",
    "derivative work: <span style=\"border:1px solid #f57900;padding:1px;\"><a href=\"//commons.wikimedia.org/wiki/User:SharkD\" title=\"User:SharkD\"><span style=\"color:#8f5902;padding-left:1px;\">SharkD</span></a> <a href=\"//commons.wikimedia.org/wiki/User_talk:SharkD\" title=\"User talk:SharkD\"><span style=\"color:#fff;background:#fcaf3e;\">&nbsp;Talk&nbsp;</span></a></span> - <a href=\"//commons.wikimedia.org/wiki/File:Hcl-hcv_models.svg\" title=\"File:Hcl-hcv models.svg\">Hcl-hcv_models.svg</a>\n",
    "<a href=\"//commons.wikimedia.org/wiki/File:HSV_color_solid_cone.png\" title=\"File:HSV color solid cone.png\">HSV_color_solid_cone.png</a>, <a href=\"http://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=9802544\">Link</a>**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert from RGB to HSV\n",
    "hsv = cv2.cvtColor(crop_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define the Yellow Colour in HSV\n",
    "#RGB\n",
    "#[[[222,255,0]]]\n",
    "#BGR\n",
    "#[[[0,255,222]]]\n",
    "\"\"\"\n",
    "To know which color to track in HSV, Put in BGR. Use ColorZilla to get the color registered by the camera\n",
    ">>> yellow = np.uint8([[[B,G,R ]]])\n",
    ">>> hsv_yellow = cv2.cvtColor(yellow,cv2.COLOR_BGR2HSV)\n",
    ">>> print( hsv_yellow )\n",
    "[[[ 34 255 255]]\n",
    "\"\"\"\n",
    "lower_yellow = np.array([20,100,100])\n",
    "upper_yellow = np.array([50,255,255])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this piece of code, you are converting the cropped_image (crop_img) into HSV.<br>\n",
    "Then, you select which color in HSV you want to track, selecting from the base of the color cone, a single point. Because HSV values are quite difficult to generate, it's better that you use a color picker tool like  <a href=\"https://chrome.google.com/webstore/detail/colorzilla/bhlhnicpbhignbdhedgjhgdocnmhomnp\" title=\"ColorZilla\">ColorZilla</a> to pick the RGB coding of the color to track, in this case, the yellow line in the simulation.<br>\n",
    "\n",
    "Once you have it, use the example code given in a python terminal or create a tiny program that uses numpy as np and cv2 to convert it to HSV.<br>\n",
    "In the example given, the color of the line is HSV = [[[ 34 255 255]]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have it then you have to select an upper and lower bound to consider the region of the cone base that you will consider Yellow. Bigger the region more gradients of your picked colour will be accepted. This will depend on how your robot detects and the colour variations in the image and how critical is mixing similar colours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "HSV filter",
    "image": true,
    "name": "perception_unit3_filter2",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/perception_unit3_filter2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step: Apply the mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to generate a version of the cropped image in which you only see two colors: black and white. The white will be all the colors you consider yellow and the rest will be black. It's a binary image.\n",
    "\n",
    "Why do you need to do this? It has two functions:<br>\n",
    "\n",
    "* In doing this, you don't have continuous detection. It is the color or it's NOT, there is no in-between. This is vital for the centroid calculation that will be done after, because it only works on the principal of YES or NO.\n",
    "* Second, it will allow the generation of the result image afterwards, in which you extract everything on the image except the color line, seeing only what you are interested in seeing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Threshold the HSV image to get only yellow colors\n",
    "mask = cv2.inRange(hsv, lower_yellow, upper_yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bitwise-AND mask and original image\n",
    "res = cv2.bitwise_and(crop_img,crop_img, mask= mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You, then, merge the cropped, colored image in HSV with the binary mask image, to color only the detections, leaving the rest in black."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Binary Mask Filter",
    "image": true,
    "name": "perception_unit3_filter3",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/perception_unit3_filter3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth step: Get The Centroids, draw a circle where the centroid is and show all the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centroids, in essence, represent points in space where mass concentrates; the center of mass. Centroid and centers of mass are the same thing as far as this course is concerned. And they are calculated using Integrals.<br>\n",
    "This is extrapolated into images. But, instead of having mass, we have color. The place where there is more of the color that you are looking for is where the centroid will be. It's the center of mass of the blobs seen in an image.<br>\n",
    "That's why you applied the mask to make the image binary. This way, you can easily calculate where the center of mass is located. This is because it's a discrete function, not a continuous one. This means that it allows us to integrate in a discrete fashion, and not need a function describing the fluctuations in quantity of color throughout the region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These centroids are vital in blob tracking because they give you a precise point in space where the blob is. You will use this to follow the blob and, therefore, follow the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is needed to calculate the centroids of the color blobs. You use the image moments for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate centroid of the blob of binary image using ImageMoments\n",
    "m = cv2.moments(mask, False)\n",
    "try:\n",
    "    cx, cy = m['m10']/m['m00'], m['m01']/m['m00']\n",
    "except ZeroDivisionError:\n",
    "    cy, cx = height/2, width/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, you will obtain the coordinates of the cropped image, where detection of the centroid of the positive color yellow occur. If nothing is detected, it will be positioned in the center of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you have to assign the correct Cy, Cx values. Don't get the height and width mixed up, or you will have problems in the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a more detailed explanation of an OpenCV exercise and all that can be obtained with contour features: http://docs.opencv.org/trunk/dd/d49/tutorial_py_contour_features.html\n",
    "\n",
    "If you are interested in all the mathematical justifications:https://en.wikipedia.org/wiki/Image_moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw the centroid in the resultut image\n",
    "# cv2.circle(img, center, radius, color[, thickness[, lineType[, shift]]]) \n",
    "cv2.circle(res,(int(cx), int(cy)), 10,(0,0,255),-1)\n",
    "\n",
    "cv2.imshow(\"Original\", cv_image)\n",
    "cv2.imshow(\"HSV\", hsv)\n",
    "cv2.imshow(\"MASK\", mask)\n",
    "cv2.imshow(\"RES\", res)\n",
    "\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV allows you to draw a lot of things over the images, not only geometric shapes. But in this case, a circle will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.circle(res,(centre_cicle_x, centre_cicle_y), LineWidth,(BGRColour of line),TypeOfLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using this feature to draw a circle in the location of the calculated centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Added centroid",
    "image": true,
    "name": "perception_unit3_filter4",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/perception_unit3_filter4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Info: http://docs.opencv.org/2.4/modules/core/doc/drawing_functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, finally, you show all of the image variables with their titles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "All image filters and original image",
    "image": true,
    "name": "perception_unit2_linefollowfilters",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/perception_unit2_linefollowfilters.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Move the TurtleBot3 based on the position of the Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_x = cx - width / 2;\n",
    "twist_object = Twist();\n",
    "twist_object.linear.x = 0.1;\n",
    "twist_object.angular.z = -error_x / 5000;\n",
    "rospy.loginfo(\"ANGULAR VALUE SENT===>\"+str(twist_object.angular.z))\n",
    "# Make it start turning\n",
    "self.movekobuki_object.move_robot(twist_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This control is passed on a Proportional control. This means that it will oscillate a lot and probably have an error. But, it is the simplest way of moving the robot and get the job done.<br>\n",
    "It always gives a constant linear movement and the angular Z velocity depends on the difference between the centroid center in X and the center of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move the robot, you can use this module :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**move_robot.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "from geometry_msgs.msg import Twist\n",
    "\n",
    "\n",
    "class MoveKobuki(object):\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        self.cmd_vel_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)\n",
    "        self.last_cmdvel_command = Twist()\n",
    "        self._cmdvel_pub_rate = rospy.Rate(10)\n",
    "        self.shutdown_detected = False\n",
    "\n",
    "    def move_robot(self, twist_object):\n",
    "        self.cmd_vel_pub.publish(twist_object)\n",
    "                                    \n",
    "    def clean_class(self):\n",
    "        # Stop Robot\n",
    "        twist_object = Twist()\n",
    "        twist_object.angular.z = 0.0\n",
    "        self.move_robot(twist_object)\n",
    "        self.shutdown_detected = True\n",
    "\n",
    "def main():\n",
    "    rospy.init_node('move_robot_node', anonymous=True)\n",
    "    \n",
    "    \n",
    "    movekobuki_object = MoveKobuki()\n",
    "    twist_object = Twist()\n",
    "    # Make it start turning\n",
    "    twist_object.angular.z = 0.15\n",
    "    \n",
    "    \n",
    "    rate = rospy.Rate(5)\n",
    "    \n",
    "    ctrl_c = False\n",
    "    def shutdownhook():\n",
    "        # works better than the rospy.is_shut_down()\n",
    "        movekobuki_object.clean_class()\n",
    "        rospy.loginfo(\"shutdown time!\")\n",
    "        ctrl_c = True\n",
    "    \n",
    "    rospy.on_shutdown(shutdownhook)\n",
    "    \n",
    "    while not ctrl_c:\n",
    "        movekobuki_object.move_robot(twist_object)\n",
    "        rate.sleep()\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**END move_robot.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have an example of how all of this code would be put together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**follow_line_step_hsv.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from geometry_msgs.msg import Twist\n",
    "from sensor_msgs.msg import Image\n",
    "from move_robot import MoveKobuki\n",
    "\n",
    "class LineFollower(object):\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        self.bridge_object = CvBridge()\n",
    "        self.image_sub = rospy.Subscriber(\"/camera/rgb/image_raw\",Image,self.camera_callback)\n",
    "        self.movekobuki_object = MoveKobuki()\n",
    "\n",
    "    def camera_callback(self,data):\n",
    "        \n",
    "        try:\n",
    "            # We select bgr8 because its the OpneCV encoding by default\n",
    "            cv_image = self.bridge_object.imgmsg_to_cv2(data, desired_encoding=\"bgr8\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "            \n",
    "        # We get image dimensions and crop the parts of the image we don't need\n",
    "        # Bear in mind that because the first value of the image matrix is start and second value is down limit.\n",
    "        # Select the limits so that it gets the line not too close and not too far, and the minimum portion possible\n",
    "        # To make process faster.\n",
    "        height, width, channels = cv_image.shape\n",
    "        descentre = 160\n",
    "        rows_to_watch = 100\n",
    "        crop_img = cv_image[(height)/2+descentre:(height)/2+(descentre+rows_to_watch)][1:width]\n",
    "        \n",
    "        # Convert from RGB to HSV\n",
    "        hsv = cv2.cvtColor(crop_img, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Define the Yellow Colour in HSV\n",
    "        #RGB\n",
    "        #[[[222,255,0]]]\n",
    "        #BGR\n",
    "        #[[[0,255,222]]]\n",
    "        \"\"\"\n",
    "        To know which color to track in HSV, Put in BGR. Use ColorZilla to get the color registered by the camera\n",
    "        >>> yellow = np.uint8([[[B,G,R ]]])\n",
    "        >>> hsv_yellow = cv2.cvtColor(yellow,cv2.COLOR_BGR2HSV)\n",
    "        >>> print( hsv_yellow )\n",
    "        [[[ 34 255 255]]\n",
    "        \"\"\"\n",
    "        lower_yellow = np.array([20,100,100])\n",
    "        upper_yellow = np.array([50,255,255])\n",
    "\n",
    "        # Threshold the HSV image to get only yellow colors\n",
    "        mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "        \n",
    "        # Calculate centroid of the blob of binary image using ImageMoments\n",
    "        m = cv2.moments(mask, False)\n",
    "        try:\n",
    "            cx, cy = m['m10']/m['m00'], m['m01']/m['m00']\n",
    "        except ZeroDivisionError:\n",
    "            cy, cx = height/2, width/2\n",
    "        \n",
    "        \n",
    "        # Bitwise-AND mask and original image\n",
    "        res = cv2.bitwise_and(crop_img,crop_img, mask= mask)\n",
    "        \n",
    "        # Draw the centroid in the resultut image\n",
    "        # cv2.circle(img, center, radius, color[, thickness[, lineType[, shift]]]) \n",
    "        cv2.circle(res,(int(cx), int(cy)), 10,(0,0,255),-1)\n",
    "\n",
    "        cv2.imshow(\"Original\", cv_image)\n",
    "        cv2.imshow(\"HSV\", hsv)\n",
    "        cv2.imshow(\"MASK\", mask)\n",
    "        cv2.imshow(\"RES\", res)\n",
    "        \n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "        \n",
    "        error_x = cx - width / 2;\n",
    "        twist_object = Twist();\n",
    "        twist_object.linear.x = 0.1;\n",
    "        twist_object.angular.z = -error_x / 5000;\n",
    "        rospy.loginfo(\"ANGULAR VALUE SENT===>\"+str(twist_object.angular.z))\n",
    "        # Make it start turning\n",
    "        self.movekobuki_object.move_robot(twist_object)\n",
    "        \n",
    "    def clean_up(self):\n",
    "        self.movekobuki_object.clean_class()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "\n",
    "def main():\n",
    "    rospy.init_node('line_following_node', anonymous=True)\n",
    "    \n",
    "    \n",
    "    line_follower_object = LineFollower()\n",
    "   \n",
    "    rate = rospy.Rate(5)\n",
    "    ctrl_c = False\n",
    "    def shutdownhook():\n",
    "        # works better than the rospy.is_shut_down()\n",
    "        line_follower_object.clean_up()\n",
    "        rospy.loginfo(\"shutdown time!\")\n",
    "        ctrl_c = True\n",
    "    \n",
    "    rospy.on_shutdown(shutdownhook)\n",
    "    \n",
    "    while not ctrl_c:\n",
    "        rate.sleep()\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**END follow_line_step_hsv.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 3.2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a python script inside your package with the provided code. Execute it and see how it performs.<br>\n",
    "Try some improvements:\n",
    "\n",
    "* Lower the speed of the robot to see if it works better. Change the linear and the angular speeds.\n",
    "* Change the behavior of the robot, maybe create a recovery behavior for when it loses the line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise 3.2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 3.3**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the color to be tracked. Try to track the three different star colors. Use the color picker for getting the exact RGB color for:\n",
    "\n",
    "* RedStar\n",
    "* GreenStar\n",
    "* BlueStar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have an example of how it should look with the blue star:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Colour Blue tracking",
    "image": true,
    "name": "perception_unit2_bluestar",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/perception_unit2_bluestar.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, try changing the values of the upper and lower bounds to the following options and see the results:\n",
    "\n",
    "* LOOSE color detection: lower_yellow = np.array([0,50,50]), upper_yellow = np.array([255,255,255])\n",
    "* STRICT color detection: lower_yellow = np.array([33,254,254]), upper_yellow = np.array([36,255,255])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have an example of what you should see when changing the lower and upper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOSE color detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "",
    "image": true,
    "name": "perception_unit2_colourbounds_open2",
    "width": "10cm"
   },
   "source": [
    "<img id=\"fig-A.1\" src=\"img/perception_unit2_colourbounds_open2.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STRICT color detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "",
    "image": true,
    "name": "perception_unit2_colourbounds_closed2",
    "width": "10cm"
   },
   "source": [
    "<img id=\"fig-A.2\" src=\"img/perception_unit2_colourbounds_closed2.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that in the loose version, all the colors that are green and yellow are detected. While in the strict, you can see that even in the yellow line, there is a part that is cut off because it's slightly different to the camera sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise 3.3**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 3.4**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try executing the code with different values in the **descentre** and the rows to watch and see how the follow line performs.<br>\n",
    "Try the following values:<br>\n",
    "\n",
    "* descentre = 0, rows_to_watch = 20. In this case, you control the center of the image and a very small piece.\n",
    "* descentre = 0, rows_to_watch = 200. In this case, you are controlling nearly all of the lower part of the image.\n",
    "* descentre = 200, rows_to_watch = 20. In this case, you are controlling only a small fraction of the lower part of the image.\n",
    "\n",
    "Test these values and see which one is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise 3.4**</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_metadata": {
   "chapter": "2 - Vision Basics in ROS Part 2, Follow a line",
   "chapter_title": "Unit 2: Vision Basics in ROS Part 2, Follow a line",
   "course_title": "ROS PERCEPTION IN 5 DAYS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
