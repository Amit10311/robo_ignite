{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mastering with ROS: Turtlebot3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotis_logo.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/object_rec_unit.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotignite_logo_text.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 5: Perception and Object Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time of completion: <b>1'5h</b><br><br>\n",
    "This Unit will show you how to use Perception and Object Recognition to get the position of graspable objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">END OF SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Chapter, you learned how to perform blob tracking using the RGB camera of the robot. But you need to know that the blob tracking you performed was getting the position in the 2D space, not in 3D. Although it is possible to get the 3D position with the RGB image (using the Are), you would need to do some extra work, and it is not very accurate. So, the practice in order to get the 3D position of an object in an environment, is to use the depth (PointCloud) data of the sensor. And is what you are going to do in thus Unit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most usefull perception skills is being able to recognise objects. This allows you to create robots that can grasp objects and understand the world around them a little bit better.\n",
    "<br><br>\n",
    "There are two main skills to master here:<br>\n",
    "\n",
    "* **Recognise flat surfaces**: This allows the robot to detect places where objects usually are. For instance, tables or shelves. It's the first step to take when searching for objects.\n",
    "\n",
    "* **Recognise objects**: Once you know where to look, you have to be able to recognise different object in the scene and localise where they are placed in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Unit, we are going to focus on the Object Recognition, since the Turtlebot3 robot is an small robot, and isn't likely going to be detecting tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as you may have seen, we have made a modification in the simulation, in order to make easier the task of recognising objects in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ball to grasp has now a black and white texture, which makes it easier to detect by the camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Fetch robot in object recognition world",
    "image": true,
    "name": "perception_unit3_demo",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/ball_with_texture.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Also, we introduce you to your new robotic friend. The Turtlebot3 Waffle with the open manipulator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... with the proper introductions made, let's start working!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some pictures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The firt step will be to take some pictures of the object we want to grasp, in order to detect some key points that define the object. With this key points, we will be able to detect the object later, by comparing the pictures taken with the object being detected by the camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this purpose we will use the **find_object_2d** package. So, in order to see how to do this, just follow the next exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">Exercise 5.1</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) The first step is to create your own object recognition package:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd;cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catkin_create_pkg my_object_recognition_pkg rospy object_recognition_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Inside your object recognition package, create a new launch file called **start_find_object_2d.launch**. Copy the following code inside it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "\n",
    "<launch> \n",
    "    <arg name=\"camera_rgb_topic\" default=\"/camera/rgb/image_raw\" />\n",
    "    <node name=\"find_object_2d_node\" pkg=\"find_object_2d\" type=\"find_object_2d\" output=\"screen\">\n",
    "        <remap from=\"image\" to=\"$(arg camera_rgb_topic)\"/>\n",
    "    </node>\n",
    "    \n",
    "</launch> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you just need to set the RGB camera image source and the system will be ready to go. In this case, it's **/camera/rgb/image_raw**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Launch this file and go to The Graphical Tools tab. You should see something similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "CokeCan in ObjectRecognition Gui",
    "image": true,
    "name": "perception_unit3_objectrec2dinit",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/photos1.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, you will be able to see the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rec1.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Now, it's time to get some pics of the object we want to grasp! In order to do this, select the **Edit -> Add object from scene** option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/photos3.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add previously taken images directly, but bear in mind that there are some peculiarities. The images appear in this object recogniser mirrored, if you compare them with the images from the cameras. So you should be careful with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) In the **Add Object** screen, you just have to follow the steps in order to select the section of the image that you consider to be the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the **Take picture** button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Add Object",
    "image": true,
    "name": "perception_unit3_addobject1",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/rec2.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the desired section of the image. In this case, it's the ball. Try to make the selection a little bit bigger than the ball itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Add Object result",
    "image": true,
    "name": "perception_unit3_addobject2",
    "width": "10cm"
   },
   "source": [
    "<img src=\"img/rec3.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, click the **End** button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Once done, you should be detecting the object in the table. This system compares the images received by the camera with the saved ones, and looks for matches. If it matches in enough points, it considers it the desired object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rec4.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) So, the last step will be to save all of the objects added. There are 2 main ways of doing this:<br>\n",
    "\n",
    "* Saving the objects as images: **File -> Save Objects**. This will save all of the images taken in a folder\n",
    "* Saving the whole session: **File -> Save Session**. This will save a binary with all of the images and settings. This is the most compact way of doing it, although you won't have access to the images of the objects. It depends on your needs\n",
    "\n",
    "For now, let's just save the whole session. Inside your package, create a new folder named **saved_pictures**, and save the session inside this folder. You can name it **ball_session**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rec5.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">End of Exercise 5.1</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, once you have your session stored, you need to be able to always start an object recognition session with all of that stored data. In order to do so, just follow the next exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">Exercise 5.2</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Create a new launch file inside your package named **start_find_object_3d_session.launch**, and copy the following content into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<launch>\n",
    "\t\t\n",
    "\t<node name=\"find_object_3d\" pkg=\"find_object_2d\" type=\"find_object_2d\" output=\"screen\">\n",
    "\t\t<param name=\"gui\" value=\"true\" type=\"bool\"/>\n",
    "\t\t<param name=\"settings_path\" value=\"~/.ros/find_object_2d.ini\" type=\"str\"/>\n",
    "\t\t<param name=\"subscribe_depth\" value=\"true\" type=\"bool\"/>\n",
    "\t\t<param name=\"session_path\" value=\"$(find my_object_recognition_pkg)/saved_pictures/ball_session.bin\" type=\"str\"/>\n",
    "\t\t<param name=\"objects_path\" value=\"\" type=\"str\"/>\n",
    "\t\t<param name=\"object_prefix\" value=\"object\" type=\"str\"/>\n",
    "\t\t\n",
    "\t\t<remap from=\"rgb/image_rect_color\" to=\"/camera/rgb/image_raw\"/>\n",
    "\t\t<remap from=\"depth_registered/image_raw\" to=\"/camera/depth/image_raw\"/>\n",
    "\t\t<remap from=\"depth_registered/camera_info\" to=\"/camera/depth/camera_info\"/>\n",
    "\t</node>\n",
    "\t\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Launch the file. You should then be able to get the TF of the detected object published. If you have multiple images of the same object, you will get multiple frames of objects. It's up to you to filter them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tfs2.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tfs3.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tfs1.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) You can also see the object detected by executing the following command in another terminal while the prior launch is working:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #2</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun find_object_2d print_objects_detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/object_detected.png\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">End of Exercise 5.2</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! So now we are able to get the TF of the object we want to grasp. But... how can we get the position of that object? Which, in fact, is the important data we really want to know if we want to grasp it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as you have seen in the previous exercise, we now have the TF from the detected object to the **camera_link** being published. And, obviously, we also have the TF from this camera_link frame to the **base_footprint** frame, which represents the base of the robot. So... with this TF data being published, we can already know the position of that object related to the base of the robot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in order to get the position of the object, you just need to check the value of its TF regarding the world frame. You can check that by using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun tf tf_echo base_footprint <object_frame>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if the frame of your object is named, like in this notebook, **object_8**, the command would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun tf tf_echo base_footprint object_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, you will get something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/object_tf.png\" width=\"600\" />"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_metadata": {
   "chapter": "3 - Flat Surface and Object Recognition",
   "chapter_title": "Unit 3: Flat Surface and Object Recognition",
   "course_title": "ROS PERCEPTION IN 5 DAYS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
