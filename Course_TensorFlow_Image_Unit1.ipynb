{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1: Create your own ROS Package that Recognises Images with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotignite_logo_text.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_image_unit1_intro.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Estimated time to completion:</b> 1.5 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Simulated robot:</b> Mira Robot\n",
    "<br><br>\n",
    "<b>What will you learn in this unit?</b>\n",
    "* Use a TensorFlow Model that has learned hundreds of images from the ImageNet Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate ROS and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**Example 1.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's get straight to the point. The first thing you will have to do is create a package that will contain all the scripts related to this unit in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd;cd ../src;catkin_create_pkg tf_unit1_pkg rospy std_msgs sensor_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ..;source devel/setup.bash;rospack profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd tf_unit1_pkg;mkdir launch;mkdir scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have your package. Next, place all of your **Python Scripts** into **scripts** and your **ROS launch files** into **launch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create your TensorFlow scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to create two python scripts that will retrieve the image data, classify it based on a downloaded TensorFlow model, and publish the results into a ROS topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"image_recognition\">**Python Program {1.1A-py}: image_recognition.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this first python script is the one responsible for retrieving ROS images from a topic and sending them to a classification class that will decide which objects are present on the scene.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "from sensor_msgs.msg import Image\n",
    "from std_msgs.msg import String\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import classify_image\n",
    "\n",
    "\n",
    "class RosTensorFlow():\n",
    "    def __init__(self):\n",
    "        classify_image.maybe_download_and_extract()\n",
    "        self._session = tf.Session()\n",
    "        classify_image.create_graph()\n",
    "        self._cv_bridge = CvBridge()\n",
    "\n",
    "        self._sub = rospy.Subscriber('image', Image, self.callback, queue_size=1)\n",
    "        self._pub = rospy.Publisher('result', String, queue_size=1)\n",
    "        self.score_threshold = rospy.get_param('~score_threshold', 0.1)\n",
    "        self.use_top_k = rospy.get_param('~use_top_k', 5)\n",
    "\n",
    "    def callback(self, image_msg):\n",
    "        cv_image = self._cv_bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\n",
    "        # copy from\n",
    "        # classify_image.py\n",
    "        image_data = cv2.imencode('.jpg', cv_image)[1].tostring()\n",
    "        # Creates graph from saved GraphDef.\n",
    "        softmax_tensor = self._session.graph.get_tensor_by_name('softmax:0')\n",
    "        predictions = self._session.run(\n",
    "            softmax_tensor, {'DecodeJpeg/contents:0': image_data})\n",
    "        predictions = np.squeeze(predictions)\n",
    "        # Creates node ID --> English string lookup.\n",
    "        node_lookup = classify_image.NodeLookup()\n",
    "        top_k = predictions.argsort()[-self.use_top_k:][::-1]\n",
    "        for node_id in top_k:\n",
    "            human_string = node_lookup.id_to_string(node_id)\n",
    "            score = predictions[node_id]\n",
    "            if score > self.score_threshold:\n",
    "                rospy.loginfo('%s (score = %.5f)' % (human_string, score))\n",
    "                self._pub.publish(human_string)\n",
    "\n",
    "    def main(self):\n",
    "        rospy.spin()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classify_image.setup_args()\n",
    "    rospy.init_node('rostensorflow')\n",
    "    tensor = RosTensorFlow()\n",
    "    tensor.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's comment on the most relevant pieces of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify_image.maybe_download_and_extract()\n",
    "self._session = tf.Session()\n",
    "classify_image.create_graph()\n",
    "self._cv_bridge = CvBridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four lines are what make most of the magic happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify_image.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first line will download the **freezed-tensorflow-model**. A more exact model that was downloaded is **classify_image_graph_def.pb**, from the **imagenet_2012_challenge**. It's a model prepared for image recognition of hundreds of objects, trained with thousands of pre-labeled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you have a list of the objects that this model should be able to detect. On this list, you have the **encoding used by TensorFlow** and the **human-readable labels**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[imagenet_synset_to_human_label_map](extra_files/imagenet_synset_to_human_label_map.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are quite a few! This means that you could probably be able to use this for basic object recognition programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self._session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you are starting a TensorFlow session. This will give you access to all of the TensorFlow functionality and you will be able to use the **tensor soft-max** from the downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify_image.create_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may talk about this line later, in the second script. Just know that you are initialising all of the necessary elements for recognition using the model. You will be able to see this *graph** through the *TensorBoard** tools at the end of this unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self._cv_bridge = CvBridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is our beloved CV_Bridge, which will allow us to use all of the power of **OpenCV** with ROS type images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also comment on the three lines that are important from a ROS point of view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self._sub = rospy.Subscriber('image', Image, self.callback, queue_size=1)\n",
    "self._pub = rospy.Publisher('result', String, queue_size=1)\n",
    "self.score_threshold = rospy.get_param('~score_threshold', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you declare a subscriber to an image topic, which we will remap to our particular robot camera's RGB topic.<br>\n",
    "You also declare a publisher, where you will write the highest-score object that was recognised.<br>\n",
    "And finally, you set the **score_threshold**. This you can increase up to 1.0. The higher the value, the most sure the detection has to be to consider it a correct and valid one. In this case, it's rather low to have loads of detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"image_recognition\">**END Python Program {1.1A-py}: image_recognition.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move on to the second python script. This one does all the heavy lifting of preparing the downloaded model to have human readable tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"classify_image\">**Python Program {1.1B-py}: classify_image.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-274e5b6bdd70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# this file is from https://github.com/tensorflow/models\n",
    "\n",
    "\"\"\"Simple image classification with Inception.\n",
    "\n",
    "Run image classification with Inception trained on ImageNet 2012 Challenge data\n",
    "set.\n",
    "\n",
    "This program creates a graph from a saved GraphDef protocol buffer,\n",
    "and runs inference on an input JPEG image. It outputs human readable\n",
    "strings of the top 5 predictions along with their probabilities.\n",
    "\n",
    "Change the --image_file argument to any jpg image to compute a\n",
    "classification of that image.\n",
    "\n",
    "Please see the tutorial and website for a detailed description of how\n",
    "to use this script to perform image recognition.\n",
    "\n",
    "https://tensorflow.org/tutorials/image_recognition/\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# pylint: disable=line-too-long\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "# pylint: enable=line-too-long\n",
    "\n",
    "\n",
    "class NodeLookup(object):\n",
    "  \"\"\"Converts integer node ID's to human readable labels.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               label_lookup_path=None,\n",
    "               uid_lookup_path=None):\n",
    "    if not label_lookup_path:\n",
    "      label_lookup_path = os.path.join(\n",
    "          FLAGS.model_dir, 'imagenet_2012_challenge_label_map_proto.pbtxt')\n",
    "    if not uid_lookup_path:\n",
    "      uid_lookup_path = os.path.join(\n",
    "          FLAGS.model_dir, 'imagenet_synset_to_human_label_map.txt')\n",
    "    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n",
    "\n",
    "  def load(self, label_lookup_path, uid_lookup_path):\n",
    "    \"\"\"Loads a human readable English name for each softmax node.\n",
    "\n",
    "    Args:\n",
    "      label_lookup_path: string UID to integer node ID.\n",
    "      uid_lookup_path: string UID to human-readable string.\n",
    "\n",
    "    Returns:\n",
    "      dict from integer node ID to human-readable string.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(uid_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', uid_lookup_path)\n",
    "    if not tf.gfile.Exists(label_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', label_lookup_path)\n",
    "\n",
    "    # Loads mapping from string UID to human-readable string\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n",
    "    uid_to_human = {}\n",
    "    p = re.compile(r'[n\\d]*[ \\S,]*')\n",
    "    for line in proto_as_ascii_lines:\n",
    "      parsed_items = p.findall(line)\n",
    "      uid = parsed_items[0]\n",
    "      human_string = parsed_items[2]\n",
    "      uid_to_human[uid] = human_string\n",
    "\n",
    "    # Loads mapping from string UID to integer node ID.\n",
    "    node_id_to_uid = {}\n",
    "    proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n",
    "    for line in proto_as_ascii:\n",
    "      if line.startswith('  target_class:'):\n",
    "        target_class = int(line.split(': ')[1])\n",
    "      if line.startswith('  target_class_string:'):\n",
    "        target_class_string = line.split(': ')[1]\n",
    "        node_id_to_uid[target_class] = target_class_string[1:-2]\n",
    "\n",
    "    # Loads the final mapping of integer node ID to human-readable string\n",
    "    node_id_to_name = {}\n",
    "    for key, val in node_id_to_uid.items():\n",
    "      if val not in uid_to_human:\n",
    "        tf.logging.fatal('Failed to locate: %s', val)\n",
    "      name = uid_to_human[val]\n",
    "      node_id_to_name[key] = name\n",
    "\n",
    "    return node_id_to_name\n",
    "\n",
    "  def id_to_string(self, node_id):\n",
    "    if node_id not in self.node_lookup:\n",
    "      return ''\n",
    "    return self.node_lookup[node_id]\n",
    "\n",
    "\n",
    "def create_graph():\n",
    "  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
    "  # Creates graph from saved graph_def.pb.\n",
    "  with tf.gfile.FastGFile(os.path.join(\n",
    "      FLAGS.model_dir, 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "\n",
    "def run_inference_on_image(image):\n",
    "  \"\"\"Runs inference on an image.\n",
    "\n",
    "  Args:\n",
    "    image: Image file name.\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  if not tf.gfile.Exists(image):\n",
    "    tf.logging.fatal('File does not exist %s', image)\n",
    "  image_data = tf.gfile.FastGFile(image, 'rb').read()\n",
    "\n",
    "  # Creates graph from saved GraphDef.\n",
    "  create_graph()\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    # Some useful tensors:\n",
    "    # 'softmax:0': A tensor containing the normalized prediction across\n",
    "    #   1000 labels.\n",
    "    # 'pool_3:0': A tensor containing the next-to-last layer containing 2048\n",
    "    #   float description of the image.\n",
    "    # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG\n",
    "    #   encoding of the image.\n",
    "    # Runs the softmax tensor by feeding the image_data as input to the graph.\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n",
    "    predictions = sess.run(softmax_tensor,\n",
    "                           {'DecodeJpeg/contents:0': image_data})\n",
    "    predictions = np.squeeze(predictions)\n",
    "\n",
    "    # Creates node ID --> English string lookup.\n",
    "    node_lookup = NodeLookup()\n",
    "\n",
    "    top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1]\n",
    "    for node_id in top_k:\n",
    "      human_string = node_lookup.id_to_string(node_id)\n",
    "      score = predictions[node_id]\n",
    "      print('%s (score = %.5f)' % (human_string, score))\n",
    "\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "  \"\"\"Download and extract model tar file.\"\"\"\n",
    "  dest_directory = FLAGS.model_dir\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  maybe_download_and_extract()\n",
    "  image = (FLAGS.image_file if FLAGS.image_file else\n",
    "           os.path.join(FLAGS.model_dir, 'cropped_panda.jpg'))\n",
    "  run_inference_on_image(image)\n",
    "\n",
    "\n",
    "def setup_args():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # classify_image_graph_def.pb:\n",
    "  #   Binary representation of the GraphDef protocol buffer.\n",
    "  # imagenet_synset_to_human_label_map.txt:\n",
    "  #   Map from synset ID to a human readable string.\n",
    "  # imagenet_2012_challenge_label_map_proto.pbtxt:\n",
    "  #   Text representation of a protocol buffer mapping a label to synset ID.\n",
    "  parser.add_argument(\n",
    "      '--model_dir',\n",
    "      type=str,\n",
    "      #default='/tmp/imagenet',\n",
    "      default='/home/user/catkin_ws/src/',\n",
    "      help=\"\"\"\\\n",
    "      Path to classify_image_graph_def.pb,\n",
    "      imagenet_synset_to_human_label_map.txt, and\n",
    "      imagenet_2012_challenge_label_map_proto.pbtxt.\\\n",
    "      \"\"\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--image_file',\n",
    "      type=str,\n",
    "      default='',\n",
    "      help='Absolute path to image file.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--num_top_predictions',\n",
    "      type=int,\n",
    "      default=5,\n",
    "      help='Display this many predictions.'\n",
    "  )\n",
    "  global FLAGS\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  return unparsed\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + setup_args())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"classify_image\">**END Python Program {1.1B-py}: classify_image.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we said in the intro, this course won't go into the details of how TensorFlow and DeepLearning work. We will only comment on the basic elements and parameters that you might want to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will set the TensorFlow compressed model and labeling that you choose to use. It will be downloaded and extracted in the **/tmp/imagenet** folder each time you run this. This is the way to go if you want other people to use your models and have the latest version of it. You can also download it to your package and use it from there. Less overhead each time, but not updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"start_image_recognition\">**Launch File {1.1-launch}: start_image_recognition.launch** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have to launch the script **image_recognition.py** from a roslaunch and re-map the image topic to one of our robots, in this case **Mira Robot**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "\n",
    "    <launch> \n",
    "        <arg name=\"rgb_image_topic\" default=\"/mira/mira/camera1/image_raw\" />\n",
    "    \n",
    "        <node \n",
    "        name=\"rostensorflow_imgae_recognition_node\"\n",
    "        pkg=\"tf_unit1_pkg\"\n",
    "        type=\"image_recognition.py\"\n",
    "        args=\"\"\n",
    "        output=\"screen\">\n",
    "    \n",
    "        <remap from=\"image\" to=\"$(arg rgb_image_topic)\" />\n",
    "    \n",
    "        </node>\n",
    "    \n",
    "   </launch> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"start_image_recognition\">**END Launch File {1.1-launch}: start_image_recognition.launch** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, just comment on the re-mapping. We need to re-map the image topic to the one where Mira is publishing the images. One way to know is by just running the **image_view** node and seeing which topic outputs the image that you want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun rqt_image_view rqt_image_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an image topic and see the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_unit1_rqtimage1.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_unit1_rqtimage2.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"start_image_recognition\">**END Launch File {1.1-launch}: start_image_recognition.launch** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Learn to spawn some objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared some scripts so that you can spawn various objects into the scene, in the position that you want.<br>\n",
    "Here, you have the list of object spawners and an example of how to change their spawning positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roslaunch models_spawn_library_pkg spawn_banana.launch\n",
    "roslaunch models_spawn_library_pkg spawn_coke_can.launch\n",
    "roslaunch models_spawn_library_pkg spawn_cup.launch\n",
    "roslaunch models_spawn_library_pkg spawn_tennisball.launch\n",
    "roslaunch models_spawn_library_pkg spawn_watermelon.launch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is an example of how to change the banana's spawn position in a launch file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<include file=\"$(find models_spawn_library_pkg)/launch/spawn_banana.launch\">\n",
    "    <arg name=\"x\" value=\"0.5\" />\n",
    "    <arg name=\"y\" value=\"0.5\" />\n",
    "    <arg name=\"z\" value=\"0.5\" />\n",
    "</include>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also spawn basic geometric shapes, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  <include file=\"$(find simple_objects_resize)/launch/simple_urdf_spawn.launch\">\n",
    "    <arg name=\"robot_name\" value=\"simple_box\" />\n",
    "    <arg name=\"urdf_robot_file\" value=\"$(find simple_objects_resize)/urdf/simple_box.urdf\" />\n",
    "    <arg name=\"x\" value=\"2.0\" />\n",
    "    <arg name=\"y\" value=\"0.0\" />\n",
    "    <arg name=\"z\" value=\"0.5\" />\n",
    "  </include>\n",
    "  \n",
    "  <include file=\"$(find simple_objects_resize)/launch/simple_urdf_spawn.launch\">\n",
    "    <arg name=\"robot_name\" value=\"simple_cylinder\" />\n",
    "    <arg name=\"urdf_robot_file\" value=\"$(find simple_objects_resize)/urdf/simple_cylinder.urdf\" />\n",
    "    <arg name=\"x\" value=\"0.0\" />\n",
    "    <arg name=\"y\" value=\"2.0\" />\n",
    "    <arg name=\"z\" value=\"0.5\" />\n",
    "  </include>\n",
    "  \n",
    "  <include file=\"$(find simple_objects_resize)/launch/simple_urdf_spawn.launch\">\n",
    "    <arg name=\"robot_name\" value=\"simple_sphere\" />\n",
    "    <arg name=\"urdf_robot_file\" value=\"$(find simple_objects_resize)/urdf/simple_sphere.urdf\" />\n",
    "    <arg name=\"x\" value=\"0.0\" />\n",
    "    <arg name=\"y\" value=\"-2.0\" />\n",
    "    <arg name=\"z\" value=\"0.5\" />\n",
    "  </include>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: You are ready to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we ready to launch the recogniser? Sure, just remember how to move Mira's head around:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun teleop_twist_keyboard teleop_twist_keyboard.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #2</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roslaunch tf_unit1_pkg start_image_recognition.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">END **Example 1.1**</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should start having an output of the recognised objects through **WebShell2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 1.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play around a bit, create the following:\n",
    "* Create a launch file that starts the recognition system and moves the head of Mira Robot to search for four objects. The program is finished when it finds all of them. These objects are: Banana, Cup, TennisBall, and Cube.\n",
    "* Through the launch file, you have to also spawn the objects in a position that Mira Robot has the highest probability of finding them in.\n",
    "* As an EXTRA task, try to create a bash script that spawns the objects randomly, starts the searching, and when found, deletes all of the objects from the scene, and then starts again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise 1.1**</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
