{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3: Train your Own TensorFlow Image Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_image_unit3_intro.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotignite_logo_text.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Estimated time to completion:</b> 2.5-10 hours, depending on the training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Simulated robot:</b> Mira Robot\n",
    "<br><br>\n",
    "<b>What will you learn with this unit?</b>\n",
    "* Label images\n",
    "* Prepare the package for training\n",
    "* Train the model and monitor it through TensorBoard\n",
    "* Use the trained model in a ROS environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">**Example 3.1**</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Intro to Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is the purpose of this unit? Well, quite simple: What happens if you want to recognise something that is not on the ImageNet model list?<br>\n",
    "In this example, you are going to learn all of the steps necessary to make Mira Robot recognise **itself**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Labeling Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, one of the most time-consuming tasks of this example is also the one that is unavoidable, if you want to train a custom element. Luckily for you, you won't have to do it for this example because, in the git you downloaded in **Unit 2**, you have two folders with a huge number of images of the **Mira Robot** labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a look, you will see that you have **two folders.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roscd tf_unit1_pkg/course_tflow_image_student_data;ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* images_1_labels: Has only **ONE** label [mira_robot]\n",
    "* images_2_labels: Has **TWO** labels [mira_robot, object]. All of the objects in the scene were labeled with the tag **object**, while all of the Mira Robots were labeled with **mira_robot**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the **images_1_labels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do you label images? We will use a program that is called <a href=\"https://github.com/tzutalin/labelImg\">LableImg</a>. This program generates **.xml** files based on images and how you label them. It makes the process less painful than writing the files by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice one image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 labelImg/labelImg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something like this if you open the Graphical Interface by clicking on the icon:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/font-awesome_desktop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_images_unit3_label_img.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now upload any image through the IDE. Here, you have an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tesnsorflow_image_unit3_clasify_image.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just open it with the GUI and you can start labeling by clicking the **CreateRectagleBox**, or by pressing the **W** key on the keyboard. Place these boxes where the object you want to label is, to mark it. Here is an example, try it yourself:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tesnsorflow_image_unit3_clasify_image_2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tesnsorflow_image_unit3_clasify_image_3.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have everything you want to train labeled, you have to save. This will generate an **.xml** file with the same name as your image. Do not change the names, if you want to make your life easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tesnsorflow_image_unit3_clasify_image.xml](extra_files/tesnsorflow_image_unit3_clasify_image.xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside it, you have all of the information from the image, as well as the position, size, and label of each box. This will be used for the training of the TensorFlow model to validate the learning."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<annotation>\n",
    "\t<folder>Desktop</folder>\n",
    "\t<filename>tesnsorflow_image_unit3_clasify_image.png</filename>\n",
    "\t<path>/home/rdaneel/Desktop/tesnsorflow_image_unit3_clasify_image.png</path>\n",
    "\t<source>\n",
    "\t\t<database>Unknown</database>\n",
    "\t</source>\n",
    "\t<size>\n",
    "\t\t<width>1095</width>\n",
    "\t\t<height>553</height>\n",
    "\t\t<depth>3</depth>\n",
    "\t</size>\n",
    "\t<segmented>0</segmented>\n",
    "\t<object>\n",
    "\t\t<name>table</name>\n",
    "\t\t<pose>Unspecified</pose>\n",
    "\t\t<truncated>1</truncated>\n",
    "\t\t<difficult>0</difficult>\n",
    "\t\t<bndbox>\n",
    "\t\t\t<xmin>288</xmin>\n",
    "\t\t\t<ymin>413</ymin>\n",
    "\t\t\t<xmax>681</xmax>\n",
    "\t\t\t<ymax>553</ymax>\n",
    "\t\t</bndbox>\n",
    "\t</object>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each box is inside an **object** tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing you have to do is **COPY** **10%** of the images into a **test** folder, and the other **90%** to a **train** folder. The percentages really depend on you. If you are unsure of whether the model works, put more images in the **test** folder.<br>\n",
    "It is **VERY IMPORTANT** that the images that are in the **test** folder **DON'T APPEAR** in the **train** folder. This guarantees that when testing, the training model is tested with images that it doesn't know. Otherwise, it won't learn correctly. It would be like testing students with the extact same exercise that you did yesterday in class... You won't know if they have learned or just have a good memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. Now, you just have to do the same thing for as many images as you need for the training. To make life a bit easier, you can import a full folder with all of the images inside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Image Data for the TensorFlow training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, TensorFlow doesn't use **.xml** files. Instead, it uses a file type called **.records**. So, you have to convert your xml files.<br>\n",
    "This is divided into **two main steps**:<br>\n",
    "* Convert XML to CSV\n",
    "* Convert CSV to RECORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: XML to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You first have to copy the images that we provide you into your working directory package: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the images files to a generic images folder in the scripts directory\n",
    "roscd tf_unit1_pkg\n",
    "rm -rf course_tflow_image_student_data\n",
    "git clone https://bitbucket.org/theconstructcore/course_tflow_image_student_data.git\n",
    "\n",
    "# We clean any data you might have created previously\n",
    "rm -rf data\n",
    "mkdir data\n",
    "    \n",
    "# Generate Image Folder with the images wanted\n",
    "rm -rf images\n",
    "mkdir images\n",
    "cp -a course_tflow_image_student_data/images_1_labels/. images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use this **data** folder afterwards for the **CSV** and **RECORD** files that are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have to make the conversion from **XML** to **CSV**. For this, you will use the following python script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**Python Program {3.1-py}: xml_to_csv.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    for directory in ['train', 'test']:\n",
    "        image_path = os.path.join(os.getcwd(), 'images/{}'.format(directory))\n",
    "        xml_df = xml_to_csv(image_path)\n",
    "        xml_df.to_csv('data/{}_labels.csv'.format(directory), index=None, encoding='utf-8')\n",
    "        print('Successfully converted xml to csv.')\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**END Python Program {3.1-py}: xml_to_csv.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now execute this script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert XML to CSV files\n",
    "roscd tf_unit1_pkg\n",
    "python scripts/xml_to_csv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have two files in the **data** folder: **test_labels.csv** and **train_labels.csv**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: CSV to RECORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to generate the Record files, you need to execute another python script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**Python Program {3.2-py}: generate_tfrecord_n.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "\"\"\"\n",
    "Usage:\n",
    "  # From tensorflow/models/\n",
    "  # Create train data:\n",
    "  python generate_tfrecord.py --csv_input=data/train_labels.csv  --output_path=data/train.record\n",
    "\n",
    "  # Create test data:\n",
    "  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=data/test.record\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "from extract_training_lables_csv import extract_training_labels_csv, class_text_to_int\n",
    "import sys\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "    \n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('image_path_input', '', 'Path to the Images refered to in the CSV')\n",
    "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
    "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "def create_tf_example(group, path, unique_label_array):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    \n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class'], unique_label_array))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
    "    \n",
    "    path = os.path.join(os.getcwd(), FLAGS.image_path_input)\n",
    "    examples = pd.read_csv(FLAGS.csv_input)\n",
    "    unique_label_array = extract_training_labels_csv(examples)\n",
    "    grouped = split(examples, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, path, unique_label_array)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
    "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**END Python Program {3.2-py}: generate_tfrecord_n.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**Python Program {3.2b-py}: extract_training_lables_csv.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_training_labels_csv(csv_object):\n",
    "    \"\"\"\n",
    "    We suppose that in the test and train image sets there are all the labels\n",
    "    \"\"\"\n",
    "    unique_label_set = set([])\n",
    "    d = csv_object.loc[: , \"class\"]\n",
    "    for key,value in d.iteritems():\n",
    "        unique_label_set.add(value)\n",
    "    \n",
    "    # We convert to have functions like sort\n",
    "    unique_label_list = list(unique_label_set)\n",
    "    unique_label_list.sort()\n",
    "    return unique_label_list\n",
    "\n",
    "def class_text_to_int(row_label, unique_label_array):\n",
    "    \"\"\"\n",
    "    The labels will be assigned by alpabetical order\n",
    "    This is very importatnt for the config file for training\n",
    "    \"\"\"\n",
    "    class_integer = int(unique_label_array.index(row_label) + 1)\n",
    "    #print (\"Label=\"+str(row_label)+\", Index=\"+str(class_integer))\n",
    "    return class_integer\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print \"Opening CSV...\"\n",
    "    csv_input_for_labels = \"scripts/dummy1.csv\"\n",
    "    examples = pd.read_csv(csv_input_for_labels)\n",
    "    print \"Opened CSV...\"\n",
    "    unique_label_array = extract_training_labels_csv(examples)\n",
    "    label_contents = \"\"\n",
    "    for lable in unique_label_array:\n",
    "        print \"Generating Index for lable==\"+str(lable)\n",
    "        index = class_text_to_int(lable, unique_label_array)\n",
    "        print \"Label==\"+str(lable)+\", index =\"+str(index)\n",
    "        label_contents += \"item {\\n    id : \"+str(index)+\"\\n    name : \"+str(lable)+\"\\n}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**END Python Program {3.2b-py}: extract_training_lables_csv.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **extract_training_lables_csv.py**, you extract the labels that you are going to use. In this first example, 3.1, the images only have **one label**, which is **mira_robot**. This will therefore, extract the label **mira_robot** from csv files generates previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in **generate_tfrecord_n.py** file, there is a line that we have to know about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import dataset_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you are importing from a folder called **object_detection**. To make this course easier, you are going to download it into your package and set the python path to find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roscd tf_unit1_pkg\n",
    "# Download the git models with the object detection module inside\n",
    "rm -rf models\n",
    "# We dont clone from git because its somewhat unstable , you can try though :git clone https://github.com/tensorflow/models.git\n",
    "cp -r course_tflow_image_student_data/tf_models/models ./\n",
    "# Compile protos messages python modules\n",
    "cd models/research\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "echo \"Check proto python files generated\"\n",
    "ls object_detection/protos/*_pb2.py\n",
    "# We make all the modules inside models/research and also the slim folder inside available anywhere in python interpreter.\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
    "echo $PYTHONPATH\n",
    "roscd tf_unit1_pkg/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you basically copy it from the repo we give you **course_tflow_image_student_data** and compile the **protobuffer** files to generate the python classes for each one. You don't need to know what they do, just know that you need them to be able to use this TensorFlow Library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, you can now execute the **python script generate_tfrecord.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roscd tf_unit1_pkg/scripts\n",
    "python scripts/generate_tfrecord_n.py --image_path_input=images/train --csv_input=data/train_labels.csv  --output_path=data/train.record\n",
    "python scripts/generate_tfrecord_n.py --image_path_input=images/test --csv_input=data/test_labels.csv  --output_path=data/test.record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should generate the **.record** files for the **train** and **test** in the **data** folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3: Extra step: Check that the record files are ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a step that is good to know how to do, although it's not necessary for the training. You have to know how to read what there is inside of a **.record** file, so that you can check other people's files and make sure that everything went ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, you have to launch a python script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**Python Program {3.3-py}: tfrecord_inspector.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print \"TRAIN.RECORD data ==>\"\n",
    "for example in tf.python_io.tf_record_iterator(\"data/train.record\"):\n",
    "    result = tf.train.Example.FromString(example)\n",
    "    print result\n",
    "    \n",
    "print \"TEST.RECORD data ==>\"\n",
    "for example in tf.python_io.tf_record_iterator(\"data/test.record\"):\n",
    "    result = tf.train.Example.FromString(example)\n",
    "    print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**END Python Program {3.3-py}: tfrecord_inspector.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's simply converting the **record** file to a String. You can then save that output into a log file, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Tf Record has been done\n",
    "roscd tf_unit1_pkg\n",
    "rm tfrecord_inspector.log\n",
    "python scripts/tfrecord_inspector.py >> tfrecord_inspector.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then read the **tfrecord_inspector.log** file. Beware that this can be a very big file because it also contains the pixel image data. We recommend that you use VIM to open it, it's the fastest way and it doesn't open all the data at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vim tfrecord_inspector.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then type **\":q\"**, to exit VIM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Copy Model Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, you need a **TensorFlow** model. This is made by a series of files that define the different DeepLearing Neural Network operations. You can find loads of models; some are **faster** than others, some are **more precise** and change depending on their application. Some are just for images, others for sound, still others for stockmarket data... You name it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your case, you have to download the following model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We clean up previous images\n",
    "rm -rf ./models/research/object_detection/images\n",
    "\n",
    "cp -a data/. ./models/research/object_detection/data\n",
    "cp -r images ./models/research/object_detection/\n",
    "cp -r training ./models/research/object_detection/\n",
    "\n",
    "# Copy Selected model from the user\n",
    "cp -r course_tflow_image_student_data/tf_models/ssd_mobilenet_v1_coco_11_06_2017 ./models/research/object_detection/\n",
    "cp course_tflow_image_student_data/tf_models/ssd_mobilenet_v1_coco.config ./models/research/object_detection/training/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you are copying the **ssd_mobilenet_v1_coco.config** file and the **ssd_mobilenet_v1_coco_11_06_2017.tar.gz**. The **.tar.gz** contains the binary information of the model, used by TensorFlow. It's more complex than that, but it's not necessary to know for this course.<br>\n",
    "The **.config** file **IS** important to know because here you will have to change some elements to indicate where to extract the TensorFlow model from, how many labels you have, the basic size of the training images... It configures all of the aspects of the training procedure. We are going to just comment on the essentials. Open the **ssd_mobilenet_v1_coco.config** in the **IDE**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number Of Classes: How many labels you have; in our case, **ONE**, mira_robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model {\n",
    "  ssd {\n",
    "    num_classes: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fine_tune_checkpoint: Here you state the model file path. In our case, **ssd_mobilenet_v1_coco_11_06_2017/model.ckpt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "}\n",
    "  fine_tune_checkpoint: \"ssd_mobilenet_v1_coco_11_06_2017/model.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Batch Size: How many images are used in each training step. If you have RAM memory problems, you have to **LOWER** this value. The minimum is, of course, **1**. But the lower it is , the slower you will train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config: {\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Record File Paths and Label file PBTXT: Where to get the training and test image data, and also the labels list (which is extracted from the **object-detection.pbtxt** file that you are going to create now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"data/train.record\"\n",
    "  }\n",
    "  label_map_path: \"training/object-detection.pbtxt\"\n",
    "}\n",
    "\n",
    "eval_config: {\n",
    "  num_examples: 8000\n",
    "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
    "  # Remove the below line to evaluate indefinitely.\n",
    "  max_evals: 10\n",
    "}\n",
    "\n",
    "eval_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"data/test.record\"\n",
    "  }\n",
    "  label_map_path: \"training/object-detection.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_readers: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, the **ssd_mobilenet_v1_coco.config** should look like the one in the public git, inside the **tf_models** folder. It is advisable that you always download the files from the official source though, just in case there was an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Label List file object-detection.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's create this file called **object-detection.pbtxt**. Here you will state the labels for your training and the **ID** associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roscd tf_unit1_pkg\n",
    "rm -rf training\n",
    "mkdir training\n",
    "python scripts/generate_pbtxt_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at this **generate_pbtxt_file.py** file. Of course you can generate this **pbtxt** file by hand, but this script will give you the power to then change the number of labels much easier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**END Python Program {3.3b-py}: generate_pbtxt_file.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from extract_training_lables_csv import extract_training_labels_csv, class_text_to_int\n",
    "\n",
    "\n",
    "def create_label_contents(csv_input_for_labels):\n",
    "    print \"Opening CSV...\"\n",
    "    examples = pd.read_csv(csv_input_for_labels)\n",
    "    print \"Opened CSV...\"\n",
    "    unique_label_array = extract_training_labels_csv(examples)\n",
    "    label_contents = \"\"\n",
    "    for lable in unique_label_array:\n",
    "        print \"Generating Index for lable==\"+str(lable)\n",
    "        index = class_text_to_int(lable, unique_label_array)\n",
    "        label_contents += \"item {\\n    id : \"+str(index)+\"\\n    name : '\"+str(lable)+\"'\\n}\\n\"\n",
    "    \n",
    "    return label_contents\n",
    "\n",
    "def generate_pbtxt_files(file_path, csv_input_for_labels):\n",
    "    print \"Openening file==\"+str(file_path)\n",
    "    file = open(file_path,'w')\n",
    "    #contents = \"item {\\nid : 1\\nname : 'mira_robot'\\n}\\nitem {\\nid: 2\\nname: 'object'\\n}\"\n",
    "    print \"Start create_label_contents...\"\n",
    "    contents = create_label_contents(csv_input_for_labels)\n",
    "    print \"Done create_label_contents...\"\n",
    "    file.write(contents)\n",
    "    file.close() \n",
    "    print \"Pbtxt Generated...\"+str(file_path)\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    file_path = \"training/object-detection.pbtxt\"\n",
    "    csv_input_for_labels = \"data/train_labels.csv\"\n",
    "    generate_pbtxt_files(file_path, csv_input_for_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**Python Program {3.3b-py}: generate_pbtxt_file.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item {\n",
    "    id : 1\n",
    "    name : 'label1'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'label2'\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "item {\n",
    "  id: n\n",
    "  name: 'labeln'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, because there is only **ONE** label, you just have to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item {\n",
    "    id : 1\n",
    "    name : 'mira_robot'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: It's Time to Train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's the moment of truth. We have to start the training process and cross our fingers that everything was set up correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the object_detection module is inside the python path.<br>\n",
    "If you are launching this in a new WebShell, or you are restarting from here, the python path won't have it.<br>\n",
    "**THIS IS A SOURCE OF ERRORS**, so please make sure to check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roscd tf_unit1_pkg\n",
    "cd models/research\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
    "echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **And now, start the training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roscd tf_unit1_pkg\n",
    "cd models/research/object_detection\n",
    "# We set it so that train.py can find object_recognition\n",
    "python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should see that it starts to output the step times. That means it's training.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_image_unit3_steps.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the scripts gets killed, it means that you chose a **batch_size** that was too big for the processing power you have. Lower it in the **ssd_mobilenet_v1_coco.config** and try again. Also, using a lot of training images might kill the process as well, so again, remove some of the images, redo the previous steps, and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also monitor the system load by executing **top** or **htop** (this last one has more colours). Here you can see the load average and the RAM memory used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tesnorflow_image_unit3_top.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tesnorflow_images_unit3_htop.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Use TensorBoard to check the training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you did in **Unit 2**, you can start the **TensorBoard** client and monitor the training progress. Just run the following commands in another **WebShell**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #2</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate tensorboard:\n",
    "roscd tf_unit1_pkg\n",
    "cd scripts/models/research/object_detection\n",
    "tensorboard --logdir=training/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, connect the TensorBoard client to your browser, as explained in **Unit 2**, and get the IP, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #2</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all was done correctly, you should get something similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_image_unit3_label1_start.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after about **2 hours**, you should have something similar to this. As you can see, the **TotalLoss** has stabilised around the **1.00** value. That's considered to be a model that has already learned and won't get any better. But, basically, you have to stop the learning process when the **TotalLoss** stabilises and mantains its value. Otherwise, you might **overtrain** your model, which is not recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_image_unit3_label1_end.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AB0017;color:white;\">**WARNING**</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that **RobotIgnite** has an automatic system that stops your session if you don't use your session for around 30 minutes, so please continue working on your session while you do the training.<br>\n",
    "If you want a platform to do this training for heavy and complex models that take days, please use our **developement platform** <a href=\"http://www.theconstructsim.com/rds-ros-development-studio/\">ROS Developement Studio</a>. You can have it for free if you want a small system, and you can pay if you want full **GPU Support** and more than **8 Cores** for cutting your training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_images_unit3_rds.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#AB0017;color:white;\">**END WARNING**</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Export Inference Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process generates an **inference graph** at the end. This graph is a file that you will use to load it and make predictions and detections of the objects on a scene, in real time. This makes sense because you only have to do the training once, and then use that knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is export the files to your **scripts folder** for ease of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roscd tf_unit1_pkg\n",
    "cd models/research\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
    "echo $PYTHONPATH\n",
    "\n",
    "roscd tf_unit1_pkg\n",
    "python scripts/extract_newest_ckpt_name.py models/research/object_detection/training/ newest_ckpt.txt\n",
    "index_newest_ckpt=`cat newest_ckpt.txt`\n",
    "    \n",
    "cd models/research/object_detection/\n",
    "rm -rf learned_model\n",
    "echo \"NewestVersion==\"$index_newest_ckpt\n",
    "\n",
    "\n",
    "python export_inference_graph.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --pipeline_config_path training/$model_config_file_name \\\n",
    "    --trained_checkpoint_prefix training/model.ckpt-$index_newest_ckpt \\\n",
    "    --output_directory learned_model\n",
    "ls learned_model\n",
    "\n",
    "roscd tf_unit1_pkg\n",
    "rm -rf learned_model\n",
    "cp -r models/research/object_detection/learned_model ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the **index_newest_ckpt** string in one command? This number is replaced by the **last model version** from the training. To know that, the **extract_newest_ckpt_name.py** file was executed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**Python Program {3.3c-py}: extract_newest_ckpt_name.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def extract_newest_ckpt_name(ckpt_folder_path, output_data_file_path):\n",
    "    \"\"\"\n",
    "    It serach inside the ckpt_folder_path for ckpt files, and saves into the \n",
    "    output_data_file_path the one with the biggest number, which is the most recent one.\n",
    "    \"\"\"\n",
    "    files_list = [f for f in listdir(ckpt_folder_path) if isfile(join(ckpt_folder_path, f))]\n",
    "    print (str(files_list))\n",
    "    matching = [s for s in files_list if \"model.ckpt-\" in s]\n",
    "    print (str(matching))\n",
    "    meta_files_list = [s for s in matching if \".meta\" in s]\n",
    "    print (str(meta_files_list))\n",
    "    \n",
    "    # Get the highest number file model.ckpt-XXX.meta\n",
    "    MAX_index_file_version = 0\n",
    "    for meta_file in meta_files_list:\n",
    "        # filename = model.ckpt-XXX\n",
    "        filename, file_extension = path.splitext(meta_file)\n",
    "        # aux1 = XXX.meta\n",
    "        index_file_version = int(filename.split(\"-\")[1])\n",
    "        if index_file_version > MAX_index_file_version:\n",
    "            MAX_index_file_version = index_file_version\n",
    "    \n",
    "    \n",
    "    print \"MAX_INDEX=\"+str(MAX_index_file_version)\n",
    "    \n",
    "    print \"Opening file==\"+str(output_data_file_path)\n",
    "    file = open(output_data_file_path,'w')\n",
    "    print \"Start create_label_contents...\"\n",
    "    contents = str(MAX_index_file_version)\n",
    "    print \"Done create_label_contents...\"\n",
    "    file.write(contents)\n",
    "    file.close() \n",
    "    print \"Pbtxt Generated...\"+str(output_data_file_path)\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    python scripts/extract_newest_ckpt_name.py /home/user/simulation_ws/src/tensorflow_image_automatic_learning/models/research/object_detection/training/ /home/user/simulation_ws/src/tensorflow_image_automatic_learning/newest_ckpt.txt\n",
    "    \"\"\"\n",
    "    ckpt_folder_path= sys.argv[1] \n",
    "    output_data_file_path= sys.argv[2]\n",
    "    extract_newest_ckpt_name(ckpt_folder_path, output_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**Python Program {3.3c-py}: extract_newest_ckpt_name.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roscd tf_unit1_pkg/scripts\n",
    "cd models/research/object_detection\n",
    "ls training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **extract_newest_ckpt_name.py** file will go instide the **models/research/object_detection/training** folder, and get the **ckpt** file with the highest number. This will we used for the generation of a **frozen-model** named **learned_model**, similar to the one you used in **Unit2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Copy Validation Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also copy validation images into the **test_images** folder, inside **object_detection**, to launch a validation script afterwards. This is crucial to see how well our model does with images that it hasn't seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We copy and rename the images inside test to the test_images dir\n",
    "# I did it manually\n",
    "roscd tf_unit1_pkg\n",
    "cp -a course_tflow_image_student_data/validation_images/. models/research/object_detection/test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are ones that haven't been used in the training or the testing. They don't even have any Mira inside. This is just to test how well they perform in unknown situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Launch the Testing Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to use these **validation images** to test the model. To do so, you have to launch this script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**Python Program {3.4-py}: validate_learning.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import sys\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "#from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from extract_training_lables_csv import extract_training_labels_csv\n",
    "\n",
    "if tf.__version__ < '1.4.0':\n",
    "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
    "  \n",
    "# /home/user/simulation_ws/src/tensorflow_image_automatic_learning\n",
    "path_to_learn_pkg= sys.argv[1]\n",
    "# learned_model\n",
    "PATH_TO_MODEL_LEARNED= sys.argv[2]\n",
    "# my_images/validation\n",
    "PATH_TO_TEST_IMAGES_DIR = sys.argv[3]\n",
    "\n",
    "\n",
    "research_module_path = os.path.join(path_to_learn_pkg,\"models/research\")\n",
    "object_detection_module_path = os.path.join(research_module_path,\"object_detection\")\n",
    "sys.path.append(research_module_path)\n",
    "sys.path.append(object_detection_module_path)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = PATH_TO_MODEL_LEARNED + '/frozen_inference_graph.pb'\n",
    "#final_path_to_ckpt = os.path.join(path_to_learn_pkg,PATH_TO_CKPT)\n",
    "final_path_to_ckpt = PATH_TO_CKPT\n",
    "\n",
    "# List of the strings that is used to add correct label for each box. In our case mira_robot\n",
    "PATH_TO_LABELS = os.path.join(path_to_learn_pkg, 'training/object-detection.pbtxt')\n",
    "\n",
    "csv_input_for_labels=os.path.join(path_to_learn_pkg,'data/train_labels.csv')\n",
    "examples = pd.read_csv(csv_input_for_labels)\n",
    "print \"Opened CSV...\"\n",
    "unique_label_array = extract_training_labels_csv(examples)\n",
    "NUM_CLASSES = len(unique_label_array)\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(final_path_to_ckpt, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "    \n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "      \n",
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "#final_path_to_test_img = os.path.join(path_to_learn_pkg,PATH_TO_TEST_IMAGES_DIR)\n",
    "final_path_to_test_img = PATH_TO_TEST_IMAGES_DIR\n",
    "\n",
    "numberof_validation_img = len([f for f in listdir(final_path_to_test_img) if isfile(join(final_path_to_test_img, f))])\n",
    "\n",
    "TEST_IMAGE_PATHS = [ os.path.join(final_path_to_test_img, 'image{}.jpg'.format(i)) for i in range(1, numberof_validation_img+1) ]\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict\n",
    "  \n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  #plt.figure(figsize=IMAGE_SIZE)\n",
    "  #plt.imshow(image_np)\n",
    "  imgplot = plt.imshow(image_np)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**END Python Program {3.4-py}: validate_learning.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGE_PATHS = [ os.path.join(final_path_to_test_img, 'image{}.jpg'.format(i)) for i in range(1, 6) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line specifies the range of images from the validation folder, which were copied into the **test_images**. They have to all be named something similar to **imageNumber.jpg**. And if you have a range from 1-5,then you have to state **range(1,5+1)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can execute the script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We copy and rename the images inside test to the test_images dir\n",
    "# I did it manually\n",
    "roscd tf_unit1_pkg\n",
    "pwd_now=$(pwd)\n",
    "python ./scripts/validate_learning.py $pwd_now $pwd_now\"/learned_model\" $pwd_now\"/models/research/object_detection/test_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now just have to go to the **Graphical Tools** and you will get the **validations images**, one by one, with their detections. To go to the next one, close the image to make the next one appear. As you may see, not all of them are correctly classified. But that's where adding more images and different modules comes into play. We won't talk about that in this basic course, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/font-awesome_desktop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 3.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it with your own uploaded validation images. See how it recognises people, or if it behaves with other robots, animals or objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise 3.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Launch the Testing Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here comes the **ROS** connection again. So, what we want is for our robot, Mira, to recognise itself in the virtual world. And perhaps in the **real world**. So, we have to make recognitions in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.1: Create the Python script that combines TensorFlow with ROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is a combination of the **test_training.py** and the **image_recognition.py** from **Unit 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**Python Program {3.5-py}: search_for_mira_robot.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "#from matplotlib import pyplot as plt\n",
    "#from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import rospkg\n",
    "import rospy\n",
    "from sensor_msgs.msg import Image\n",
    "from std_msgs.msg import String\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "\n",
    "if tf.__version__ < '1.4.0':\n",
    "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
    "  \n",
    "# get an instance of RosPack with the default search paths\n",
    "rospack = rospkg.RosPack()\n",
    "# get the file path for rospy_tutorials\n",
    "\n",
    "path_to_learn_pkg = rospack.get_path('learn_newobjects_tf_pkg')\n",
    "research_module_path = os.path.join(path_to_learn_pkg,\"scripts/models/research\")\n",
    "object_detection_module_path = os.path.join(path_to_learn_pkg,\"scripts/models/research/object_detection\")\n",
    "sys.path.append(object_detection_module_path)\n",
    "\n",
    "#print(sys.path)\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "# What model to download.\n",
    "MODEL_NAME = 'learned_model'\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "scripts_module_path = os.path.join(path_to_learn_pkg,\"scripts/\")\n",
    "final_path_to_ckpt = os.path.join(scripts_module_path,PATH_TO_CKPT)\n",
    "\n",
    "# List of the strings that is used to add correct label for each box. In our case mira_robot\n",
    "PATH_TO_LABELS = os.path.join(scripts_module_path, 'training/object-detection.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(final_path_to_ckpt, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "    \n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "      \n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for a single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframing is required to translate the mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict\n",
    "  \n",
    "\n",
    "class RosTensorFlow():\n",
    "    def __init__(self):\n",
    "        # Processing the variable to process only half of the frame's lower load\n",
    "        self._process_this_frame = True\n",
    "        self._cv_bridge = CvBridge()\n",
    "\n",
    "        self._sub = rospy.Subscriber('image', Image, self.callback, queue_size=1)\n",
    "        self._pub = rospy.Publisher('result', String, queue_size=1)\n",
    "        self.score_threshold = rospy.get_param('~score_threshold', 0.1)\n",
    "        self.use_top_k = rospy.get_param('~use_top_k', 5)\n",
    "        \n",
    "        \n",
    "\n",
    "    def callback(self, image_msg):\n",
    "        if (self._process_this_frame):\n",
    "            \n",
    "            image_np = self._cv_bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\n",
    "    \n",
    "            # Expand dimensions since the model expects images to have shapes: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            # Actual detection.\n",
    "            output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "            # Visualization of the results of a detection.\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                output_dict['detection_boxes'],\n",
    "                output_dict['detection_classes'],\n",
    "                output_dict['detection_scores'],\n",
    "                category_index,\n",
    "                instance_masks=output_dict.get('detection_masks'),\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=8)\n",
    "            cv2.imshow(\"Image window\", image_np)\n",
    "            cv2.waitKey(1)\n",
    "        else:\n",
    "            pass\n",
    "        # We invert it\n",
    "        self._process_this_frame = not self._process_this_frame\n",
    "        \n",
    "        \n",
    "        \n",
    "    def main(self):\n",
    "        rospy.spin()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rospy.init_node('search_mira_robot_node')\n",
    "    tensor = RosTensorFlow()\n",
    "    tensor.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"import_pb_to_tensorboard\">**END Python Program {3.5-py}: search_for_mira_robot.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.2: Create the launch file for starting the SearchFor MiraRobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just create the launch file in exactly the same way as you did in Unit 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"start_image_recognition\">**Launch File {3.6-launch}: start_search_mira_robot.launch** </p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "\n",
    "<launch> \n",
    "    <arg name=\"rgb_image_topic\" default=\"/mira/mira/camera1/image_raw\" />\n",
    "    \n",
    "    <node \n",
    "    name=\"search_for_mira_robot_node\"\n",
    "    pkg=\"tf_unit1_pkg\"\n",
    "    type=\"search_for_mira_robot.py\"\n",
    "    args=\"\"\n",
    "    output=\"screen\">\n",
    "    \n",
    "    <remap from=\"image\" to=\"$(arg rgb_image_topic)\" />\n",
    "    \n",
    "    </node>\n",
    "    \n",
    "</launch> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"start_image_recognition\">**END Launch File {3.6-launch}: start_search_mira_robot.launch** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, you launch it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Launch the Testing for Mira in the learn_newobjects_tf_pkg main.launch world\n",
    "roslaunch tf_unit1_pkg start_search_mira_robot.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have to go to the **Graphical Tools**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/font-awesome_desktop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_image_unit3_label1_results1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_image_unit3_label1_results2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions of Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably have seen that teaching this model only **ONE** thing has led to making it an **object** recogniser, rather than  having the ability to differentiate between **mira_robot** and **everything else**.<br>\n",
    "So, that's the next step that **you will have to do** in the following exercise, **3.2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#407EAF;color:white;\">END **Example 3.1**</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 3.2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now you have to make Mira Robot **differentiate** between **mira_robots** and **other objects**.<br>\n",
    "This means that you will have to train with **TWO** labels. Therefore, you will have to make all of the necessary modifications so that it trains with the label **mira_robot** and the label **object**.<br>\n",
    "* To make this task less painful, we have already provided a folder with all of the images labeled with two tags. You can find them in the public git **course_tflow_image_student_data/images_2_labels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**END Exercise 3.2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">Solution Exercise 3.2</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please try to do it by yourself, unless you get stuck or need some inspiration. You will learn much more if you fight for each exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotignite_logo_text.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow this link to open the solutions notebook for Unit 3:[solutions_tensofrflow_images_unit3](extra_files/solutions_tensofrflow_images_unit3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning process should look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_image_unit3_15hourslearning_labels2.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a learning process of more than 15 hours. But, as you can see, after about the 8th hour, there is no significant improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your detection process should look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorflow_image_unit3_ex3-2_solution.gif\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">END Solution Exercise 3.2</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
