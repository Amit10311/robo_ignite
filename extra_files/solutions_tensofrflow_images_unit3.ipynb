{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions for Unit 3 TensorFlow Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/robotignite_logo_text.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index: \n",
    "* <a href=\"#SolutionExercise3-2\">Solution Exercise 3.2</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Exercise 3.2 <p id=\"SolutionExercise3-2\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we leave you some of the main files needed to do this exercise. Its supposing that the model graph is exported to a folder named **mira_robot_graph_2_15h** instead of **mira_robot_graph** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the modification, you have to basicaly put 2 instead of only one label, and add that extra label in the PBTXT file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"generate_tfrecord_2\">**Python Program {Sol-3.1-py}: generate_tfrecord_2.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "\"\"\"\n",
    "Usage:\n",
    "  # From tensorflow/models/\n",
    "  # Create train data:\n",
    "  python generate_tfrecord.py --csv_input=data/train_labels.csv  --output_path=data/train.record\n",
    "\n",
    "  # Create test data:\n",
    "  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=data/test.record\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "import sys\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "    \n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
    "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "# TO-DO replace this with label map\n",
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'mira_robot':\n",
    "        return 1\n",
    "    elif row_label == 'object':\n",
    "        return 2\n",
    "    else:\n",
    "        None\n",
    "\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
    "    path = os.path.join(os.getcwd(), 'images')\n",
    "    examples = pd.read_csv(FLAGS.csv_input)\n",
    "    grouped = split(examples, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
    "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"generate_tfrecord_2\">**END Python Program {Sol-3.1-py}: generate_tfrecord_2.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"object-detection\">**Pbtxt Program {Sol-3.2-pbtxt}: object-detection.pbtxt** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item {\n",
    "    id : 1\n",
    "    name : 'mira_robot'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'object'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"object-detection\">**END Pbtxt Program {Sol-3.2-pbtxt}: object-detection.pbtxt** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"ssd_mobilenet_v1_coco\">**Config File {Sol-3.3-config}: ssd_mobilenet_v1_coco.config** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD with Mobilenet v1 configuration for MSCOCO Dataset.\n",
    "# Users should configure the fine_tune_checkpoint field in the train config as\n",
    "# well as the label_map_path and input_path fields in the train_input_reader and\n",
    "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
    "# should be configured.\n",
    "\n",
    "model {\n",
    "  ssd {\n",
    "    num_classes: 2\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      ssd_anchor_generator {\n",
    "        num_layers: 6\n",
    "        min_scale: 0.2\n",
    "        max_scale: 0.95\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        aspect_ratios: 3.0\n",
    "        aspect_ratios: 0.3333\n",
    "      }\n",
    "    }\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 300\n",
    "        width: 300\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      convolutional_box_predictor {\n",
    "        min_depth: 0\n",
    "        max_depth: 0\n",
    "        num_layers_before_predictor: 0\n",
    "        use_dropout: false\n",
    "        dropout_keep_probability: 0.8\n",
    "        kernel_size: 1\n",
    "        box_code_size: 4\n",
    "        apply_sigmoid_to_scores: false\n",
    "        conv_hyperparams {\n",
    "          activation: RELU_6,\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 0.00004\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            truncated_normal_initializer {\n",
    "              stddev: 0.03\n",
    "              mean: 0.0\n",
    "            }\n",
    "          }\n",
    "          batch_norm {\n",
    "            train: true,\n",
    "            scale: true,\n",
    "            center: true,\n",
    "            decay: 0.9997,\n",
    "            epsilon: 0.001,\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: 'ssd_mobilenet_v1'\n",
    "      min_depth: 16\n",
    "      depth_multiplier: 1.0\n",
    "      conv_hyperparams {\n",
    "        activation: RELU_6,\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 0.00004\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          truncated_normal_initializer {\n",
    "            stddev: 0.03\n",
    "            mean: 0.0\n",
    "          }\n",
    "        }\n",
    "        batch_norm {\n",
    "          train: true,\n",
    "          scale: true,\n",
    "          center: true,\n",
    "          decay: 0.9997,\n",
    "          epsilon: 0.001,\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    loss {\n",
    "      classification_loss {\n",
    "        weighted_sigmoid {\n",
    "        }\n",
    "      }\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      hard_example_miner {\n",
    "        num_hard_examples: 3000\n",
    "        iou_threshold: 0.99\n",
    "        loss_type: CLASSIFICATION\n",
    "        max_negatives_per_positive: 3\n",
    "        min_negatives_per_image: 0\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 1e-8\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "train_config: {\n",
    "  batch_size: 24\n",
    "  optimizer {\n",
    "    rms_prop_optimizer: {\n",
    "      learning_rate: {\n",
    "        exponential_decay_learning_rate {\n",
    "          initial_learning_rate: 0.004\n",
    "          decay_steps: 800720\n",
    "          decay_factor: 0.95\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.9\n",
    "      decay: 0.9\n",
    "      epsilon: 1.0\n",
    "    }\n",
    "  }\n",
    "  fine_tune_checkpoint: \"ssd_mobilenet_v1_coco_11_06_2017/model.ckpt\"\n",
    "  from_detection_checkpoint: true\n",
    "  # Note: The below line limits the training process to 200K steps, which we\n",
    "  # empirically found to be sufficient enough to train the pets dataset. This\n",
    "  # effectively bypasses the learning rate schedule (the learning rate will\n",
    "  # never decay). Remove the below line to train indefinitely.\n",
    "  num_steps: 200000\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    ssd_random_crop {\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "train_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"data/train.record\"\n",
    "  }\n",
    "  label_map_path: \"training/object-detection.pbtxt\"\n",
    "}\n",
    "\n",
    "eval_config: {\n",
    "  num_examples: 8000\n",
    "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
    "  # Remove the below line to evaluate indefinitely.\n",
    "  max_evals: 10\n",
    "}\n",
    "\n",
    "eval_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"data/test.record\"\n",
    "  }\n",
    "  label_map_path: \"training/object-detection.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_readers: 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"ssd_mobilenet_v1_coco\">**END Config File {Sol-3.3-config}: ssd_mobilenet_v1_coco.config** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"test_learning\">**Python Program {Sol-3.4-py}: test_learning.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "#from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import rospkg\n",
    "\n",
    "if tf.__version__ < '1.4.0':\n",
    "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
    "  \n",
    "# get an instance of RosPack with the default search paths\n",
    "rospack = rospkg.RosPack()\n",
    "# get the file path for rospy_tutorials\n",
    "\n",
    "path_to_learn_pkg = rospack.get_path('learn_newobjects_tf_pkg')\n",
    "research_module_path = os.path.join(path_to_learn_pkg,\"scripts/models/research\")\n",
    "object_detection_module_path = os.path.join(path_to_learn_pkg,\"scripts/models/research/object_detection\")\n",
    "sys.path.append(object_detection_module_path)\n",
    "\n",
    "#print(sys.path)\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "# What model to download.\n",
    "MODEL_NAME = 'mira_robot_graph_2_15h'\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "scripts_module_path = os.path.join(path_to_learn_pkg,\"scripts/\")\n",
    "final_path_to_ckpt = os.path.join(scripts_module_path,PATH_TO_CKPT)\n",
    "#final_path_to_ckpt = os.path.join(object_detection_module_path,PATH_TO_CKPT)\n",
    "\n",
    "\n",
    "\n",
    "# List of the strings that is used to add correct label for each box. In our case mira_robot\n",
    "PATH_TO_LABELS = os.path.join('training', 'object-detection.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(final_path_to_ckpt, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "    \n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "      \n",
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = 'validation_images'\n",
    "#final_path_to_test_img = os.path.join(object_detection_module_path,PATH_TO_TEST_IMAGES_DIR)\n",
    "final_path_to_test_img = os.path.join(scripts_module_path,PATH_TO_TEST_IMAGES_DIR)\n",
    "\n",
    "\n",
    "TEST_IMAGE_PATHS = [ os.path.join(final_path_to_test_img, 'image{}.jpg'.format(i)) for i in range(1, 7) ]\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict\n",
    "  \n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  #plt.figure(figsize=IMAGE_SIZE)\n",
    "  #plt.imshow(image_np)\n",
    "  imgplot = plt.imshow(image_np)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"test_learning\">**END Python Program {Sol-3.4-py}: test_learning.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"search_for_mira_robot\">**Python Program {Sol-3.5-py}: search_for_mira_robot.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "#from matplotlib import pyplot as plt\n",
    "#from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import rospkg\n",
    "import rospy\n",
    "from sensor_msgs.msg import Image\n",
    "from std_msgs.msg import String\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "\n",
    "if tf.__version__ < '1.4.0':\n",
    "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
    "  \n",
    "# get an instance of RosPack with the default search paths\n",
    "rospack = rospkg.RosPack()\n",
    "# get the file path for rospy_tutorials\n",
    "\n",
    "path_to_learn_pkg = rospack.get_path('learn_newobjects_tf_pkg')\n",
    "research_module_path = os.path.join(path_to_learn_pkg,\"scripts/models/research\")\n",
    "object_detection_module_path = os.path.join(path_to_learn_pkg,\"scripts/models/research/object_detection\")\n",
    "sys.path.append(object_detection_module_path)\n",
    "\n",
    "#print(sys.path)\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "# What model to download.\n",
    "MODEL_NAME = 'mira_robot_graph_2_15h'\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "scripts_module_path = os.path.join(path_to_learn_pkg,\"scripts/\")\n",
    "final_path_to_ckpt = os.path.join(scripts_module_path,PATH_TO_CKPT)\n",
    "#final_path_to_ckpt = os.path.join(object_detection_module_path,PATH_TO_CKPT)\n",
    "\n",
    "\n",
    "\n",
    "# List of the strings that is used to add correct label for each box. In our case mira_robot\n",
    "PATH_TO_LABELS = os.path.join(scripts_module_path, 'training/object-detection.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(final_path_to_ckpt, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "    \n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "      \n",
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "#PATH_TO_TEST_IMAGES_DIR = 'validation_images'\n",
    "#final_path_to_test_img = os.path.join(object_detection_module_path,PATH_TO_TEST_IMAGES_DIR)\n",
    "#final_path_to_test_img = os.path.join(scripts_module_path,PATH_TO_TEST_IMAGES_DIR)\n",
    "\n",
    "\n",
    "#TEST_IMAGE_PATHS = [ os.path.join(final_path_to_test_img, 'image{}.jpg'.format(i)) for i in range(1, 7) ]\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict\n",
    "  \n",
    "\n",
    "class RosTensorFlow():\n",
    "    def __init__(self):\n",
    "        # Processing variable to process only half of the frames lower load\n",
    "        self._process_this_frame = True\n",
    "        self._cv_bridge = CvBridge()\n",
    "\n",
    "        self._sub = rospy.Subscriber('image', Image, self.callback, queue_size=1)\n",
    "        self._pub = rospy.Publisher('result', String, queue_size=1)\n",
    "        self.score_threshold = rospy.get_param('~score_threshold', 0.1)\n",
    "        self.use_top_k = rospy.get_param('~use_top_k', 5)\n",
    "        \n",
    "        \n",
    "\n",
    "    def callback(self, image_msg):\n",
    "        if (self._process_this_frame):\n",
    "            \n",
    "            image_np = self._cv_bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\n",
    "    \n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            # Actual detection.\n",
    "            output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "            # Visualization of the results of a detection.\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                output_dict['detection_boxes'],\n",
    "                output_dict['detection_classes'],\n",
    "                output_dict['detection_scores'],\n",
    "                category_index,\n",
    "                instance_masks=output_dict.get('detection_masks'),\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=8)\n",
    "            cv2.imshow(\"Image window\", image_np)\n",
    "            cv2.waitKey(1)\n",
    "        else:\n",
    "            pass\n",
    "        # We invert it\n",
    "        self._process_this_frame = not self._process_this_frame\n",
    "        \n",
    "        \n",
    "        \n",
    "    def main(self):\n",
    "        rospy.spin()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rospy.init_node('search_mira_robot_node')\n",
    "    tensor = RosTensorFlow()\n",
    "    tensor.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"search_for_mira_robot\">**END Python Program {Sol-3.5-py}: search_for_mira_robot.py** </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
