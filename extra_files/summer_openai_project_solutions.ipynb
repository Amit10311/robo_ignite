{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions for OpenAI ROS Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/robotignite_logo_text.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"#SolutionStep1\">Solution Step 1</a>\n",
    "* <a href=\"#SolutionStep2\">Solution Step 2</a>\n",
    "* <a href=\"#SolutionStep3\">Solution Step 3</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Step 1: Create Robot Environment <p id=\"SolutionStep1\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**Python File: hopper_robot_env.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import rospy\n",
    "import time\n",
    "from openai_ros import robot_gazebo_env\n",
    "from gazebo_msgs.msg import ContactsState\n",
    "from sensor_msgs.msg import Imu\n",
    "from nav_msgs.msg import Odometry\n",
    "from geometry_msgs.msg import Point, Quaternion, Vector3\n",
    "from sensor_msgs.msg import JointState\n",
    "from std_msgs.msg import Float64\n",
    "\n",
    "\n",
    "\n",
    "class HopperEnv(robot_gazebo_env.RobotGazeboEnv):\n",
    "    \"\"\"Superclass for all HopperEnv environments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes a new HopperEnv environment.\n",
    "        \n",
    "        To check any topic we need to have the simulations running, we need to do two things:\n",
    "        1) Unpause the simulation: without that th stream of data doesnt flow. This is for simulations\n",
    "        that are pause for whatever the reason\n",
    "        2) If the simulation was running already for some reason, we need to reset the controlers.\n",
    "        This has to do with the fact that some plugins with tf, dont understand the reset of the simulation\n",
    "        and need to be reseted to work properly.\n",
    "        \n",
    "        The Sensors: The sensors accesible are the ones considered usefull for AI learning.\n",
    "        \n",
    "        Sensor Topic List:\n",
    "        * /drone/down_camera/image_raw: RGB Camera facing down.\n",
    "        * /drone/front_camera/image_raw: RGB Camera facing front.\n",
    "        * /drone/imu: IMU of the drone giving acceleration and orientation relative to world.\n",
    "        * /drone/sonar: Sonar readings facing front\n",
    "        * /drone/gt_pose: Get position and orientation in Global space\n",
    "        * /drone/gt_vel: Get the linear velocity , the angular doesnt record anything.\n",
    "        \n",
    "        Actuators Topic List: \n",
    "        * /cmd_vel: Move the Drone Around when you have taken off.\n",
    "        * /drone/takeoff: Publish into it to take off\n",
    "        * /drone/land: Publish to make ParrotDrone Land\n",
    "        \n",
    "        Args:\n",
    "        \"\"\"\n",
    "        rospy.logdebug(\"Start HopperEnv INIT...\")\n",
    "        # Variables that we give through the constructor.\n",
    "        # None in this case\n",
    "\n",
    "        # Internal Vars\n",
    "        # Doesnt have any accesibles\n",
    "        self.controllers_list = ['joint_state_controller',\n",
    "                                 'haa_joint_position_controller',\n",
    "                                 'hfe_joint_position_controller',\n",
    "                                 'kfe_joint_position_controller']\n",
    "\n",
    "        # It doesnt use namespace\n",
    "        self.robot_name_space = \"monoped\"\n",
    "\n",
    "        # We launch the init function of the Parent Class robot_gazebo_env.RobotGazeboEnv\n",
    "        super(HopperEnv, self).__init__(controllers_list=self.controllers_list,\n",
    "                                            robot_name_space=self.robot_name_space,\n",
    "                                            reset_controls=False,\n",
    "                                            start_init_physics_parameters=False,\n",
    "                                            reset_world_or_sim=\"WORLD\")\n",
    "\n",
    "\n",
    "\n",
    "        rospy.logdebug(\"HopperEnv unpause1...\")\n",
    "        self.gazebo.unpauseSim()\n",
    "        #self.controllers_object.reset_controllers()\n",
    "        \n",
    "        self._check_all_systems_ready()\n",
    "\n",
    "\n",
    "        # We Start all the ROS related Subscribers and publishers\n",
    "        rospy.Subscriber(\"/odom\", Odometry, self._odom_callback)\n",
    "        # We use the IMU for orientation and linearacceleration detection\n",
    "        rospy.Subscriber(\"/monoped/imu/data\", Imu, self._imu_callback)\n",
    "        # We use it to get the contact force, to know if its in the air or stumping too hard.\n",
    "        rospy.Subscriber(\"/lowerleg_contactsensor_state\", ContactsState, self._contact_callback)\n",
    "        # We use it to get the joints positions and calculate the reward associated to it\n",
    "        rospy.Subscriber(\"/monoped/joint_states\", JointState, self._joints_state_callback)\n",
    "        \n",
    "\n",
    "        self.publishers_array = []\n",
    "        self._haa_joint_pub = rospy.Publisher('/monoped/haa_joint_position_controller/command', Float64, queue_size=1)\n",
    "        self._hfe_joint_pub = rospy.Publisher('/monoped/hfe_joint_position_controller/command', Float64, queue_size=1)\n",
    "        self._kfe_joint_pub = rospy.Publisher('/monoped/kfe_joint_position_controller/command', Float64, queue_size=1)\n",
    "        \n",
    "        self.publishers_array.append(self._haa_joint_pub)\n",
    "        self.publishers_array.append(self._hfe_joint_pub)\n",
    "        self.publishers_array.append(self._kfe_joint_pub)\n",
    "\n",
    "        self._check_all_publishers_ready()\n",
    "\n",
    "        self.gazebo.pauseSim()\n",
    "        \n",
    "        rospy.logdebug(\"Finished HopperEnv INIT...\")\n",
    "\n",
    "    # Methods needed by the RobotGazeboEnv\n",
    "    # ----------------------------\n",
    "    \n",
    "\n",
    "    def _check_all_systems_ready(self):\n",
    "        \"\"\"\n",
    "        Checks that all the sensors, publishers and other simulation systems are\n",
    "        operational.\n",
    "        \"\"\"\n",
    "        rospy.logdebug(\"HopperEnv check_all_systems_ready...\")\n",
    "        self._check_all_sensors_ready()\n",
    "        rospy.logdebug(\"END HopperEnv _check_all_systems_ready...\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    # CubeSingleDiskEnv virtual methods\n",
    "    # ----------------------------\n",
    "\n",
    "    def _check_all_sensors_ready(self):\n",
    "        rospy.logdebug(\"START ALL SENSORS READY\")\n",
    "        self._check_odom_ready()\n",
    "        self._check_imu_ready()\n",
    "        self._check_lowerleg_contactsensor_state_ready()\n",
    "        self._check_joint_states_ready()\n",
    "        rospy.logdebug(\"ALL SENSORS READY\")\n",
    "\n",
    "        \n",
    "    def _check_odom_ready(self):\n",
    "        self.odom = None\n",
    "        rospy.logdebug(\"Waiting for /odom to be READY...\")\n",
    "        while self.odom is None and not rospy.is_shutdown():\n",
    "            try:\n",
    "                self.odom = rospy.wait_for_message(\"/odom\", Odometry, timeout=1.0)\n",
    "                rospy.logdebug(\"Current /odom READY=>\")\n",
    "\n",
    "            except:\n",
    "                rospy.logerr(\"Current /odom not ready yet, retrying for getting odom\")\n",
    "        return self.odom\n",
    "        \n",
    "        \n",
    "    def _check_imu_ready(self):\n",
    "        self.imu = None\n",
    "        rospy.logdebug(\"Waiting for /monoped/imu/data to be READY...\")\n",
    "        while self.imu is None and not rospy.is_shutdown():\n",
    "            try:\n",
    "                self.imu = rospy.wait_for_message(\"/monoped/imu/data\", Imu, timeout=1.0)\n",
    "                rospy.logdebug(\"Current /monoped/imu/data READY=>\")\n",
    "\n",
    "            except:\n",
    "                rospy.logerr(\"Current /monoped/imu/data not ready yet, retrying for getting imu\")\n",
    "        return self.imu\n",
    "\n",
    "        \n",
    "    def _check_lowerleg_contactsensor_state_ready(self):\n",
    "        self.lowerleg_contactsensor_state = None\n",
    "        rospy.logdebug(\"Waiting for /lowerleg_contactsensor_state to be READY...\")\n",
    "        while self.lowerleg_contactsensor_state is None and not rospy.is_shutdown():\n",
    "            try:\n",
    "                self.lowerleg_contactsensor_state = rospy.wait_for_message(\"/lowerleg_contactsensor_state\", ContactsState, timeout=1.0)\n",
    "                rospy.logdebug(\"Current /lowerleg_contactsensor_state READY=>\")\n",
    "\n",
    "            except:\n",
    "                rospy.logerr(\"Current /lowerleg_contactsensor_state not ready yet, retrying for getting lowerleg_contactsensor_state\")\n",
    "        return self.lowerleg_contactsensor_state\n",
    "        \n",
    "    def _check_joint_states_ready(self):\n",
    "        self.joint_states = None\n",
    "        rospy.logdebug(\"Waiting for /monoped/joint_states to be READY...\")\n",
    "        while self.joint_states is None and not rospy.is_shutdown():\n",
    "            try:\n",
    "                self.joint_states = rospy.wait_for_message(\"/monoped/joint_states\", JointState, timeout=1.0)\n",
    "                rospy.logdebug(\"Current /monoped/joint_states READY=>\")\n",
    "\n",
    "            except:\n",
    "                rospy.logerr(\"Current /monoped/joint_states not ready yet, retrying for getting joint_states\")\n",
    "        return self.joint_states\n",
    "\n",
    "\n",
    "\n",
    "    def _odom_callback(self, data):\n",
    "        self.odom = data\n",
    "    \n",
    "    def _imu_callback(self, data):\n",
    "        self.imu = data\n",
    "        \n",
    "    def _contact_callback(self, data):\n",
    "        self.lowerleg_contactsensor_state = data\n",
    "        \n",
    "    def _joints_state_callback(self, data):\n",
    "        self.joint_states = data\n",
    "\n",
    "\n",
    "    def _check_all_publishers_ready(self):\n",
    "        \"\"\"\n",
    "        Checks that all the publishers are working\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        rospy.logdebug(\"START ALL SENSORS READY\")\n",
    "        for publisher_object in self.publishers_array:\n",
    "            self._check_pub_connection(publisher_object)\n",
    "        rospy.logdebug(\"ALL SENSORS READY\")\n",
    "\n",
    "    def _check_pub_connection(self, publisher_object):\n",
    "\n",
    "        rate = rospy.Rate(10)  # 10hz\n",
    "        while publisher_object.get_num_connections() == 0 and not rospy.is_shutdown():\n",
    "            rospy.logdebug(\"No susbribers to publisher_object yet so we wait and try again\")\n",
    "            try:\n",
    "                rate.sleep()\n",
    "            except rospy.ROSInterruptException:\n",
    "                # This is to avoid error when world is rested, time when backwards.\n",
    "                pass\n",
    "        rospy.logdebug(\"publisher_object Publisher Connected\")\n",
    "\n",
    "        rospy.logdebug(\"All Publishers READY\")\n",
    "        \n",
    "    \n",
    "    # Methods that the TrainingEnvironment will need to define here as virtual\n",
    "    # because they will be used in RobotGazeboEnv GrandParentClass and defined in the\n",
    "    # TrainingEnvironment.\n",
    "    # ----------------------------\n",
    "    def _set_init_pose(self):\n",
    "        \"\"\"Sets the Robot in its init pose\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _init_env_variables(self):\n",
    "        \"\"\"Inits variables needed to be initialised each time we reset at the start\n",
    "        of an episode.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _compute_reward(self, observations, done):\n",
    "        \"\"\"Calculates the reward to give based on the observations given.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _set_action(self, action):\n",
    "        \"\"\"Applies the given action to the simulation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _is_done(self, observations):\n",
    "        \"\"\"Checks if episode done based on observations given.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    # Methods that the TrainingEnvironment will need.\n",
    "    # ----------------------------\n",
    "    def move_joints(self, joints_array, epsilon=0.05, update_rate=10):\n",
    "        \"\"\"\n",
    "        It will move the Hopper Joints to the given Joint_Array values\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        for publisher_object in self.publishers_array:\n",
    "          joint_value = Float64()\n",
    "          joint_value.data = joints_array[i]\n",
    "          rospy.logdebug(\"JointsPos>>\"+str(joint_value))\n",
    "          publisher_object.publish(joint_value)\n",
    "          i += 1\n",
    "        self.wait_time_for_execute_movement(joints_array, epsilon, update_rate)\n",
    "    \n",
    "    def wait_time_for_execute_movement(self, joints_array, epsilon, update_rate):\n",
    "        \"\"\"\n",
    "        We wait until Joints are where we asked them to be based on the joints_states\n",
    "        :param joints_array:Joints Values in radians of each of the three joints of hopper leg.\n",
    "        :param epsilon: Error acceptable in odometry readings.\n",
    "        :param update_rate: Rate at which we check the joint_states.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        rospy.logdebug(\"START wait_until_twist_achieved...\")\n",
    "        \n",
    "        rate = rospy.Rate(update_rate)\n",
    "        start_wait_time = rospy.get_rostime().to_sec()\n",
    "        end_wait_time = 0.0\n",
    "\n",
    "        rospy.logdebug(\"Desired JointsState>>\" + str(joints_array))\n",
    "        rospy.logdebug(\"epsilon>>\" + str(epsilon))\n",
    "        \n",
    "        while not rospy.is_shutdown():\n",
    "            current_joint_states = self._check_joint_states_ready()\n",
    "            \n",
    "            values_to_check = [ current_joint_states.position[0],\n",
    "                                current_joint_states.position[1],\n",
    "                                current_joint_states.position[2]]\n",
    "            \n",
    "            vel_values_are_close = self.check_array_similar(joints_array,values_to_check,epsilon)\n",
    "            \n",
    "            if vel_values_are_close:\n",
    "                rospy.logdebug(\"Reached JointStates!\")\n",
    "                end_wait_time = rospy.get_rostime().to_sec()\n",
    "                break\n",
    "            rospy.logdebug(\"Not there yet, keep waiting...\")\n",
    "            rate.sleep()\n",
    "        delta_time = end_wait_time- start_wait_time\n",
    "        rospy.logdebug(\"[Wait Time=\" + str(delta_time)+\"]\")\n",
    "        \n",
    "        rospy.logdebug(\"END wait_until_jointstate_achieved...\")\n",
    "        \n",
    "        return delta_time\n",
    "    \n",
    "    def check_array_similar(self,ref_value_array,check_value_array,epsilon):\n",
    "        \"\"\"\n",
    "        It checks if the check_value id similar to the ref_value\n",
    "        \"\"\"\n",
    "        rospy.logdebug(\"ref_value_array=\"+str(ref_value_array))\n",
    "        rospy.logdebug(\"check_value_array=\"+str(check_value_array))\n",
    "        return numpy.allclose(ref_value_array, check_value_array, atol=epsilon)\n",
    "    \n",
    "\n",
    "    def get_odom(self):\n",
    "        return self.odom\n",
    "    \n",
    "    def get_imu(self):\n",
    "        return self.imu\n",
    "    \n",
    "    def get_lowerleg_contactsensor_state(self):\n",
    "        return self.lowerleg_contactsensor_state\n",
    "        \n",
    "    def get_joint_states(self):\n",
    "        return self.joint_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**END Python File: hopper_robot_env.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Step 2: Create Task Environment <p id=\"SolutionStep2\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**Python File: hopper_task_env.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rospy\n",
    "import numpy\n",
    "from gym import spaces\n",
    "import hopper_robot_env\n",
    "from gym.envs.registration import register\n",
    "from geometry_msgs.msg import Point\n",
    "from geometry_msgs.msg import Vector3\n",
    "from tf.transformations import euler_from_quaternion\n",
    "\n",
    "timestep_limit_per_episode = 10000 # Can be any Value\n",
    "\n",
    "register(\n",
    "        id='HopperStayUp-v0',\n",
    "        entry_point='hopper_task_env:HopperStayUpEnv',\n",
    "        timestep_limit=timestep_limit_per_episode,\n",
    "    )\n",
    "\n",
    "class HopperStayUpEnv(hopper_robot_env.HopperEnv):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Make Hopper Learn how to Stay Up indefenitly\n",
    "        \"\"\"\n",
    "        \n",
    "        # Only variable needed to be set here\n",
    "        \"\"\"\n",
    "        For this version, we consider 6 actions\n",
    "        1-2) Increment/Decrement haa_joint\n",
    "        3-4) Increment/Decrement hfe_joint\n",
    "        5-6) Increment/Decrement kfe_joint\n",
    "        \"\"\"\n",
    "        rospy.logdebug(\"Start HopperStayUpEnv INIT...\")\n",
    "        number_actions = rospy.get_param('/monoped/n_actions')\n",
    "        self.action_space = spaces.Discrete(number_actions)\n",
    "        \n",
    "        # We set the reward range, which is not compulsory but here we do it.\n",
    "        self.reward_range = (-numpy.inf, numpy.inf)\n",
    "        \n",
    "        \n",
    "        # Actions and Observations\n",
    "        \n",
    "        self.init_joint_states = Vector3()\n",
    "        self.init_joint_states.x = rospy.get_param('/monoped/init_joint_states/haa_joint')\n",
    "        self.init_joint_states.y = rospy.get_param('/monoped/init_joint_states/hfe_joint')\n",
    "        self.init_joint_states.z = rospy.get_param('/monoped/init_joint_states/kfe_joint')\n",
    "        \n",
    "        \n",
    "        # Get Desired Point to Get\n",
    "        self.desired_point = Point()\n",
    "        self.desired_point.x = rospy.get_param(\"/monoped/desired_point/x\")\n",
    "        self.desired_point.y = rospy.get_param(\"/monoped/desired_point/y\")\n",
    "        self.desired_point.z = rospy.get_param(\"/monoped/desired_point/z\")\n",
    "        self.accepted_error_in_des_pos = rospy.get_param(\"/monoped/accepted_error_in_des_pos\")\n",
    "        \n",
    "        self.desired_yaw = rospy.get_param(\"/monoped/desired_yaw\")\n",
    "        \n",
    "        self.joint_increment_value = rospy.get_param(\"/monoped/joint_increment_value\")\n",
    "        self.accepted_joint_error = rospy.get_param(\"/monoped/accepted_joint_error\")\n",
    "        self.update_rate = rospy.get_param(\"/monoped/update_rate\")\n",
    "\n",
    "        self.dec_obs = rospy.get_param(\"/monoped/number_decimals_precision_obs\")\n",
    "        \n",
    "        self.desired_force = rospy.get_param(\"/monoped/desired_force\")\n",
    "        \n",
    "        self.max_x_pos = rospy.get_param(\"/monoped/max_x_pos\")\n",
    "        self.max_y_pos = rospy.get_param(\"/monoped/max_y_pos\")\n",
    "        \n",
    "        self.min_height = rospy.get_param(\"/monoped/min_height\")\n",
    "        self.max_height = rospy.get_param(\"/monoped/max_height\")\n",
    "        \n",
    "        self.distance_from_desired_point_max = rospy.get_param(\"/monoped/distance_from_desired_point_max\")\n",
    "        \n",
    "        self.max_incl_roll = rospy.get_param(\"/monoped/max_incl\")\n",
    "        self.max_incl_pitch = rospy.get_param(\"/monoped/max_incl\")\n",
    "        self.max_contact_force = rospy.get_param(\"/monoped/max_contact_force\")\n",
    "        \n",
    "        self.maximum_haa_joint = rospy.get_param(\"/monoped/max_incl\")\n",
    "        self.maximum_hfe_joint = rospy.get_param(\"/monoped/max_incl\")\n",
    "        self.maximum_kfe_joint = rospy.get_param(\"/monoped/max_incl\")\n",
    "        self.min_kfe_joint = rospy.get_param(\"/monoped/max_incl\")\n",
    "        \n",
    "        # We place the Maximum and minimum values of observations\n",
    "\n",
    "        high = numpy.array([self.distance_from_desired_point_max,\n",
    "                            self.max_incl_roll,\n",
    "                            self.max_incl_pitch,\n",
    "                            3.14,\n",
    "                            self.max_contact_force,\n",
    "                            self.maximum_haa_joint,\n",
    "                            self.maximum_hfe_joint,\n",
    "                            self.maximum_kfe_joint,\n",
    "                            self.max_x_pos,\n",
    "                            self.max_y_pos,\n",
    "                            self.max_height\n",
    "                            ])\n",
    "                                        \n",
    "        low = numpy.array([ 0.0,\n",
    "                            -1*self.max_incl_roll,\n",
    "                            -1*self.max_incl_pitch,\n",
    "                            -1*3.14,\n",
    "                            0.0,\n",
    "                            self.maximum_haa_joint,\n",
    "                            self.maximum_hfe_joint,\n",
    "                            self.min_kfe_joint,\n",
    "                            -1*self.max_x_pos,\n",
    "                            -1*self.max_y_pos,\n",
    "                            self.min_height\n",
    "                            ])\n",
    "\n",
    "        \n",
    "        self.observation_space = spaces.Box(low, high)\n",
    "        \n",
    "        rospy.logdebug(\"ACTION SPACES TYPE===>\"+str(self.action_space))\n",
    "        rospy.logdebug(\"OBSERVATION SPACES TYPE===>\"+str(self.observation_space))\n",
    "        \n",
    "        # Rewards\n",
    "        self.weight_joint_position = rospy.get_param(\"/monoped/rewards_weight/weight_joint_position\")\n",
    "        self.weight_contact_force = rospy.get_param(\"/monoped/rewards_weight/weight_contact_force\")\n",
    "        self.weight_orientation = rospy.get_param(\"/monoped/rewards_weight/weight_orientation\")\n",
    "        self.weight_distance_from_des_point = rospy.get_param(\"/monoped/rewards_weight/weight_distance_from_des_point\")\n",
    "        \n",
    "        self.alive_reward =rospy.get_param(\"/monoped/alive_reward\")\n",
    "        self.done_reward =rospy.get_param(\"/monoped/done_reward\")\n",
    "\n",
    "        # Here we will add any init functions prior to starting the MyRobotEnv\n",
    "        super(HopperStayUpEnv, self).__init__()\n",
    "        \n",
    "        rospy.logdebug(\"END HopperStayUpEnv INIT...\")\n",
    "\n",
    "    def _set_init_pose(self):\n",
    "        \"\"\"\n",
    "        Sets the Robot in its init linear and angular speeds\n",
    "        and lands the robot. Its preparing it to be reseted in the world.\n",
    "        \"\"\"\n",
    "\n",
    "        joints_array = [self.init_joint_states.x,\n",
    "                        self.init_joint_states.y,\n",
    "                        self.init_joint_states.z]\n",
    "        \n",
    "        self.move_joints(   joints_array,\n",
    "                            epsilon=self.accepted_joint_error,\n",
    "                            update_rate=self.update_rate)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def _init_env_variables(self):\n",
    "        \"\"\"\n",
    "        Inits variables needed to be initialised each time we reset at the start\n",
    "        of an episode.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # For Info Purposes\n",
    "        self.cumulated_reward = 0.0\n",
    "        # We get the initial pose to mesure the distance from the desired point.\n",
    "        odom = self.get_odom()\n",
    "        self.previous_distance_from_des_point = self.get_distance_from_desired_point(odom.pose.pose.position)\n",
    "\n",
    "        \n",
    "\n",
    "    def _set_action(self, action):\n",
    "        \"\"\"\n",
    "        It sets the joints of monoped based on the action integer given\n",
    "        based on the action number given.\n",
    "        :param action: The action integer that sets what movement to do next.\n",
    "        \"\"\"\n",
    "        \n",
    "        rospy.logdebug(\"Start Set Action ==>\"+str(action))\n",
    "       \n",
    "        # We get current Joints values\n",
    "        joint_states = self.get_joint_states()\n",
    "        joint_states_position = joint_states.position\n",
    "        rospy.logdebug(\"get_action_to_position>>>\"+str(joint_states_position))\n",
    "        \n",
    "        action_position = [0.0,0.0,0.0]\n",
    "        if action == 0: #Increment haa_joint\n",
    "            action_position[0] = joint_states_position[0] + self.joint_increment_value\n",
    "            action_position[1] = joint_states_position[1]\n",
    "            action_position[2] = joint_states_position[2]\n",
    "        elif action == 1: #Decrement haa_joint\n",
    "            action_position[0] = joint_states_position[0] - self.joint_increment_value\n",
    "            action_position[1] = joint_states_position[1]\n",
    "            action_position[2] = joint_states_position[2]\n",
    "        elif action == 2: #Increment hfe_joint\n",
    "            action_position[0] = joint_states_position[0]\n",
    "            action_position[1] = joint_states_position[1] + self.joint_increment_value\n",
    "            action_position[2] = joint_states_position[2]\n",
    "        elif action == 3: #Decrement hfe_joint\n",
    "            action_position[0] = joint_states_position[0]\n",
    "            action_position[1] = joint_states_position[1] - self.joint_increment_value\n",
    "            action_position[2] = joint_states_position[2]\n",
    "        elif action == 4: #Increment kfe_joint\n",
    "            action_position[0] = joint_states_position[0]\n",
    "            action_position[1] = joint_states_position[1]\n",
    "            action_position[2] = joint_states_position[2] + self.joint_increment_value\n",
    "        elif action == 5:  #Decrement kfe_joint\n",
    "            action_position[0] = joint_states_position[0]\n",
    "            action_position[1] = joint_states_position[1]\n",
    "            action_position[2] = joint_states_position[2] - self.joint_increment_value\n",
    "\n",
    "        \n",
    "        # We tell monoped where to place its joints next\n",
    "        self.move_joints(   action_position,\n",
    "                            epsilon=self.accepted_joint_error,\n",
    "                            update_rate=self.update_rate)\n",
    "        \n",
    "        rospy.logdebug(\"END Set Action ==>\"+str(action))\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Here we define what sensor data defines our robots observations\n",
    "        To know which Variables we have access to, we need to read the\n",
    "        HopperEnv API DOCS\n",
    "        Returns the state of the robot needed for OpenAI QLearn Algorithm\n",
    "        The state will be defined by an array of the:\n",
    "        1) distance from desired point in meters\n",
    "        2) The pitch orientation in radians\n",
    "        3) the Roll orientation in radians\n",
    "        4) the Yaw orientation in radians\n",
    "        5) Force in contact sensor in Newtons\n",
    "        6-7-8) State of the 3 joints in radians\n",
    "        9) Height of the Base\n",
    "\n",
    "        observation = [distance_from_desired_point,\n",
    "                 base_roll,\n",
    "                 base_pitch,\n",
    "                 base_yaw,\n",
    "                 force_magnitude,\n",
    "                 joint_states_haa,\n",
    "                 joint_states_hfe,\n",
    "                 joint_states_kfe,\n",
    "                 height_base]\n",
    "        :return: observation\n",
    "        \"\"\"\n",
    "        rospy.logdebug(\"Start Get Observation ==>\")\n",
    "        \n",
    "        distance_from_desired_point = self.get_distance_from_desired_point(self.desired_point)\n",
    "\n",
    "        base_orientation = self.get_base_rpy()\n",
    "        base_roll = base_orientation.x\n",
    "        base_pitch = base_orientation.y\n",
    "        base_yaw = base_orientation.z\n",
    "\n",
    "        force_magnitude = self.get_contact_force_magnitude()\n",
    "\n",
    "        joint_states = self.get_joint_states()\n",
    "        joint_states_haa = joint_states.position[0]\n",
    "        joint_states_hfe = joint_states.position[1]\n",
    "        joint_states_kfe = joint_states.position[2]\n",
    "        \n",
    "        odom = self.get_odom()\n",
    "        base_position = odom.pose.pose.position\n",
    "\n",
    "        observation = []\n",
    "        observation.append(round(distance_from_desired_point,self.dec_obs))\n",
    "        observation.append(round(base_roll,self.dec_obs))\n",
    "        observation.append(round(base_pitch,self.dec_obs))\n",
    "        observation.append(round(base_yaw,self.dec_obs))\n",
    "        observation.append(round(force_magnitude,self.dec_obs))\n",
    "        observation.append(round(joint_states_haa,self.dec_obs))\n",
    "        observation.append(round(joint_states_hfe,self.dec_obs))\n",
    "        observation.append(round(joint_states_kfe,self.dec_obs))\n",
    "        \n",
    "        observation.append(round(base_position.x,self.dec_obs))\n",
    "        observation.append(round(base_position.y,self.dec_obs))\n",
    "        observation.append(round(base_position.z,self.dec_obs)) # height\n",
    "\n",
    "        return observation\n",
    "        \n",
    "\n",
    "    def _is_done(self, observations):\n",
    "        \"\"\"\n",
    "        We consider the episode done if:\n",
    "        1) The Monopeds height is lower than a threshhold\n",
    "        2) The Orientation is outside a threshold\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        height_base = observations[10]\n",
    "        \n",
    "        monoped_height_ok = self.monoped_height_ok(height_base)\n",
    "        monoped_orientation_ok = self.monoped_orientation_ok()\n",
    "\n",
    "        done = not(monoped_height_ok and monoped_orientation_ok)\n",
    "        \n",
    "        return done\n",
    "\n",
    "    def _compute_reward(self, observations, done):\n",
    "        \"\"\"\n",
    "        We Base the rewards in if its done or not and we base it on\n",
    "        the joint poisition, effort, contact force, orientation and distance from desired point.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        joints_state_array = observations[5:8]\n",
    "        r1 = self.calculate_reward_joint_position(joints_state_array, self.weight_joint_position)\n",
    "        # Desired Force in Newtons, taken form idle contact with 9.81 gravity.\n",
    "        \n",
    "        force_magnitude = observations[4]\n",
    "        r2 = self.calculate_reward_contact_force(force_magnitude, self.weight_contact_force)\n",
    "        \n",
    "        rpy_array = observations[1:4]\n",
    "        r3 = self.calculate_reward_orientation(rpy_array, self.weight_orientation)\n",
    "        \n",
    "        \n",
    "        current_position = Point()\n",
    "        current_position.x = observations[8]\n",
    "        current_position.y = observations[9]\n",
    "        current_position.z = observations[10]\n",
    "        r4 = self.calculate_reward_distance_from_des_point(current_position, self.weight_distance_from_des_point)\n",
    "\n",
    "        # The sign depend on its function.\n",
    "        total_reward = self.alive_reward - r1 - r2 - r3 - r4\n",
    "\n",
    "        rospy.logdebug(\"###############\")\n",
    "        rospy.logdebug(\"alive_bonus=\" + str(self.alive_reward))\n",
    "        rospy.logdebug(\"r1 joint_position=\" + str(r1))\n",
    "        rospy.logdebug(\"r2 contact_force=\" + str(r2))\n",
    "        rospy.logdebug(\"r3 orientation=\" + str(r3))\n",
    "        rospy.logdebug(\"r4 distance=\" + str(r4))\n",
    "        rospy.logdebug(\"total_reward=\" + str(total_reward))\n",
    "        rospy.logdebug(\"###############\")\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "    # Internal TaskEnv Methods\n",
    "    \n",
    "    def is_in_desired_position(self,current_position, epsilon=0.05):\n",
    "        \"\"\"\n",
    "        It return True if the current position is similar to the desired poistion\n",
    "        \"\"\"\n",
    "        \n",
    "        is_in_desired_pos = False\n",
    "        \n",
    "        \n",
    "        x_pos_plus = self.desired_point.x + epsilon\n",
    "        x_pos_minus = self.desired_point.x - epsilon\n",
    "        y_pos_plus = self.desired_point.y + epsilon\n",
    "        y_pos_minus = self.desired_point.y - epsilon\n",
    "        \n",
    "        x_current = current_position.x\n",
    "        y_current = current_position.y\n",
    "        \n",
    "        x_pos_are_close = (x_current <= x_pos_plus) and (x_current > x_pos_minus)\n",
    "        y_pos_are_close = (y_current <= y_pos_plus) and (y_current > y_pos_minus)\n",
    "        \n",
    "        is_in_desired_pos = x_pos_are_close and y_pos_are_close\n",
    "        \n",
    "        rospy.logdebug(\"###### IS DESIRED POS ? ######\")\n",
    "        rospy.logdebug(\"current_position\"+str(current_position))\n",
    "        rospy.logdebug(\"x_pos_plus\"+str(x_pos_plus)+\",x_pos_minus=\"+str(x_pos_minus))\n",
    "        rospy.logdebug(\"y_pos_plus\"+str(y_pos_plus)+\",y_pos_minus=\"+str(y_pos_minus))\n",
    "        rospy.logdebug(\"x_pos_are_close\"+str(x_pos_are_close))\n",
    "        rospy.logdebug(\"y_pos_are_close\"+str(y_pos_are_close))\n",
    "        rospy.logdebug(\"is_in_desired_pos\"+str(is_in_desired_pos))\n",
    "        rospy.logdebug(\"############\")\n",
    "        \n",
    "        return is_in_desired_pos\n",
    "    \n",
    "    def is_inside_workspace(self,current_position):\n",
    "        \"\"\"\n",
    "        Check if the monoped is inside the Workspace defined\n",
    "        \"\"\"\n",
    "        is_inside = False\n",
    "\n",
    "        rospy.logdebug(\"##### INSIDE WORK SPACE? #######\")\n",
    "        rospy.logdebug(\"XYZ current_position\"+str(current_position))\n",
    "        rospy.logdebug(\"work_space_x_max\"+str(self.work_space_x_max)+\",work_space_x_min=\"+str(self.work_space_x_min))\n",
    "        rospy.logdebug(\"work_space_y_max\"+str(self.work_space_y_max)+\",work_space_y_min=\"+str(self.work_space_y_min))\n",
    "        rospy.logdebug(\"work_space_z_max\"+str(self.work_space_z_max)+\",work_space_z_min=\"+str(self.work_space_z_min))\n",
    "        rospy.logdebug(\"############\")\n",
    "\n",
    "        if current_position.x > self.work_space_x_min and current_position.x <= self.work_space_x_max:\n",
    "            if current_position.y > self.work_space_y_min and current_position.y <= self.work_space_y_max:\n",
    "                if current_position.z > self.work_space_z_min and current_position.z <= self.work_space_z_max:\n",
    "                    is_inside = True\n",
    "        \n",
    "        return is_inside\n",
    "        \n",
    "    def sonar_detected_something_too_close(self, sonar_value):\n",
    "        \"\"\"\n",
    "        Detects if there is something too close to the monoped front\n",
    "        \"\"\"\n",
    "        rospy.logdebug(\"##### SONAR TOO CLOSE? #######\")\n",
    "        rospy.logdebug(\"sonar_value\"+str(sonar_value)+\",min_sonar_value=\"+str(self.min_sonar_value))\n",
    "        rospy.logdebug(\"############\")\n",
    "        \n",
    "        too_close = sonar_value < self.min_sonar_value\n",
    "        \n",
    "        return too_close\n",
    "        \n",
    "    def monoped_has_flipped(self,current_orientation):\n",
    "        \"\"\"\n",
    "        Based on the orientation RPY given states if the monoped has flipped\n",
    "        \"\"\"\n",
    "        has_flipped = True\n",
    "        \n",
    "        \n",
    "        self.max_roll = rospy.get_param(\"/monoped/max_roll\")\n",
    "        self.max_pitch = rospy.get_param(\"/monoped/max_pitch\")\n",
    "        \n",
    "        rospy.logdebug(\"#### HAS FLIPPED? ########\")\n",
    "        rospy.logdebug(\"RPY current_orientation\"+str(current_orientation))\n",
    "        rospy.logdebug(\"max_roll\"+str(self.max_roll)+\",min_roll=\"+str(-1*self.max_roll))\n",
    "        rospy.logdebug(\"max_pitch\"+str(self.max_pitch)+\",min_pitch=\"+str(-1*self.max_pitch))\n",
    "        rospy.logdebug(\"############\")\n",
    "        \n",
    "        \n",
    "        if current_orientation.x > -1*self.max_roll and current_orientation.x <= self.max_roll:\n",
    "            if current_orientation.y > -1*self.max_pitch and current_orientation.y <= self.max_pitch:\n",
    "                    has_flipped = False\n",
    "        \n",
    "        return has_flipped\n",
    "        \n",
    "    def get_distance_from_desired_point(self, current_position):\n",
    "        \"\"\"\n",
    "        Calculates the distance from the current position to the desired point\n",
    "        :param start_point:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        distance = self.get_distance_from_point(current_position,\n",
    "                                                self.desired_point)\n",
    "    \n",
    "        return distance\n",
    "    \n",
    "    def get_distance_from_point(self, pstart, p_end):\n",
    "        \"\"\"\n",
    "        Given a Vector3 Object, get distance from current position\n",
    "        :param p_end:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        a = numpy.array((pstart.x, pstart.y, pstart.z))\n",
    "        b = numpy.array((p_end.x, p_end.y, p_end.z))\n",
    "    \n",
    "        distance = numpy.linalg.norm(a - b)\n",
    "    \n",
    "        return distance\n",
    "        \n",
    "    def get_orientation_euler(self, quaternion_vector):\n",
    "        # We convert from quaternions to euler\n",
    "        orientation_list = [quaternion_vector.x,\n",
    "                            quaternion_vector.y,\n",
    "                            quaternion_vector.z,\n",
    "                            quaternion_vector.w]\n",
    "    \n",
    "        roll, pitch, yaw = euler_from_quaternion(orientation_list)\n",
    "        return roll, pitch, yaw\n",
    "        \n",
    "\n",
    "    def get_base_rpy(self):\n",
    "\n",
    "        imu = self.get_imu()\n",
    "        base_orientation = imu.orientation\n",
    "        \n",
    "        euler_rpy = Vector3()\n",
    "        euler = euler_from_quaternion([ base_orientation.x,\n",
    "                                        base_orientation.y,\n",
    "                                        base_orientation.z,\n",
    "                                        base_orientation.w]\n",
    "                                        )\n",
    "        euler_rpy.x = euler[0]\n",
    "        euler_rpy.y = euler[1]\n",
    "        euler_rpy.z = euler[2]\n",
    "        \n",
    "        return euler_rpy\n",
    "        \n",
    "    def get_contact_force_magnitude(self):\n",
    "        \"\"\"\n",
    "        You will see that because the X axis is the one pointing downwards, it will be the one with\n",
    "        higher value when touching the floor\n",
    "        For a Robot of total mas of 0.55Kg, a gravity of 9.81 m/sec**2, Weight = 0.55*9.81=5.39 N\n",
    "        Falling from around 5centimetres ( negligible height ), we register peaks around\n",
    "        Fx = 7.08 N\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # We get the Contact Sensor data\n",
    "        lowerleg_contactsensor_state = self.get_lowerleg_contactsensor_state()\n",
    "        # We extract what we need that is only the total_wrench force\n",
    "        contact_force = self.get_contact_force(lowerleg_contactsensor_state)\n",
    "        # We create an array with each component XYZ\n",
    "        contact_force_np = numpy.array((contact_force.x, contact_force.y, contact_force.z))\n",
    "        # We calculate the magnitude of the Force Vector, array.\n",
    "        force_magnitude = numpy.linalg.norm(contact_force_np)\n",
    "\n",
    "        return force_magnitude\n",
    "        \n",
    "    def get_contact_force(self, lowerleg_contactsensor_state):\n",
    "        \"\"\"\n",
    "        /lowerleg_contactsensor_state/states[0]/contact_positions ==> PointContact in World\n",
    "        /lowerleg_contactsensor_state/states[0]/contact_normals ==> NormalContact in World\n",
    "\n",
    "        ==> One is an array of all the forces, the other total,\n",
    "         and are relative to the contact link referred to in the sensor.\n",
    "        /lowerleg_contactsensor_state/states[0]/wrenches[]\n",
    "        /lowerleg_contactsensor_state/states[0]/total_wrench\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        # We create an empty element , in case there is no contact.\n",
    "        contact_force = Vector3()\n",
    "        for state in lowerleg_contactsensor_state.states:\n",
    "            self.contact_force = state.total_wrench.force\n",
    "        \n",
    "        return contact_force\n",
    "        \n",
    "        \n",
    "    def monoped_height_ok(self, height_base):\n",
    "\n",
    "        height_ok = self.min_height <= height_base < self.max_height\n",
    "        return height_ok\n",
    "        \n",
    "    def monoped_orientation_ok(self):\n",
    "\n",
    "        orientation_rpy = self.get_base_rpy()\n",
    "        roll_ok = self.max_incl_roll > abs(orientation_rpy.x)\n",
    "        pitch_ok = self.max_incl_pitch > abs(orientation_rpy.y)\n",
    "        orientation_ok = roll_ok and pitch_ok\n",
    "        return orientation_ok\n",
    "        \n",
    "        \n",
    "    def calculate_reward_joint_position(self, joints_state_array, weight=1.0):\n",
    "        \"\"\"\n",
    "        We calculate reward base on the joints configuration. The more near 0 the better.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        acumulated_joint_pos = 0.0\n",
    "        for joint_pos in joints_state_array:\n",
    "            # Abs to remove sign influence, it doesnt matter the direction of turn.\n",
    "            acumulated_joint_pos += abs(joint_pos)\n",
    "            rospy.logdebug(\"calculate_reward_joint_position>>acumulated_joint_pos=\" + str(acumulated_joint_pos))\n",
    "        reward = weight * acumulated_joint_pos\n",
    "        rospy.logdebug(\"calculate_reward_joint_position>>reward=\" + str(reward))\n",
    "        return reward\n",
    "        \n",
    "    def calculate_reward_contact_force(self, force_magnitude, weight=1.0):\n",
    "        \"\"\"\n",
    "        We calculate reward base on the contact force.\n",
    "        The nearest to the desired contact force the better.\n",
    "        We use exponential to magnify big departures from the desired force.\n",
    "        Default ( 7.08 N ) desired force was taken from reading of the robot touching\n",
    "        the ground from a negligible height of 5cm.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        force_displacement = force_magnitude - self.desired_force\n",
    "\n",
    "        rospy.logdebug(\"calculate_reward_contact_force>>force_magnitude=\" + str(force_magnitude))\n",
    "        rospy.logdebug(\"calculate_reward_contact_force>>force_displacement=\" + str(force_displacement))\n",
    "        # Abs to remove sign\n",
    "        reward = weight * abs(force_displacement)\n",
    "        rospy.logdebug(\"calculate_reward_contact_force>>reward=\" + str(reward))\n",
    "        return reward\n",
    "        \n",
    "    def calculate_reward_orientation(self, rpy_array, weight=1.0):\n",
    "        \"\"\"\n",
    "        We calculate the reward based on the orientation.\n",
    "        The more its closser to 0 the better because it means its upright\n",
    "        desired_yaw is the yaw that we want it to be.\n",
    "        to praise it to have a certain orientation, here is where to set it.\n",
    "        :param: rpy_array: Its an array with Roll Pitch and Yaw in place 0, 1 and 2 respectively.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        yaw_displacement = rpy_array[2] - self.desired_yaw\n",
    "        acumulated_orientation_displacement = abs(rpy_array[0]) + abs(rpy_array[1]) + abs(yaw_displacement)\n",
    "        reward = weight * acumulated_orientation_displacement\n",
    "        rospy.logdebug(\"calculate_reward_orientation>>reward=\" + str(reward))\n",
    "        return reward\n",
    "        \n",
    "    def calculate_reward_distance_from_des_point(self, current_position, weight=1.0):\n",
    "        \"\"\"\n",
    "        We calculate the distance from the desired point.\n",
    "        The closser the better\n",
    "        :param weight:\n",
    "        :return:reward\n",
    "        \"\"\"\n",
    "        distance = self.get_distance_from_desired_point(current_position)\n",
    "        reward = weight * distance\n",
    "        rospy.logdebug(\"calculate_reward_orientation>>reward=\" + str(reward))\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**END Python File: hopper_task_env.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Step 3: Create Training Script (Qlearn) <p id=\"SolutionStep3\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**Python File: start_training.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import gym\n",
    "import numpy\n",
    "import time\n",
    "import qlearn\n",
    "from gym import wrappers\n",
    "# ROS packages required\n",
    "import rospy\n",
    "import rospkg\n",
    "# import our training environment\n",
    "import hopper_task_env\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    rospy.init_node('hopper_stay_up_qlearn', anonymous=True, log_level=rospy.WARN)\n",
    "\n",
    "    # Create the Gym environment\n",
    "    env = gym.make('HopperStayUp-v0')\n",
    "    rospy.loginfo(\"Gym environment done\")\n",
    "\n",
    "    # Set the logging system\n",
    "    rospack = rospkg.RosPack()\n",
    "    pkg_path = rospack.get_path('my_hopper_openai_example')\n",
    "    outdir = pkg_path + '/training_results'\n",
    "    env = wrappers.Monitor(env, outdir, force=True)\n",
    "    rospy.loginfo(\"Monitor Wrapper started\")\n",
    "\n",
    "    last_time_steps = numpy.ndarray(0)\n",
    "\n",
    "    # Loads parameters from the ROS param server\n",
    "    # Parameters are stored in a yaml file inside the config directory\n",
    "    # They are loaded at runtime by the launch file\n",
    "    Alpha = rospy.get_param(\"/monoped/alpha\")\n",
    "    Epsilon = rospy.get_param(\"/monoped/epsilon\")\n",
    "    Gamma = rospy.get_param(\"/monoped/gamma\")\n",
    "    epsilon_discount = rospy.get_param(\"/monoped/epsilon_discount\")\n",
    "    nepisodes = rospy.get_param(\"/monoped/nepisodes\")\n",
    "    nsteps = rospy.get_param(\"/monoped/nsteps\")\n",
    "\n",
    "    # Initialises the algorithm that we are going to use for learning\n",
    "    qlearn = qlearn.QLearn(actions=range(env.action_space.n),\n",
    "                           alpha=Alpha, gamma=Gamma, epsilon=Epsilon)\n",
    "    initial_epsilon = qlearn.epsilon\n",
    "\n",
    "    start_time = time.time()\n",
    "    highest_reward = 0\n",
    "\n",
    "    # Starts the main training loop: the one about the episodes to do\n",
    "    for x in range(nepisodes):\n",
    "        rospy.logdebug(\"############### START EPISODE=>\" + str(x))\n",
    "\n",
    "        cumulated_reward = 0\n",
    "        done = False\n",
    "        if qlearn.epsilon > 0.05:\n",
    "            qlearn.epsilon *= epsilon_discount\n",
    "\n",
    "        # Initialize the environment and get first state of the robot\n",
    "        observation = env.reset()\n",
    "        state = ''.join(map(str, observation))\n",
    "\n",
    "        # Show on screen the actual situation of the robot\n",
    "        # env.render()\n",
    "        # for each episode, we test the robot for nsteps\n",
    "        for i in range(nsteps):\n",
    "            rospy.logwarn(\"############### Start Step=>\" + str(i))\n",
    "            # Pick an action based on the current state\n",
    "            action = qlearn.chooseAction(state)\n",
    "            rospy.logwarn(\"Next action is:%d\", action)\n",
    "            # Execute the action in the environment and get feedback\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            rospy.logwarn(str(observation) + \" \" + str(reward))\n",
    "            cumulated_reward += reward\n",
    "            if highest_reward < cumulated_reward:\n",
    "                highest_reward = cumulated_reward\n",
    "\n",
    "            nextState = ''.join(map(str, observation))\n",
    "\n",
    "            # Make the algorithm learn based on the results\n",
    "            rospy.logwarn(\"# state we were=>\" + str(state))\n",
    "            rospy.logwarn(\"# action that we took=>\" + str(action))\n",
    "            rospy.logwarn(\"# reward that action gave=>\" + str(reward))\n",
    "            rospy.logwarn(\"# episode cumulated_reward=>\" + str(cumulated_reward))\n",
    "            rospy.logwarn(\"# State in which we will start next step=>\" + str(nextState))\n",
    "            qlearn.learn(state, action, reward, nextState)\n",
    "\n",
    "            if not (done):\n",
    "                rospy.logwarn(\"NOT DONE\")\n",
    "                state = nextState\n",
    "            else:\n",
    "                rospy.logwarn(\"DONE\")\n",
    "                last_time_steps = numpy.append(last_time_steps, [int(i + 1)])\n",
    "                break\n",
    "            rospy.logwarn(\"############### END Step=>\" + str(i))\n",
    "            #raw_input(\"Next Step...PRESS KEY\")\n",
    "            # rospy.sleep(2.0)\n",
    "        m, s = divmod(int(time.time() - start_time), 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        rospy.logerr((\"EP: \" + str(x + 1) + \" - [alpha: \" + str(round(qlearn.alpha, 2)) + \" - gamma: \" + str(\n",
    "            round(qlearn.gamma, 2)) + \" - epsilon: \" + str(round(qlearn.epsilon, 2)) + \"] - Reward: \" + str(\n",
    "            cumulated_reward) + \"     Time: %d:%02d:%02d\" % (h, m, s)))\n",
    "\n",
    "    rospy.loginfo((\"\\n|\" + str(nepisodes) + \"|\" + str(qlearn.alpha) + \"|\" + str(qlearn.gamma) + \"|\" + str(\n",
    "        initial_epsilon) + \"*\" + str(epsilon_discount) + \"|\" + str(highest_reward) + \"| PICTURE |\"))\n",
    "\n",
    "    l = last_time_steps.tolist()\n",
    "    l.sort()\n",
    "\n",
    "    # print(\"Parameters: a=\"+str)\n",
    "    rospy.loginfo(\"Overall score: {:0.2f}\".format(last_time_steps.mean()))\n",
    "    rospy.loginfo(\"Best 100 score: {:0.2f}\".format(reduce(lambda x, y: x + y, l[-100:]) / len(l[-100:])))\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**END Python File: start_training.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**Param File: hopper_qlearn_params.yaml** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monoped: #namespace\n",
    "\n",
    "    #qlearn parameters\n",
    "    alpha: 0.1\n",
    "    gamma: 0.7\n",
    "    epsilon: 0.9\n",
    "    epsilon_discount: 0.999\n",
    "    nepisodes: 500\n",
    "    nsteps: 10000\n",
    "\n",
    "    n_actions: 6 # 1-2) Increment/Decrement haa_joint\n",
    "                 # 3-4) Increment/Decrement hfe_joint\n",
    "                 # 5-6) Increment/Decrement kfe_joint\n",
    "                 \n",
    "    n_observations: 6 # We have 6 different observations\n",
    "\n",
    "\n",
    "    init_joint_states:\n",
    "      haa_joint: 0.0\n",
    "      hfe_joint: 0.0\n",
    "      kfe_joint: 0.0\n",
    "\n",
    "    desired_point:\n",
    "      x: 0.0\n",
    "      y: 0.0\n",
    "      z: 1.0\n",
    "    accepted_error_in_des_pos: 0.1 # Accepted error un meters of the desired position\n",
    "      \n",
    "    number_decimals_precision_obs: 1\n",
    "      \n",
    "    desired_force: 7.08 # In Newtons, normal contact force when stanting still with 9.81 gravity\n",
    "    max_contact_force: 14.16 # The maxixmum contact force that we consoder acceptable.\n",
    "    \n",
    "    desired_yaw: 0.0 # Desired yaw in radians for the hopper to stay\n",
    "    \n",
    "    max_x_pos: 3.0 # Maximum Position of the base in X axis in World\n",
    "    max_y_pos: 3.0 # Maximum Position of the base in Y axis in World\n",
    "    \n",
    "    max_height: 3.0   # in meters\n",
    "    min_height: 0.5   # in meters\n",
    "    \n",
    "    max_incl: 1.57       # Maximum acceptable roll and pitch for the hoppers base. More is conidered a fallen robot\n",
    "    \n",
    "    distance_from_desired_point_max: 3.0 # Distance that we consider maximum to be from the desired point\n",
    "    \n",
    "    joint_increment_value: 0.05  # in radians\n",
    "    accepted_joint_error: 0.1 # Radians of accepted error in the joints, should be bigger than the joint increment\n",
    "    update_rate: 20 # Hz frequency in whihc we check that the joints are in the correct place, the bigger the more precise but more overhead\n",
    "    \n",
    "    maximum_haa_joint: 1.6 # Max and Min value ( sign change ) based on the URDF joint values\n",
    "    maximum_hfe_joint: 1.6 # Max and Min value ( sign change ) based on the URDF joint values\n",
    "    maximum_kfe_joint: 0.0 # Max based on the URDF joint values\n",
    "    min_kfe_joint: -1.6 #  Min value based on the URDF joint values\n",
    "    \n",
    "    done_reward: 1000.0 # reward\n",
    "    alive_reward: 100.0 # reward\n",
    "    \n",
    "    rewards_weight:\n",
    "      weight_joint_position: 1.0 # Weight for joint positions ( joints in the zero is perfect )\n",
    "      weight_contact_force: 1.0 # Weight for contact force similar to desired ( weight of monoped )\n",
    "      weight_orientation: 1.0 # Weight for orientation ( vertical is perfect )\n",
    "      weight_distance_from_des_point: 1.0 # Weight for distance from desired point ( on the point is perfect )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**END Param File: hopper_qlearn_params.yaml** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**Python File: qlearn.py** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Q-learning approach for different RL problems\n",
    "as part of the basic series on reinforcement learning @\n",
    "https://github.com/vmayoral/basic_reinforcement_learning\n",
    " \n",
    "Inspired by https://gym.openai.com/evaluations/eval_kWknKOkPQ7izrixdhriurA\n",
    " \n",
    "        @author: Victor Mayoral Vilches <victor@erlerobotics.com>\n",
    "'''\n",
    " \n",
    "import random\n",
    " \n",
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon, alpha, gamma):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha      # discount constant\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.actions = actions\n",
    " \n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    " \n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        '''\n",
    "        Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))            \n",
    "        '''\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    " \n",
    "    def chooseAction(self, state, return_q=False):\n",
    "        q = [self.getQ(state, a) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    " \n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))] \n",
    "            maxQ = max(q)\n",
    " \n",
    "        count = q.count(maxQ)\n",
    "        # In case there're several state-action max values \n",
    "        # we select a random one among them\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    " \n",
    "        action = self.actions[i]        \n",
    "        if return_q: # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    " \n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**END Python File: qlearn.py** </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**Launch File: start_training.launch** </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<launch>\n",
    " \n",
    "    <!-- Load the parameters for the algorithm -->\n",
    "    <rosparam command=\"load\" file=\"$(find hopper_project_pkg)/config/hopper_qlearn_params.yaml\" />\n",
    " \n",
    "    <!-- Launch the training system -->\n",
    "    <node pkg=\"hopper_project_pkg\" name=\"hopper_stay_up_qlearn\" type=\"start_training.py\" output=\"screen\"/>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#3B8F10;color:white;\" id=\"prg-2-1\">**END Launch File: start_training.launch** </p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
