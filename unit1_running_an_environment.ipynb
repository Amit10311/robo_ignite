{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Running an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's make the robot learn how to move around this simple loop maze.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun gym_construct simple_qlearn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will execute a simple learning protocol of 10 episodes. It is rewarded more for going forwards than turning, and the episode stops when the time ends or the robot runs into a wall (the distance from the nearest wall is less than 0.2 meters). You should see something similar to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/turtlebotlearning_example.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... how does this work? Let's have a look at the script you have just executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosed gym_construct simple_qlearn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will open the specified file **<i>simple_qlearn.py</i>** using the <a href=\"http://www.vim.org\" target=\"_blank\">**vim**</a> editor, which is a text editor for terminals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:green;\">**NOTE:** Enter **:q** to get out of the vim editor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find this script inside the **scripts** folder of the **gym_construct** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the following file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**simple_qlearn.py**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-1-319c8b4a904f>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-319c8b4a904f>\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    print \"Gym Make done\"\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import gym\n",
    "import gym_gazebo\n",
    "import time\n",
    "import numpy\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import qlearn\n",
    "# import liveplot\n",
    "from gym import wrappers\n",
    "from liveplot import LivePlot \n",
    "\n",
    "def render():\n",
    "    render_skip = 0 #Skip first X episodes.\n",
    "    render_interval = 50 #Show render Every Y episode.\n",
    "    render_episodes = 10 #Show Z episodes every rendering.\n",
    "\n",
    "    if (x%render_interval == 0) and (x != 0) and (x > render_skip):\n",
    "        env.render()\n",
    "    elif ((x-render_episodes)%render_interval == 0) and (x != 0) and (x > render_skip) and (render_episodes < x):\n",
    "        env.render(close=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    env = gym.make('GazeboCircuitTurtlebotLidar-v0')\n",
    "    print \"Gym Make done\"\n",
    "    outdir = '/tmp/gazebo_gym_experiments'\n",
    "    #outdir = '/home/user/catkin_ws/src/gym_construct/src/gazebo_gym_experiments'\n",
    "    # env.monitor.start(outdir, force=True, seed=None)       # I had to comment this and\n",
    "    env = wrappers.Monitor(env, outdir, force=True)          # use this to avoid warnings\n",
    "    #plotter = LivePlot(outdir)\n",
    "    print \"Monitor Wrapper started\"\n",
    "    last_time_steps = numpy.ndarray(0)\n",
    "\n",
    "    qlearn = qlearn.QLearn(actions=range(env.action_space.n),\n",
    "                    alpha=0.1, gamma=0.8, epsilon=0.9)\n",
    "\n",
    "    initial_epsilon = qlearn.epsilon\n",
    "\n",
    "    epsilon_discount = 0.999 # 1098 eps to reach 0.1\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_episodes = 10\n",
    "    highest_reward = 0\n",
    "\n",
    "    for x in range(total_episodes):\n",
    "        done = False\n",
    "\n",
    "        cumulated_reward = 0 #Should going forward give more reward than L/R ?\n",
    "        print (\"Episode = \"+str(x))\n",
    "        observation = env.reset()\n",
    "        if qlearn.epsilon > 0.05:\n",
    "            qlearn.epsilon *= epsilon_discount\n",
    "\n",
    "        #render()\n",
    "        env.render()\n",
    "\n",
    "        state = ''.join(map(str, observation))\n",
    "\n",
    "        for i in range(500):\n",
    "\n",
    "            # Pick an action based on the current state\n",
    "            action = qlearn.chooseAction(state)\n",
    "            #print (\"Action Chosen\"+str(action))\n",
    "            # Execute the action and get feedback\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            cumulated_reward += reward\n",
    "            #print (\"Reward=\"+str(reward))\n",
    "            if highest_reward < cumulated_reward:\n",
    "                highest_reward = cumulated_reward\n",
    "\n",
    "            nextState = ''.join(map(str, observation))\n",
    "\n",
    "            qlearn.learn(state, action, reward, nextState)\n",
    "\n",
    "            #env.monitor.flush(force=True)\n",
    "\n",
    "            if not(done):\n",
    "                #print \"NOT done\"\n",
    "                state = nextState\n",
    "            else:\n",
    "                print \"DONE\"\n",
    "                last_time_steps = numpy.append(last_time_steps, [int(i + 1)])\n",
    "                break \n",
    "\n",
    "        m, s = divmod(int(time.time() - start_time), 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        print (\"EP: \"+str(x+1)+\" - [alpha: \"+str(round(qlearn.alpha,2))+\" - gamma: \"+str(round(qlearn.gamma,2))+\" - epsilon: \"+str(round(qlearn.epsilon,2))+\"] - Reward: \"+str(cumulated_reward)+\"     Time: %d:%02d:%02d\" % (h, m, s))\n",
    "\n",
    "    #Github table content\n",
    "    print (\"\\n|\"+str(total_episodes)+\"|\"+str(qlearn.alpha)+\"|\"+str(qlearn.gamma)+\"|\"+str(initial_epsilon)+\"*\"+str(epsilon_discount)+\"|\"+str(highest_reward)+\"| PICTURE |\")\n",
    "\n",
    "    l = last_time_steps.tolist()\n",
    "    l.sort()\n",
    "\n",
    "    #print(\"Parameters: a=\"+str)\n",
    "    print(\"Overall score: {:0.2f}\".format(last_time_steps.mean()))\n",
    "    print(\"Best 100 score: {:0.2f}\".format(reduce(lambda x, y: x + y, l[-100:]) / len(l[-100:])))\n",
    "\n",
    "    #env.monitor.close()\n",
    "    #env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**simple_qlearn.py**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concentrate on the OpenAI-Gym basic infrastructure, leaving the AI algorithm operation out of the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first thing is the setup of the gym environment:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "env = gym.make('GazeboCircuitTurtlebotLidar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is made by giving the environment ID that will be used. This imports all the data specified in the environment configuration file: like the actions available, the rewards that will be given, and the basic communication with all of the Gazebo and ROS infrastructures. As you can see, nothing referring to ROS or Gazebo is stated here. This could be a video game, a different simulator, or even a real robot. The same program would be used, only changing the environment if the interface was the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Steps "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(500):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will state how many time steps will be made before going to the next episode. Keep this number in mind because, in the environment setup, you will have to put the same number or higher to avoid gym module related errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start up the monitoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AI learning, it's essential to record the results and learning results data in order to evaluate if it's really learning or not. Gym gives a wrapper that will record the state of the system data, called <b>observations</b>. The observations are what we consider relevant data for learning and making decisions as to what actions should be performed next."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "env = wrappers.Monitor(env, outdir, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a learning step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In between all the qlearn algorithm related code, concentrate on the **<i>env.step()</i>** method. It returns:\n",
    "<ul>\n",
    "<li><i>**observation**</i>: The observation is the state of the environment. It will return different kinds of data depending on how the environment set-up file is defined. In this case, it returns a discrete version of the laser readings of the robot. In your personal case, it has to be data needed to make AI decisions. It could be altitude, image data, sonar data, pointclouds, tactile data, etc. Anything that your AI algorithm needs in order to decide what's the next action.</li>\n",
    "<li><i>**reward**</i>: It's the reward for the current step taken. The higher the reward, the better the robot is performing, based on the conditions you stated.</li>\n",
    "<li><i>**done**</i>: It states whether the episode is done or not. In this case, it will be <b>done = True</b> if the robot has gone too close to a wall (0.2 in the laser sensor readings ).</li>\n",
    "<li><i>**info**</i>: Extra information. In this case, it's empty.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "observation, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**Exercise 1.1**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) First of all, you will have to create a local copy (to your Ignite workspace) of the **simple_qlearn.py** learning script. You can name it **my_simple_learning_turtlebot.py**. Also, copy the required Python modules, **qlearn.py** and **live_plot.py,** to your workspace. This is done because, this way, you will have your own copy of the script and you will be able to mess with it without any implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd gym_construct/scripts\n",
    "cp simple_qlearn.py /home/user/catkin_ws/src/my_simple_learning_turtlebot.py\n",
    "cp qlearn.py /home/user/catkin_ws/src/\n",
    "cp liveplot.py /home/user/catkin_ws/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, you should have these three new files in your workspace:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/openai_files_copied.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Try changing the number of **episodes**, and check if the robot performs better with more time given to learn.\n",
    "Keep in mind that in order to make it perform properly, you could easily need 2000 episodes. We recommend you just run **10** or so to see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Implement a new script using a different learning strategy; for instance, the **sarsa**. You have an almost identical example in the **circuit2_turtlebot_lidar_sarsa.py** file, inside **gym_construct/scripts**. Just change the environment to the current one, and also copy the sarsa Python module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Implement your own algorithm based on the data returned by the **env.step()** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:green;\">**NOTE:** If, for any reason, your Python script stops working, you might have to **kill its process** in order to stop it, because the **CTRL+C** command may not be enough to stop it. This is because of how these Gym Python scripts are constructed. In order to stop the process, you can do the following:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps faux | grep python\n",
    "kill -9 id__your_script_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">**End of Exercise 1.1**</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
